{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ace19\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63889b2b6da8448b86789e3453a0082c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28dc5230def94a41aededc6deca8646c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d194cda0c8462ebb9581997706a481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5551b5961eca4c05846b220daa6c01c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54fced3d7fd44ddfb6756377390670a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.47.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50764486980b47c88704c7309ee108ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file model.safetensors from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\model.safetensors\n",
      "All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5261445a430c44d8bc80a5d2e83de5b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b62a70f740845d5a0a4698afa7f7cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85bf699e9fb8470295ce7b7d154bc59f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e6e3139eae4864b0c4c98789862e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "886290ec0a084004b80b3a5d1c0096e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Failed to analyze gensim: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "loading configuration file config.json from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.47.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\model.safetensors\n",
      "All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "ERROR:__main__:Failed to analyze nltk: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "loading configuration file config.json from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.47.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\model.safetensors\n",
      "All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "ERROR:__main__:Failed to analyze sentence_transformers: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "loading configuration file config.json from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.47.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\model.safetensors\n",
      "All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "ERROR:__main__:Failed to analyze spacy: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "loading configuration file config.json from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.47.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\model.safetensors\n",
      "All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "ERROR:__main__:Failed to analyze textblob: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "loading configuration file config.json from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.47.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\model.safetensors\n",
      "All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\ace19\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "ERROR:__main__:Failed to analyze transformers: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.\n",
      "INFO:__main__:Analysis complete. Results saved to nlp_packages_analysis.json\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import inspect\n",
    "import json\n",
    "import logging\n",
    "import pkgutil\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Set\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import spacy\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class MethodInfo:\n",
    "    signature: str\n",
    "    doc: Optional[str]\n",
    "    parameters: Dict[str, str]\n",
    "    return_type: Optional[str] = None\n",
    "    category: str = \"uncategorized\"\n",
    "    complexity: str = \"unknown\"\n",
    "    semantic_group: str = \"unknown\"\n",
    "    input_output_pattern: str = \"unknown\"\n",
    "    related_methods: List[str] = field(default_factory=list)\n",
    "    usage_examples: List[str] = field(default_factory=list)\n",
    "    semantic_embedding: Optional[np.ndarray] = None\n",
    "\n",
    "@dataclass\n",
    "class ClassInfo:\n",
    "    methods: Dict[str, MethodInfo]\n",
    "    attributes: Dict[str, Any]\n",
    "    doc: Optional[str]\n",
    "    base_classes: List[str]\n",
    "    category: str = \"uncategorized\"\n",
    "    semantic_group: str = \"unknown\"\n",
    "    responsibility_cluster: str = \"unknown\"\n",
    "    design_pattern: str = \"unknown\"\n",
    "    related_classes: List[str] = field(default_factory=list)\n",
    "    semantic_embedding: Optional[np.ndarray] = None\n",
    "\n",
    "@dataclass\n",
    "class ModuleAnalysis:\n",
    "    classes: Dict[str, ClassInfo] = field(default_factory=dict)\n",
    "    functions: Dict[str, MethodInfo] = field(default_factory=dict)\n",
    "    constants: Dict[str, Any] = field(default_factory=dict)\n",
    "    modules: Set[str] = field(default_factory=set)\n",
    "    categories: Dict[str, List[str]] = field(default_factory=lambda: defaultdict(list))\n",
    "    semantic_clusters: Dict[str, List[str]] = field(default_factory=dict)\n",
    "    dependency_graph: nx.DiGraph = field(default_factory=nx.DiGraph)\n",
    "    api_groups: Dict[str, List[str]] = field(default_factory=dict)\n",
    "    functionality_domains: Dict[str, List[str]] = field(default_factory=dict)\n",
    "\n",
    "class SemanticAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.nlp = spacy.load('en_core_web_trf')\n",
    "        self.doc2vec = Doc2Vec(vector_size=100, min_count=2, epochs=30)\n",
    "\n",
    "    def compute_embedding(self, text: str) -> np.ndarray:\n",
    "        return self.sentence_model.encode(text)\n",
    "\n",
    "    def cluster_embeddings(self, embeddings: List[np.ndarray], eps=0.5) -> List[int]:\n",
    "        clustering = DBSCAN(eps=eps, min_samples=2).fit(embeddings)\n",
    "        return clustering.labels_\n",
    "\n",
    "    def extract_concepts(self, text: str) -> List[str]:\n",
    "        doc = self.nlp(text)\n",
    "        return [chunk.text for chunk in doc.noun_chunks]\n",
    "\n",
    "def categorize_item(name: str, doc: Optional[str], signature: Optional[str] = None) -> str:\n",
    "    \"\"\"Enhanced categorization using NLP techniques\"\"\"\n",
    "    name = name.lower()\n",
    "    doc = doc or \"\"\n",
    "\n",
    "    categories = {\n",
    "        \"io\": [\"load\", \"save\", \"read\", \"write\", \"import\", \"export\", \"file\", \"stream\"],\n",
    "        \"math\": [\"calculate\", \"compute\", \"sum\", \"average\", \"mean\", \"numeric\", \"statistical\"],\n",
    "        \"text\": [\"tokenize\", \"parse\", \"text\", \"string\", \"word\", \"nlp\", \"linguistic\"],\n",
    "        \"data\": [\"process\", \"transform\", \"convert\", \"format\", \"structure\", \"collection\"],\n",
    "        \"utils\": [\"util\", \"helper\", \"tool\", \"auxiliary\", \"support\"],\n",
    "        \"visualization\": [\"plot\", \"draw\", \"display\", \"show\", \"render\", \"graph\"],\n",
    "        \"ml\": [\"train\", \"predict\", \"model\", \"classify\", \"cluster\", \"learn\"],\n",
    "        \"api\": [\"request\", \"response\", \"endpoint\", \"service\", \"client\"],\n",
    "        \"validation\": [\"check\", \"verify\", \"validate\", \"assert\", \"test\"],\n",
    "        \"security\": [\"encrypt\", \"decrypt\", \"auth\", \"secure\", \"permission\"]\n",
    "    }\n",
    "\n",
    "    blob = TextBlob(f\"{name} {doc} {signature or ''}\")\n",
    "\n",
    "    # Weight different signals\n",
    "    scores = defaultdict(float)\n",
    "    for category, keywords in categories.items():\n",
    "        # Name matching\n",
    "        if any(k in name for k in keywords):\n",
    "            scores[category] += 2.0\n",
    "\n",
    "        # Keyword presence in doc\n",
    "        matches = sum(1 for k in keywords if k in doc.lower())\n",
    "        scores[category] += matches * 0.5\n",
    "\n",
    "        # Sentiment and subjectivity analysis\n",
    "        sentiment = blob.sentiment.polarity\n",
    "        if sentiment > 0:\n",
    "            scores[category] += 0.2\n",
    "\n",
    "    return max(scores.items(), key=lambda x: x[1])[0] if scores else \"other\"\n",
    "\n",
    "def cluster_by_similarity(items: Dict[str, MethodInfo], analyzer: SemanticAnalyzer) -> Dict[str, List[str]]:\n",
    "    \"\"\"Advanced clustering using multiple similarity measures\"\"\"\n",
    "    # Get embeddings for all items\n",
    "    embeddings = []\n",
    "    names = []\n",
    "    for name, info in items.items():\n",
    "        text = f\"{name} {info.doc or ''} {info.signature}\"\n",
    "        embedding = analyzer.compute_embedding(text)\n",
    "        embeddings.append(embedding)\n",
    "        names.append(name)\n",
    "\n",
    "    # Cluster using embeddings\n",
    "    clusters = defaultdict(list)\n",
    "    labels = analyzer.cluster_embeddings(embeddings)\n",
    "\n",
    "    for name, label in zip(names, labels):\n",
    "        if label >= 0:  # Ignore noise points labeled as -1\n",
    "            clusters[f\"semantic_cluster_{label}\"].append(name)\n",
    "\n",
    "    # Additional clustering by input/output patterns\n",
    "    io_patterns = defaultdict(list)\n",
    "    for name, info in items.items():\n",
    "        pattern = (tuple(info.parameters.values()), info.return_type)\n",
    "        io_patterns[pattern].append(name)\n",
    "\n",
    "    # Merge results\n",
    "    final_clusters = {}\n",
    "    final_clusters.update({f\"io_pattern_{i}\": names for i, names in enumerate(io_patterns.values())})\n",
    "    final_clusters.update(clusters)\n",
    "\n",
    "    return final_clusters\n",
    "\n",
    "def analyze_package(package_name: str) -> ModuleAnalysis:\n",
    "    \"\"\"Enhanced package analysis with semantic understanding and advanced organization\"\"\"\n",
    "    analysis = ModuleAnalysis()\n",
    "    analyzer = SemanticAnalyzer()\n",
    "\n",
    "    try:\n",
    "        package = importlib.import_module(package_name)\n",
    "        logger.info(f\"Analyzing package: {package_name}\")\n",
    "\n",
    "        if hasattr(package, '__path__'):\n",
    "            for _, name, _ in sorted(pkgutil.walk_packages(package.__path__, package.__name__ + '.')):\n",
    "                try:\n",
    "                    module = importlib.import_module(name)\n",
    "                    analysis.modules.add(name)\n",
    "\n",
    "                    # Enhanced module content analysis\n",
    "                    for item_name, item in sorted(inspect.getmembers(module)):\n",
    "                        if item_name.startswith('_'):\n",
    "                            continue\n",
    "\n",
    "                        if inspect.isclass(item):\n",
    "                            methods = {}\n",
    "                            for method_name, method in sorted(inspect.getmembers(item, inspect.isfunction)):\n",
    "                                if not method_name.startswith('_'):\n",
    "                                    sig = inspect.signature(method)\n",
    "                                    doc = inspect.getdoc(method)\n",
    "\n",
    "                                    method_info = MethodInfo(\n",
    "                                        signature=str(sig),\n",
    "                                        doc=doc,\n",
    "                                        parameters={\n",
    "                                            name: str(param.annotation)\n",
    "                                            for name, param in sig.parameters.items()\n",
    "                                        },\n",
    "                                        return_type=str(sig.return_annotation),\n",
    "                                        category=categorize_item(method_name, doc, str(sig)),\n",
    "                                        semantic_embedding=analyzer.compute_embedding(f\"{method_name} {doc or ''}\")\n",
    "                                    )\n",
    "                                    methods[method_name] = method_info\n",
    "\n",
    "                            class_doc = inspect.getdoc(item)\n",
    "                            class_info = ClassInfo(\n",
    "                                methods=methods,\n",
    "                                attributes={},\n",
    "                                doc=class_doc,\n",
    "                                base_classes=[str(b) for b in item.__bases__],\n",
    "                                category=categorize_item(item_name, class_doc),\n",
    "                                semantic_embedding=analyzer.compute_embedding(f\"{item_name} {class_doc or ''}\")\n",
    "                            )\n",
    "                            analysis.classes[f\"{name}.{item_name}\"] = class_info\n",
    "                            analysis.categories[class_info.category].append(f\"{name}.{item_name}\")\n",
    "\n",
    "                        elif inspect.isfunction(item):\n",
    "                            sig = inspect.signature(item)\n",
    "                            doc = inspect.getdoc(item)\n",
    "\n",
    "                            func_info = MethodInfo(\n",
    "                                signature=str(sig),\n",
    "                                doc=doc,\n",
    "                                parameters={\n",
    "                                    name: str(param.annotation)\n",
    "                                    for name, param in sig.parameters.items()\n",
    "                                },\n",
    "                                return_type=str(sig.return_annotation),\n",
    "                                category=categorize_item(item_name, doc, str(sig)),\n",
    "                                semantic_embedding=analyzer.compute_embedding(f\"{item_name} {doc or ''}\")\n",
    "                            )\n",
    "                            analysis.functions[f\"{name}.{item_name}\"] = func_info\n",
    "                            analysis.categories[func_info.category].append(f\"{name}.{item_name}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error analyzing module {name}: {str(e)}\")\n",
    "\n",
    "        # Build dependency graph\n",
    "        for class_name, class_info in analysis.classes.items():\n",
    "            for base in class_info.base_classes:\n",
    "                analysis.dependency_graph.add_edge(base, class_name)\n",
    "\n",
    "        # Cluster functions and methods\n",
    "        analysis.semantic_clusters = cluster_by_similarity(analysis.functions, analyzer)\n",
    "\n",
    "    except ImportError as e:\n",
    "        logger.error(f\"Could not import {package_name}: {str(e)}\")\n",
    "\n",
    "    return analysis\n",
    "\n",
    "# List of packages to analyze\n",
    "packages = sorted([\n",
    "    'gensim',\n",
    "    'spacy',\n",
    "    'nltk',\n",
    "    'transformers',\n",
    "    'textblob',\n",
    "    'sentence_transformers'\n",
    "])\n",
    "\n",
    "# Analyze each package\n",
    "analysis_results = {}\n",
    "for package in packages:\n",
    "    try:\n",
    "        analysis = analyze_package(package)\n",
    "\n",
    "        # Organize results hierarchically\n",
    "        analysis_results[package] = {\n",
    "            \"overview\": {\n",
    "                \"total_classes\": len(analysis.classes),\n",
    "                \"total_functions\": len(analysis.functions),\n",
    "                \"total_modules\": len(analysis.modules),\n",
    "                \"total_categories\": len(analysis.categories)\n",
    "            },\n",
    "            \"classes\": {\n",
    "                category: {\n",
    "                    k: vars(v) for k,v in sorted(analysis.classes.items())\n",
    "                    if v.category == category\n",
    "                }\n",
    "                for category in sorted(set(c.category for c in analysis.classes.values()))\n",
    "            },\n",
    "            \"functions\": {\n",
    "                category: {\n",
    "                    k: vars(v) for k,v in sorted(analysis.functions.items())\n",
    "                    if v.category == category\n",
    "                }\n",
    "                for category in sorted(set(f.category for f in analysis.functions.values()))\n",
    "            },\n",
    "            \"modules\": sorted(analysis.modules),\n",
    "            \"semantic_clusters\": {k: sorted(v) for k,v in sorted(analysis.semantic_clusters.items())},\n",
    "            \"dependency_graph\": nx.node_link_data(analysis.dependency_graph)\n",
    "        }\n",
    "\n",
    "        logger.info(f\"Successfully analyzed {package}\")\n",
    "        logger.info(f\"{package} analysis summary:\")\n",
    "        for key, value in analysis_results[package][\"overview\"].items():\n",
    "            logger.info(f\"- {key}: {value}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to analyze {package}: {str(e)}\")\n",
    "\n",
    "# Save results with detailed formatting\n",
    "with open('nlp_packages_analysis.json', 'w') as f:\n",
    "    json.dump(analysis_results, f, indent=2, sort_keys=True, default=str)\n",
    "\n",
    "logger.info(\"Analysis complete. Results saved to nlp_packages_analysis.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
