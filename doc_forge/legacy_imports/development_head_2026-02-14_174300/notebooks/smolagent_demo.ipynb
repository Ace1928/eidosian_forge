{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # # **SmolAgent Demo: Multi-Agent System Architecture**\n",
    "# # \n",
    "# # Welcome to the SmolAgent system, a sophisticated multi-agent architecture designed for modularity and scalability. Below is a detailed representation of its components:\n",
    "# # \n",
    "# # ```\n",
    "# # ┌────────────────────────────────────────────────────────────────────────────┐\n",
    "# # │                                SmolAgent System                           │\n",
    "# # └────────────────────────────────────────────────────────────────────────────┘\n",
    "# # \n",
    "# # ┌────────────────────────────────────────────────────────────────────────────┐\n",
    "# # │                              +----------------+                            │\n",
    "# # │                              |  Manager Agent |                            │\n",
    "# # │                              +----------------+                            │\n",
    "# # │                                       |                                    │\n",
    "# # │                          ______________|______________                     │\n",
    "# # │                         |                              |                   │\n",
    "# # │                  +----------------+         +-----------------------------+│\n",
    "# # │                  | Code Interpreter|         |       Managed Agent        |│\n",
    "# # │                  |      Tool       |         |                            |│\n",
    "# # │                  +----------------+          |  +---------------------+   |│\n",
    "# # │                                               |  |  Web Search Agent  |   |│\n",
    "# # │                                               |  +---------------------+   |│\n",
    "# # │                                               |         |       |         |│\n",
    "# # │                                               |  +-------------+ |         |│\n",
    "# # │                                               |  | Web Search  | |         |│\n",
    "# # │                                               |  |    Tool     | |         |│\n",
    "# # │                                               |  +-------------+ |         |│\n",
    "# # │                                               |                  |         |│\n",
    "# # │                                               |  +-----------------------+ |│\n",
    "# # │                                               |  |  Visit Webpage Tool   | |│\n",
    "# # │                                               |  +-----------------------+ |│\n",
    "# # │                                               +-----------------------------+│\n",
    "# # └────────────────────────────────────────────────────────────────────────────┘\n",
    "# # ```\n",
    "# # \n",
    "# # ### Component Breakdown:\n",
    "# # \n",
    "# # - **Manager Agent**: The central controller that orchestrates the entire system, ensuring seamless operation and coordination among agents.\n",
    "# # \n",
    "# # - **Code Interpreter Tool**: A powerful tool utilized by the Manager Agent to interpret and execute code, enabling dynamic task execution.\n",
    "# # \n",
    "# # - **Managed Agent**: A versatile agent under the supervision of the Manager Agent, responsible for executing specific tasks.\n",
    "# # \n",
    "# #   - **Web Search Agent**: A specialized agent dedicated to conducting web searches efficiently.\n",
    "# #     - **Web Search Tool**: A tool employed by the Web Search Agent to perform precise and effective searches.\n",
    "# #     - **Visit Webpage Tool**: A tool that allows the Web Search Agent to visit and interact with web pages, gathering necessary information.\n",
    "# # \n",
    "# # This architecture empowers the SmolAgent system to be highly adaptable, allowing each agent and tool to perform distinct roles while being easily managed and extended. The modular design ensures that the system can scale and evolve with minimal disruption.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 17:44:20,433 - INFO - Starting package installation process.\n",
      "2025-01-10 17:44:20,444 - DEBUG - Attempting to install/upgrade package: markdownify\n",
      "2025-01-10 17:44:26,067 - INFO - Successfully installed/updated package: markdownify\n",
      "2025-01-10 17:44:26,069 - DEBUG - Attempting to install/upgrade package: duckduckgo-search\n",
      "2025-01-10 17:44:28,970 - INFO - Successfully installed/updated package: duckduckgo-search\n",
      "2025-01-10 17:44:28,971 - DEBUG - Attempting to install/upgrade package: smolagents\n",
      "2025-01-10 17:44:47,662 - ERROR - Failed to install package: smolagents. Error: Command '['C:\\\\Users\\\\ace19\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\\python.exe', '-m', 'pip', 'install', 'smolagents', '--upgrade']' returned non-zero exit status 1.\n",
      "2025-01-10 17:44:47,663 - DEBUG - Attempting to install/upgrade package: pydantic\n",
      "2025-01-10 17:44:52,899 - INFO - Successfully installed/updated package: pydantic\n",
      "2025-01-10 17:44:52,901 - DEBUG - Attempting to install/upgrade package: deepspeed\n",
      "2025-01-10 17:45:07,621 - ERROR - Failed to install package: deepspeed. Error: Command '['C:\\\\Users\\\\ace19\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\\python.exe', '-m', 'pip', 'install', 'deepspeed', '--upgrade']' returned non-zero exit status 1.\n",
      "2025-01-10 17:45:07,627 - DEBUG - Attempting to install/upgrade package: gradient\n",
      "2025-01-10 17:45:32,605 - ERROR - Failed to install package: gradient. Error: Command '['C:\\\\Users\\\\ace19\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\\python.exe', '-m', 'pip', 'install', 'gradient', '--upgrade']' returned non-zero exit status 1.\n",
      "2025-01-10 17:45:32,609 - DEBUG - Attempting to install/upgrade package: transformers\n",
      "2025-01-10 17:45:35,339 - INFO - Successfully installed/updated package: transformers\n",
      "2025-01-10 17:45:35,341 - DEBUG - Attempting to install/upgrade package: torch\n",
      "2025-01-10 17:45:37,982 - INFO - Successfully installed/updated package: torch\n",
      "2025-01-10 17:45:37,986 - DEBUG - Attempting to install/upgrade package: torch_geometric\n",
      "2025-01-10 17:45:40,784 - INFO - Successfully installed/updated package: torch_geometric\n",
      "2025-01-10 17:45:40,786 - DEBUG - Attempting to install/upgrade package: nltk\n",
      "2025-01-10 17:45:48,330 - INFO - Successfully installed/updated package: nltk\n",
      "2025-01-10 17:45:48,334 - DEBUG - Attempting to install/upgrade package: spacy\n",
      "2025-01-10 17:45:52,467 - INFO - Successfully installed/updated package: spacy\n",
      "2025-01-10 17:45:52,469 - DEBUG - Attempting to install/upgrade package: gensim\n",
      "2025-01-10 17:45:55,681 - INFO - Successfully installed/updated package: gensim\n",
      "2025-01-10 17:45:55,684 - DEBUG - Attempting to install/upgrade package: ragflow\n",
      "2025-01-10 17:45:58,391 - INFO - Successfully installed/updated package: ragflow\n",
      "2025-01-10 17:45:58,393 - DEBUG - Attempting to install/upgrade package: openai\n",
      "2025-01-10 17:46:08,534 - INFO - Successfully installed/updated package: openai\n",
      "2025-01-10 17:46:08,536 - DEBUG - Attempting to install/upgrade package: faiss_cpu\n",
      "2025-01-10 17:46:13,169 - INFO - Successfully installed/updated package: faiss_cpu\n",
      "2025-01-10 17:46:13,173 - INFO - Package installation process completed.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import logging\n",
    "from typing import List\n",
    "\n",
    "# Configure logging for detailed output\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class PackageInstaller:\n",
    "    \"\"\"A class to handle the installation and upgrading of packages.\"\"\"\n",
    "\n",
    "    def __init__(self, packages: List[str] = None):\n",
    "        self.packages = packages or []\n",
    "\n",
    "    def install_packages(self):\n",
    "        \"\"\"Install and upgrade a list of packages using pip.\"\"\"\n",
    "        for package in self.packages:\n",
    "            try:\n",
    "                logging.debug(f\"Attempting to install/upgrade package: {package}\")\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--upgrade\"])\n",
    "                logging.info(f\"Successfully installed/updated package: {package}\")\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                logging.error(f\"Failed to install package: {package}. Error: {e}\")\n",
    "                continue  # Continue with the next package\n",
    "            except Exception as e:\n",
    "                logging.critical(f\"Unexpected error while installing package: {package}. Error: {e}\", exc_info=True)\n",
    "                continue  # Continue with the next package\n",
    "\n",
    "def main():\n",
    "    packages = [\n",
    "        \"markdownify\", \"duckduckgo-search\", \"smolagents\", \"pydantic\", \"deepspeed\", \n",
    "        \"gradient\", \"transformers\", \"torch\", \"torch_geometric\", \"nltk\", \"spacy\", \n",
    "        \"gensim\", \"ragflow\", \"openai\", \"faiss_cpu\"\n",
    "    ]\n",
    "\n",
    "    logging.info(\"Starting package installation process.\")\n",
    "    installer = PackageInstaller(packages)\n",
    "    installer.install_packages()\n",
    "    logging.info(\"Package installation process completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 18:25:10,438 - DEBUG - [1583482292.py:144] - ResourceManager initialized.\n",
      "2025-01-10 18:25:10,438 - DEBUG - [1583482292.py:144] - ResourceManager initialized.\n",
      "2025-01-10 18:25:10,438 - DEBUG - [1583482292.py:144] - ResourceManager initialized.\n",
      "2025-01-10 18:25:10,438 - DEBUG - [1583482292.py:144] - ResourceManager initialized.\n",
      "2025-01-10 18:25:10,438 - DEBUG - [1583482292.py:144] - ResourceManager initialized.\n",
      "2025-01-10 18:25:10,438 - DEBUG - [1583482292.py:144] - ResourceManager initialized.\n",
      "2025-01-10 18:25:10,438 - DEBUG - [1583482292.py:144] - ResourceManager initialized.\n",
      "2025-01-10 18:25:10,438 - DEBUG - [1583482292.py:144] - ResourceManager initialized.\n",
      "2025-01-10 18:25:10,438 - DEBUG - [1583482292.py:144] - ResourceManager initialized.\n",
      "2025-01-10 18:25:10,438 - DEBUG - [1583482292.py:144] - ResourceManager initialized.\n",
      "2025-01-10 18:25:10,438 - DEBUG - [1583482292.py:144] - ResourceManager initialized.\n",
      "2025-01-10 18:25:10,438 - DEBUG - [1583482292.py:144] - ResourceManager initialized.\n",
      "2025-01-10 18:25:10,438 - DEBUG - ResourceManager initialized.\n",
      "2025-01-10 18:25:10,463 - DEBUG - [1583482292.py:44] - DiskIOManager initialized with chunk size: 1048576 bytes.\n",
      "2025-01-10 18:25:10,463 - DEBUG - [1583482292.py:44] - DiskIOManager initialized with chunk size: 1048576 bytes.\n",
      "2025-01-10 18:25:10,463 - DEBUG - [1583482292.py:44] - DiskIOManager initialized with chunk size: 1048576 bytes.\n",
      "2025-01-10 18:25:10,463 - DEBUG - [1583482292.py:44] - DiskIOManager initialized with chunk size: 1048576 bytes.\n",
      "2025-01-10 18:25:10,463 - DEBUG - [1583482292.py:44] - DiskIOManager initialized with chunk size: 1048576 bytes.\n",
      "2025-01-10 18:25:10,463 - DEBUG - [1583482292.py:44] - DiskIOManager initialized with chunk size: 1048576 bytes.\n",
      "2025-01-10 18:25:10,463 - DEBUG - [1583482292.py:44] - DiskIOManager initialized with chunk size: 1048576 bytes.\n",
      "2025-01-10 18:25:10,463 - DEBUG - [1583482292.py:44] - DiskIOManager initialized with chunk size: 1048576 bytes.\n",
      "2025-01-10 18:25:10,463 - DEBUG - [1583482292.py:44] - DiskIOManager initialized with chunk size: 1048576 bytes.\n",
      "2025-01-10 18:25:10,463 - DEBUG - [1583482292.py:44] - DiskIOManager initialized with chunk size: 1048576 bytes.\n",
      "2025-01-10 18:25:10,463 - DEBUG - [1583482292.py:44] - DiskIOManager initialized with chunk size: 1048576 bytes.\n",
      "2025-01-10 18:25:10,463 - DEBUG - [1583482292.py:44] - DiskIOManager initialized with chunk size: 1048576 bytes.\n",
      "2025-01-10 18:25:10,463 - DEBUG - DiskIOManager initialized with chunk size: 1048576 bytes.\n",
      "2025-01-10 18:25:10,475 - INFO - [1583482292.py:296] - Starting directory creation process.\n",
      "2025-01-10 18:25:10,475 - INFO - [1583482292.py:296] - Starting directory creation process.\n",
      "2025-01-10 18:25:10,475 - INFO - [1583482292.py:296] - Starting directory creation process.\n",
      "2025-01-10 18:25:10,475 - INFO - [1583482292.py:296] - Starting directory creation process.\n",
      "2025-01-10 18:25:10,475 - INFO - [1583482292.py:296] - Starting directory creation process.\n",
      "2025-01-10 18:25:10,475 - INFO - [1583482292.py:296] - Starting directory creation process.\n",
      "2025-01-10 18:25:10,475 - INFO - [1583482292.py:296] - Starting directory creation process.\n",
      "2025-01-10 18:25:10,475 - INFO - [1583482292.py:296] - Starting directory creation process.\n",
      "2025-01-10 18:25:10,475 - INFO - [1583482292.py:296] - Starting directory creation process.\n",
      "2025-01-10 18:25:10,475 - INFO - [1583482292.py:296] - Starting directory creation process.\n",
      "2025-01-10 18:25:10,475 - INFO - [1583482292.py:296] - Starting directory creation process.\n",
      "2025-01-10 18:25:10,475 - INFO - [1583482292.py:296] - Starting directory creation process.\n",
      "2025-01-10 18:25:10,475 - INFO - Starting directory creation process.\n",
      "2025-01-10 18:25:10,489 - DEBUG - [1583482292.py:318] - Directory 'saved_model' already exists: ./saved_models\n",
      "2025-01-10 18:25:10,489 - DEBUG - [1583482292.py:318] - Directory 'saved_model' already exists: ./saved_models\n",
      "2025-01-10 18:25:10,489 - DEBUG - [1583482292.py:318] - Directory 'saved_model' already exists: ./saved_models\n",
      "2025-01-10 18:25:10,489 - DEBUG - [1583482292.py:318] - Directory 'saved_model' already exists: ./saved_models\n",
      "2025-01-10 18:25:10,489 - DEBUG - [1583482292.py:318] - Directory 'saved_model' already exists: ./saved_models\n",
      "2025-01-10 18:25:10,489 - DEBUG - [1583482292.py:318] - Directory 'saved_model' already exists: ./saved_models\n",
      "2025-01-10 18:25:10,489 - DEBUG - [1583482292.py:318] - Directory 'saved_model' already exists: ./saved_models\n",
      "2025-01-10 18:25:10,489 - DEBUG - [1583482292.py:318] - Directory 'saved_model' already exists: ./saved_models\n",
      "2025-01-10 18:25:10,489 - DEBUG - [1583482292.py:318] - Directory 'saved_model' already exists: ./saved_models\n",
      "2025-01-10 18:25:10,489 - DEBUG - [1583482292.py:318] - Directory 'saved_model' already exists: ./saved_models\n",
      "2025-01-10 18:25:10,489 - DEBUG - [1583482292.py:318] - Directory 'saved_model' already exists: ./saved_models\n",
      "2025-01-10 18:25:10,489 - DEBUG - [1583482292.py:318] - Directory 'saved_model' already exists: ./saved_models\n",
      "2025-01-10 18:25:10,489 - DEBUG - Directory 'saved_model' already exists: ./saved_models\n",
      "2025-01-10 18:25:10,502 - DEBUG - [1583482292.py:318] - Directory 'datasets' already exists: ./datasets\n",
      "2025-01-10 18:25:10,502 - DEBUG - [1583482292.py:318] - Directory 'datasets' already exists: ./datasets\n",
      "2025-01-10 18:25:10,502 - DEBUG - [1583482292.py:318] - Directory 'datasets' already exists: ./datasets\n",
      "2025-01-10 18:25:10,502 - DEBUG - [1583482292.py:318] - Directory 'datasets' already exists: ./datasets\n",
      "2025-01-10 18:25:10,502 - DEBUG - [1583482292.py:318] - Directory 'datasets' already exists: ./datasets\n",
      "2025-01-10 18:25:10,502 - DEBUG - [1583482292.py:318] - Directory 'datasets' already exists: ./datasets\n",
      "2025-01-10 18:25:10,502 - DEBUG - [1583482292.py:318] - Directory 'datasets' already exists: ./datasets\n",
      "2025-01-10 18:25:10,502 - DEBUG - [1583482292.py:318] - Directory 'datasets' already exists: ./datasets\n",
      "2025-01-10 18:25:10,502 - DEBUG - [1583482292.py:318] - Directory 'datasets' already exists: ./datasets\n",
      "2025-01-10 18:25:10,502 - DEBUG - [1583482292.py:318] - Directory 'datasets' already exists: ./datasets\n",
      "2025-01-10 18:25:10,502 - DEBUG - [1583482292.py:318] - Directory 'datasets' already exists: ./datasets\n",
      "2025-01-10 18:25:10,502 - DEBUG - [1583482292.py:318] - Directory 'datasets' already exists: ./datasets\n",
      "2025-01-10 18:25:10,502 - DEBUG - Directory 'datasets' already exists: ./datasets\n",
      "2025-01-10 18:25:10,514 - DEBUG - [1583482292.py:318] - Directory 'checkpoints' already exists: ./checkpoints\n",
      "2025-01-10 18:25:10,514 - DEBUG - [1583482292.py:318] - Directory 'checkpoints' already exists: ./checkpoints\n",
      "2025-01-10 18:25:10,514 - DEBUG - [1583482292.py:318] - Directory 'checkpoints' already exists: ./checkpoints\n",
      "2025-01-10 18:25:10,514 - DEBUG - [1583482292.py:318] - Directory 'checkpoints' already exists: ./checkpoints\n",
      "2025-01-10 18:25:10,514 - DEBUG - [1583482292.py:318] - Directory 'checkpoints' already exists: ./checkpoints\n",
      "2025-01-10 18:25:10,514 - DEBUG - [1583482292.py:318] - Directory 'checkpoints' already exists: ./checkpoints\n",
      "2025-01-10 18:25:10,514 - DEBUG - [1583482292.py:318] - Directory 'checkpoints' already exists: ./checkpoints\n",
      "2025-01-10 18:25:10,514 - DEBUG - [1583482292.py:318] - Directory 'checkpoints' already exists: ./checkpoints\n",
      "2025-01-10 18:25:10,514 - DEBUG - [1583482292.py:318] - Directory 'checkpoints' already exists: ./checkpoints\n",
      "2025-01-10 18:25:10,514 - DEBUG - [1583482292.py:318] - Directory 'checkpoints' already exists: ./checkpoints\n",
      "2025-01-10 18:25:10,514 - DEBUG - [1583482292.py:318] - Directory 'checkpoints' already exists: ./checkpoints\n",
      "2025-01-10 18:25:10,514 - DEBUG - [1583482292.py:318] - Directory 'checkpoints' already exists: ./checkpoints\n",
      "2025-01-10 18:25:10,514 - DEBUG - Directory 'checkpoints' already exists: ./checkpoints\n",
      "2025-01-10 18:25:10,524 - DEBUG - [1583482292.py:318] - Directory 'cache' already exists: ./cache\n",
      "2025-01-10 18:25:10,524 - DEBUG - [1583482292.py:318] - Directory 'cache' already exists: ./cache\n",
      "2025-01-10 18:25:10,524 - DEBUG - [1583482292.py:318] - Directory 'cache' already exists: ./cache\n",
      "2025-01-10 18:25:10,524 - DEBUG - [1583482292.py:318] - Directory 'cache' already exists: ./cache\n",
      "2025-01-10 18:25:10,524 - DEBUG - [1583482292.py:318] - Directory 'cache' already exists: ./cache\n",
      "2025-01-10 18:25:10,524 - DEBUG - [1583482292.py:318] - Directory 'cache' already exists: ./cache\n",
      "2025-01-10 18:25:10,524 - DEBUG - [1583482292.py:318] - Directory 'cache' already exists: ./cache\n",
      "2025-01-10 18:25:10,524 - DEBUG - [1583482292.py:318] - Directory 'cache' already exists: ./cache\n",
      "2025-01-10 18:25:10,524 - DEBUG - [1583482292.py:318] - Directory 'cache' already exists: ./cache\n",
      "2025-01-10 18:25:10,524 - DEBUG - [1583482292.py:318] - Directory 'cache' already exists: ./cache\n",
      "2025-01-10 18:25:10,524 - DEBUG - [1583482292.py:318] - Directory 'cache' already exists: ./cache\n",
      "2025-01-10 18:25:10,524 - DEBUG - [1583482292.py:318] - Directory 'cache' already exists: ./cache\n",
      "2025-01-10 18:25:10,524 - DEBUG - Directory 'cache' already exists: ./cache\n",
      "2025-01-10 18:25:10,534 - DEBUG - [1583482292.py:318] - Directory 'logs' already exists: ./logs\n",
      "2025-01-10 18:25:10,534 - DEBUG - [1583482292.py:318] - Directory 'logs' already exists: ./logs\n",
      "2025-01-10 18:25:10,534 - DEBUG - [1583482292.py:318] - Directory 'logs' already exists: ./logs\n",
      "2025-01-10 18:25:10,534 - DEBUG - [1583482292.py:318] - Directory 'logs' already exists: ./logs\n",
      "2025-01-10 18:25:10,534 - DEBUG - [1583482292.py:318] - Directory 'logs' already exists: ./logs\n",
      "2025-01-10 18:25:10,534 - DEBUG - [1583482292.py:318] - Directory 'logs' already exists: ./logs\n",
      "2025-01-10 18:25:10,534 - DEBUG - [1583482292.py:318] - Directory 'logs' already exists: ./logs\n",
      "2025-01-10 18:25:10,534 - DEBUG - [1583482292.py:318] - Directory 'logs' already exists: ./logs\n",
      "2025-01-10 18:25:10,534 - DEBUG - [1583482292.py:318] - Directory 'logs' already exists: ./logs\n",
      "2025-01-10 18:25:10,534 - DEBUG - [1583482292.py:318] - Directory 'logs' already exists: ./logs\n",
      "2025-01-10 18:25:10,534 - DEBUG - [1583482292.py:318] - Directory 'logs' already exists: ./logs\n",
      "2025-01-10 18:25:10,534 - DEBUG - [1583482292.py:318] - Directory 'logs' already exists: ./logs\n",
      "2025-01-10 18:25:10,534 - DEBUG - Directory 'logs' already exists: ./logs\n",
      "2025-01-10 18:25:10,543 - DEBUG - [1583482292.py:318] - Directory 'offload' already exists: ./offload\n",
      "2025-01-10 18:25:10,543 - DEBUG - [1583482292.py:318] - Directory 'offload' already exists: ./offload\n",
      "2025-01-10 18:25:10,543 - DEBUG - [1583482292.py:318] - Directory 'offload' already exists: ./offload\n",
      "2025-01-10 18:25:10,543 - DEBUG - [1583482292.py:318] - Directory 'offload' already exists: ./offload\n",
      "2025-01-10 18:25:10,543 - DEBUG - [1583482292.py:318] - Directory 'offload' already exists: ./offload\n",
      "2025-01-10 18:25:10,543 - DEBUG - [1583482292.py:318] - Directory 'offload' already exists: ./offload\n",
      "2025-01-10 18:25:10,543 - DEBUG - [1583482292.py:318] - Directory 'offload' already exists: ./offload\n",
      "2025-01-10 18:25:10,543 - DEBUG - [1583482292.py:318] - Directory 'offload' already exists: ./offload\n",
      "2025-01-10 18:25:10,543 - DEBUG - [1583482292.py:318] - Directory 'offload' already exists: ./offload\n",
      "2025-01-10 18:25:10,543 - DEBUG - [1583482292.py:318] - Directory 'offload' already exists: ./offload\n",
      "2025-01-10 18:25:10,543 - DEBUG - [1583482292.py:318] - Directory 'offload' already exists: ./offload\n",
      "2025-01-10 18:25:10,543 - DEBUG - [1583482292.py:318] - Directory 'offload' already exists: ./offload\n",
      "2025-01-10 18:25:10,543 - DEBUG - Directory 'offload' already exists: ./offload\n",
      "2025-01-10 18:25:10,551 - DEBUG - [1583482292.py:318] - Directory 'memory' already exists: ./memory\n",
      "2025-01-10 18:25:10,551 - DEBUG - [1583482292.py:318] - Directory 'memory' already exists: ./memory\n",
      "2025-01-10 18:25:10,551 - DEBUG - [1583482292.py:318] - Directory 'memory' already exists: ./memory\n",
      "2025-01-10 18:25:10,551 - DEBUG - [1583482292.py:318] - Directory 'memory' already exists: ./memory\n",
      "2025-01-10 18:25:10,551 - DEBUG - [1583482292.py:318] - Directory 'memory' already exists: ./memory\n",
      "2025-01-10 18:25:10,551 - DEBUG - [1583482292.py:318] - Directory 'memory' already exists: ./memory\n",
      "2025-01-10 18:25:10,551 - DEBUG - [1583482292.py:318] - Directory 'memory' already exists: ./memory\n",
      "2025-01-10 18:25:10,551 - DEBUG - [1583482292.py:318] - Directory 'memory' already exists: ./memory\n",
      "2025-01-10 18:25:10,551 - DEBUG - [1583482292.py:318] - Directory 'memory' already exists: ./memory\n",
      "2025-01-10 18:25:10,551 - DEBUG - [1583482292.py:318] - Directory 'memory' already exists: ./memory\n",
      "2025-01-10 18:25:10,551 - DEBUG - [1583482292.py:318] - Directory 'memory' already exists: ./memory\n",
      "2025-01-10 18:25:10,551 - DEBUG - [1583482292.py:318] - Directory 'memory' already exists: ./memory\n",
      "2025-01-10 18:25:10,551 - DEBUG - Directory 'memory' already exists: ./memory\n",
      "2025-01-10 18:25:10,564 - DEBUG - [1583482292.py:318] - Directory 'code_library' already exists: ./code_library\n",
      "2025-01-10 18:25:10,564 - DEBUG - [1583482292.py:318] - Directory 'code_library' already exists: ./code_library\n",
      "2025-01-10 18:25:10,564 - DEBUG - [1583482292.py:318] - Directory 'code_library' already exists: ./code_library\n",
      "2025-01-10 18:25:10,564 - DEBUG - [1583482292.py:318] - Directory 'code_library' already exists: ./code_library\n",
      "2025-01-10 18:25:10,564 - DEBUG - [1583482292.py:318] - Directory 'code_library' already exists: ./code_library\n",
      "2025-01-10 18:25:10,564 - DEBUG - [1583482292.py:318] - Directory 'code_library' already exists: ./code_library\n",
      "2025-01-10 18:25:10,564 - DEBUG - [1583482292.py:318] - Directory 'code_library' already exists: ./code_library\n",
      "2025-01-10 18:25:10,564 - DEBUG - [1583482292.py:318] - Directory 'code_library' already exists: ./code_library\n",
      "2025-01-10 18:25:10,564 - DEBUG - [1583482292.py:318] - Directory 'code_library' already exists: ./code_library\n",
      "2025-01-10 18:25:10,564 - DEBUG - [1583482292.py:318] - Directory 'code_library' already exists: ./code_library\n",
      "2025-01-10 18:25:10,564 - DEBUG - [1583482292.py:318] - Directory 'code_library' already exists: ./code_library\n",
      "2025-01-10 18:25:10,564 - DEBUG - [1583482292.py:318] - Directory 'code_library' already exists: ./code_library\n",
      "2025-01-10 18:25:10,564 - DEBUG - Directory 'code_library' already exists: ./code_library\n",
      "2025-01-10 18:25:10,575 - DEBUG - [1583482292.py:318] - Directory 'documents' already exists: ./documents\n",
      "2025-01-10 18:25:10,575 - DEBUG - [1583482292.py:318] - Directory 'documents' already exists: ./documents\n",
      "2025-01-10 18:25:10,575 - DEBUG - [1583482292.py:318] - Directory 'documents' already exists: ./documents\n",
      "2025-01-10 18:25:10,575 - DEBUG - [1583482292.py:318] - Directory 'documents' already exists: ./documents\n",
      "2025-01-10 18:25:10,575 - DEBUG - [1583482292.py:318] - Directory 'documents' already exists: ./documents\n",
      "2025-01-10 18:25:10,575 - DEBUG - [1583482292.py:318] - Directory 'documents' already exists: ./documents\n",
      "2025-01-10 18:25:10,575 - DEBUG - [1583482292.py:318] - Directory 'documents' already exists: ./documents\n",
      "2025-01-10 18:25:10,575 - DEBUG - [1583482292.py:318] - Directory 'documents' already exists: ./documents\n",
      "2025-01-10 18:25:10,575 - DEBUG - [1583482292.py:318] - Directory 'documents' already exists: ./documents\n",
      "2025-01-10 18:25:10,575 - DEBUG - [1583482292.py:318] - Directory 'documents' already exists: ./documents\n",
      "2025-01-10 18:25:10,575 - DEBUG - [1583482292.py:318] - Directory 'documents' already exists: ./documents\n",
      "2025-01-10 18:25:10,575 - DEBUG - [1583482292.py:318] - Directory 'documents' already exists: ./documents\n",
      "2025-01-10 18:25:10,575 - DEBUG - Directory 'documents' already exists: ./documents\n",
      "2025-01-10 18:25:10,585 - DEBUG - [1583482292.py:318] - Directory 'images' already exists: ./images\n",
      "2025-01-10 18:25:10,585 - DEBUG - [1583482292.py:318] - Directory 'images' already exists: ./images\n",
      "2025-01-10 18:25:10,585 - DEBUG - [1583482292.py:318] - Directory 'images' already exists: ./images\n",
      "2025-01-10 18:25:10,585 - DEBUG - [1583482292.py:318] - Directory 'images' already exists: ./images\n",
      "2025-01-10 18:25:10,585 - DEBUG - [1583482292.py:318] - Directory 'images' already exists: ./images\n",
      "2025-01-10 18:25:10,585 - DEBUG - [1583482292.py:318] - Directory 'images' already exists: ./images\n",
      "2025-01-10 18:25:10,585 - DEBUG - [1583482292.py:318] - Directory 'images' already exists: ./images\n",
      "2025-01-10 18:25:10,585 - DEBUG - [1583482292.py:318] - Directory 'images' already exists: ./images\n",
      "2025-01-10 18:25:10,585 - DEBUG - [1583482292.py:318] - Directory 'images' already exists: ./images\n",
      "2025-01-10 18:25:10,585 - DEBUG - [1583482292.py:318] - Directory 'images' already exists: ./images\n",
      "2025-01-10 18:25:10,585 - DEBUG - [1583482292.py:318] - Directory 'images' already exists: ./images\n",
      "2025-01-10 18:25:10,585 - DEBUG - [1583482292.py:318] - Directory 'images' already exists: ./images\n",
      "2025-01-10 18:25:10,585 - DEBUG - Directory 'images' already exists: ./images\n",
      "2025-01-10 18:25:10,598 - DEBUG - [1583482292.py:318] - Directory 'audio' already exists: ./audio\n",
      "2025-01-10 18:25:10,598 - DEBUG - [1583482292.py:318] - Directory 'audio' already exists: ./audio\n",
      "2025-01-10 18:25:10,598 - DEBUG - [1583482292.py:318] - Directory 'audio' already exists: ./audio\n",
      "2025-01-10 18:25:10,598 - DEBUG - [1583482292.py:318] - Directory 'audio' already exists: ./audio\n",
      "2025-01-10 18:25:10,598 - DEBUG - [1583482292.py:318] - Directory 'audio' already exists: ./audio\n",
      "2025-01-10 18:25:10,598 - DEBUG - [1583482292.py:318] - Directory 'audio' already exists: ./audio\n",
      "2025-01-10 18:25:10,598 - DEBUG - [1583482292.py:318] - Directory 'audio' already exists: ./audio\n",
      "2025-01-10 18:25:10,598 - DEBUG - [1583482292.py:318] - Directory 'audio' already exists: ./audio\n",
      "2025-01-10 18:25:10,598 - DEBUG - [1583482292.py:318] - Directory 'audio' already exists: ./audio\n",
      "2025-01-10 18:25:10,598 - DEBUG - [1583482292.py:318] - Directory 'audio' already exists: ./audio\n",
      "2025-01-10 18:25:10,598 - DEBUG - [1583482292.py:318] - Directory 'audio' already exists: ./audio\n",
      "2025-01-10 18:25:10,598 - DEBUG - [1583482292.py:318] - Directory 'audio' already exists: ./audio\n",
      "2025-01-10 18:25:10,598 - DEBUG - Directory 'audio' already exists: ./audio\n",
      "2025-01-10 18:25:10,607 - DEBUG - [1583482292.py:318] - Directory 'video' already exists: ./video\n",
      "2025-01-10 18:25:10,607 - DEBUG - [1583482292.py:318] - Directory 'video' already exists: ./video\n",
      "2025-01-10 18:25:10,607 - DEBUG - [1583482292.py:318] - Directory 'video' already exists: ./video\n",
      "2025-01-10 18:25:10,607 - DEBUG - [1583482292.py:318] - Directory 'video' already exists: ./video\n",
      "2025-01-10 18:25:10,607 - DEBUG - [1583482292.py:318] - Directory 'video' already exists: ./video\n",
      "2025-01-10 18:25:10,607 - DEBUG - [1583482292.py:318] - Directory 'video' already exists: ./video\n",
      "2025-01-10 18:25:10,607 - DEBUG - [1583482292.py:318] - Directory 'video' already exists: ./video\n",
      "2025-01-10 18:25:10,607 - DEBUG - [1583482292.py:318] - Directory 'video' already exists: ./video\n",
      "2025-01-10 18:25:10,607 - DEBUG - [1583482292.py:318] - Directory 'video' already exists: ./video\n",
      "2025-01-10 18:25:10,607 - DEBUG - [1583482292.py:318] - Directory 'video' already exists: ./video\n",
      "2025-01-10 18:25:10,607 - DEBUG - [1583482292.py:318] - Directory 'video' already exists: ./video\n",
      "2025-01-10 18:25:10,607 - DEBUG - [1583482292.py:318] - Directory 'video' already exists: ./video\n",
      "2025-01-10 18:25:10,607 - DEBUG - Directory 'video' already exists: ./video\n",
      "2025-01-10 18:25:10,621 - INFO - [1583482292.py:299] - Directory creation process completed.\n",
      "2025-01-10 18:25:10,621 - INFO - [1583482292.py:299] - Directory creation process completed.\n",
      "2025-01-10 18:25:10,621 - INFO - [1583482292.py:299] - Directory creation process completed.\n",
      "2025-01-10 18:25:10,621 - INFO - [1583482292.py:299] - Directory creation process completed.\n",
      "2025-01-10 18:25:10,621 - INFO - [1583482292.py:299] - Directory creation process completed.\n",
      "2025-01-10 18:25:10,621 - INFO - [1583482292.py:299] - Directory creation process completed.\n",
      "2025-01-10 18:25:10,621 - INFO - [1583482292.py:299] - Directory creation process completed.\n",
      "2025-01-10 18:25:10,621 - INFO - [1583482292.py:299] - Directory creation process completed.\n",
      "2025-01-10 18:25:10,621 - INFO - [1583482292.py:299] - Directory creation process completed.\n",
      "2025-01-10 18:25:10,621 - INFO - [1583482292.py:299] - Directory creation process completed.\n",
      "2025-01-10 18:25:10,621 - INFO - [1583482292.py:299] - Directory creation process completed.\n",
      "2025-01-10 18:25:10,621 - INFO - [1583482292.py:299] - Directory creation process completed.\n",
      "2025-01-10 18:25:10,621 - INFO - Directory creation process completed.\n",
      "2025-01-10 18:25:10,631 - DEBUG - [1583482292.py:287] - EnvironmentConfig initialized.\n",
      "2025-01-10 18:25:10,631 - DEBUG - [1583482292.py:287] - EnvironmentConfig initialized.\n",
      "2025-01-10 18:25:10,631 - DEBUG - [1583482292.py:287] - EnvironmentConfig initialized.\n",
      "2025-01-10 18:25:10,631 - DEBUG - [1583482292.py:287] - EnvironmentConfig initialized.\n",
      "2025-01-10 18:25:10,631 - DEBUG - [1583482292.py:287] - EnvironmentConfig initialized.\n",
      "2025-01-10 18:25:10,631 - DEBUG - [1583482292.py:287] - EnvironmentConfig initialized.\n",
      "2025-01-10 18:25:10,631 - DEBUG - [1583482292.py:287] - EnvironmentConfig initialized.\n",
      "2025-01-10 18:25:10,631 - DEBUG - [1583482292.py:287] - EnvironmentConfig initialized.\n",
      "2025-01-10 18:25:10,631 - DEBUG - [1583482292.py:287] - EnvironmentConfig initialized.\n",
      "2025-01-10 18:25:10,631 - DEBUG - [1583482292.py:287] - EnvironmentConfig initialized.\n",
      "2025-01-10 18:25:10,631 - DEBUG - [1583482292.py:287] - EnvironmentConfig initialized.\n",
      "2025-01-10 18:25:10,631 - DEBUG - [1583482292.py:287] - EnvironmentConfig initialized.\n",
      "2025-01-10 18:25:10,631 - DEBUG - EnvironmentConfig initialized.\n",
      "2025-01-10 18:25:10,643 - DEBUG - [1583482292.py:202] - AdaptiveThreadPoolExecutor initialized with initial_max_workers=4, min_workers=1, max_workers_limit=None\n",
      "2025-01-10 18:25:10,643 - DEBUG - [1583482292.py:202] - AdaptiveThreadPoolExecutor initialized with initial_max_workers=4, min_workers=1, max_workers_limit=None\n",
      "2025-01-10 18:25:10,643 - DEBUG - [1583482292.py:202] - AdaptiveThreadPoolExecutor initialized with initial_max_workers=4, min_workers=1, max_workers_limit=None\n",
      "2025-01-10 18:25:10,643 - DEBUG - [1583482292.py:202] - AdaptiveThreadPoolExecutor initialized with initial_max_workers=4, min_workers=1, max_workers_limit=None\n",
      "2025-01-10 18:25:10,643 - DEBUG - [1583482292.py:202] - AdaptiveThreadPoolExecutor initialized with initial_max_workers=4, min_workers=1, max_workers_limit=None\n",
      "2025-01-10 18:25:10,643 - DEBUG - [1583482292.py:202] - AdaptiveThreadPoolExecutor initialized with initial_max_workers=4, min_workers=1, max_workers_limit=None\n",
      "2025-01-10 18:25:10,643 - DEBUG - [1583482292.py:202] - AdaptiveThreadPoolExecutor initialized with initial_max_workers=4, min_workers=1, max_workers_limit=None\n",
      "2025-01-10 18:25:10,643 - DEBUG - [1583482292.py:202] - AdaptiveThreadPoolExecutor initialized with initial_max_workers=4, min_workers=1, max_workers_limit=None\n",
      "2025-01-10 18:25:10,643 - DEBUG - [1583482292.py:202] - AdaptiveThreadPoolExecutor initialized with initial_max_workers=4, min_workers=1, max_workers_limit=None\n",
      "2025-01-10 18:25:10,643 - DEBUG - [1583482292.py:202] - AdaptiveThreadPoolExecutor initialized with initial_max_workers=4, min_workers=1, max_workers_limit=None\n",
      "2025-01-10 18:25:10,643 - DEBUG - [1583482292.py:202] - AdaptiveThreadPoolExecutor initialized with initial_max_workers=4, min_workers=1, max_workers_limit=None\n",
      "2025-01-10 18:25:10,643 - DEBUG - [1583482292.py:202] - AdaptiveThreadPoolExecutor initialized with initial_max_workers=4, min_workers=1, max_workers_limit=None\n",
      "2025-01-10 18:25:10,643 - DEBUG - AdaptiveThreadPoolExecutor initialized with initial_max_workers=4, min_workers=1, max_workers_limit=None\n",
      "2025-01-10 18:25:10,661 - DEBUG - [1583482292.py:337] - ModelManager initialized.\n",
      "2025-01-10 18:25:10,661 - DEBUG - [1583482292.py:337] - ModelManager initialized.\n",
      "2025-01-10 18:25:10,661 - DEBUG - [1583482292.py:337] - ModelManager initialized.\n",
      "2025-01-10 18:25:10,661 - DEBUG - [1583482292.py:337] - ModelManager initialized.\n",
      "2025-01-10 18:25:10,661 - DEBUG - [1583482292.py:337] - ModelManager initialized.\n",
      "2025-01-10 18:25:10,661 - DEBUG - [1583482292.py:337] - ModelManager initialized.\n",
      "2025-01-10 18:25:10,661 - DEBUG - [1583482292.py:337] - ModelManager initialized.\n",
      "2025-01-10 18:25:10,661 - DEBUG - [1583482292.py:337] - ModelManager initialized.\n",
      "2025-01-10 18:25:10,661 - DEBUG - [1583482292.py:337] - ModelManager initialized.\n",
      "2025-01-10 18:25:10,661 - DEBUG - [1583482292.py:337] - ModelManager initialized.\n",
      "2025-01-10 18:25:10,661 - DEBUG - [1583482292.py:337] - ModelManager initialized.\n",
      "2025-01-10 18:25:10,661 - DEBUG - [1583482292.py:337] - ModelManager initialized.\n",
      "2025-01-10 18:25:10,661 - DEBUG - ModelManager initialized.\n"
     ]
    },
    {
     "ename": "DocstringParsingException",
     "evalue": "Cannot generate JSON schema for visit_webpage because the docstring has no description for the argument 'url'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDocstringParsingException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 457\u001b[0m\n\u001b[0;32m    453\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError processing feedback for tool \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtool_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m feedback_content\n\u001b[1;32m--> 457\u001b[0m \u001b[38;5;129;43m@tool\u001b[39;49m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mvisit_webpage\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./cache\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisk_io_manager\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDiskIOManager\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;124;43;03m    Visits a webpage, saves the raw content to disk, converts it to markdown, and returns the markdown string.\u001b[39;49;00m\n\u001b[0;32m    461\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;124;43;03m        str: The markdown-formatted content of the webpage, or an error message if fetching fails.\u001b[39;49;00m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;124;43;03m    \"\"\"\u001b[39;49;00m\n\u001b[0;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtool_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvisit_webpage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\smolagents\\tools.py:929\u001b[0m, in \u001b[0;36mtool\u001b[1;34m(tool_function)\u001b[0m\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtool\u001b[39m(tool_function: Callable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tool:\n\u001b[0;32m    922\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;124;03m    Converts a function into an instance of a Tool subclass.\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;124;03m        Should also have a docstring description including an 'Args:' part where each argument is described.\u001b[39;00m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 929\u001b[0m     parameters \u001b[38;5;241m=\u001b[39m \u001b[43mget_json_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_function\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m parameters:\n\u001b[0;32m    931\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TypeHintParsingException(\n\u001b[0;32m    932\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTool return type not found: make sure your function has a return type hint!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    933\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\chat_template_utils.py:328\u001b[0m, in \u001b[0;36mget_json_schema\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg, schema \u001b[38;5;129;01min\u001b[39;00m json_schema[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproperties\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arg \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m param_descriptions:\n\u001b[1;32m--> 328\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m DocstringParsingException(\n\u001b[0;32m    329\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot generate JSON schema for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because the docstring has no description for the argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    330\u001b[0m         )\n\u001b[0;32m    331\u001b[0m     desc \u001b[38;5;241m=\u001b[39m param_descriptions[arg]\n\u001b[0;32m    332\u001b[0m     enum_choices \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m(choices:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*(.*?)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*$\u001b[39m\u001b[38;5;124m\"\u001b[39m, desc, flags\u001b[38;5;241m=\u001b[39mre\u001b[38;5;241m.\u001b[39mIGNORECASE)\n",
      "\u001b[1;31mDocstringParsingException\u001b[0m: Cannot generate JSON schema for visit_webpage because the docstring has no description for the argument 'url'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import abc\n",
    "import hashlib\n",
    "from urllib.parse import urlparse\n",
    "from markdownify import markdownify\n",
    "from requests.exceptions import RequestException\n",
    "from smolagents import tool\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Dict, Optional, Iterator, TYPE_CHECKING, List\n",
    "import psutil\n",
    "from collections import deque\n",
    "import time\n",
    "import subprocess\n",
    "\n",
    "# Configure adaptive logging with dynamic levels\n",
    "logger: logging.Logger = logging.getLogger(__name__)  # Get the logger for this module\n",
    "logger.setLevel(logging.DEBUG)  # Set the base logging level to DEBUG\n",
    "log_handler: logging.StreamHandler = logging.StreamHandler()  # Create a handler to output logs to the console\n",
    "log_formatter: logging.Formatter = logging.Formatter('%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s')  # Define the log message format\n",
    "log_handler.setFormatter(log_formatter)  # Set the formatter for the handler\n",
    "logger.addHandler(log_handler)  # Add the handler to the logger\n",
    "\n",
    "class DiskIOManager:\n",
    "    \"\"\"\n",
    "    Manages disk-based read and write operations in chunks to handle large files efficiently.\n",
    "\n",
    "    This class provides methods for writing data to disk, reading data from disk,\n",
    "    and reading data from disk in chunks, optimizing for memory usage and scalability.\n",
    "    \"\"\"\n",
    "    def __init__(self, chunk_size: int = 1024 * 1024) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the DiskIOManager with a specified chunk size.\n",
    "\n",
    "        Args:\n",
    "            chunk_size (int): The size of each chunk for read and write operations in bytes.\n",
    "                              Defaults to 1MB.\n",
    "        \"\"\"\n",
    "        self.chunk_size: int = chunk_size\n",
    "        logger.debug(f\"DiskIOManager initialized with chunk size: {self.chunk_size} bytes.\")\n",
    "\n",
    "    def write_data_to_disk(self, file_path: str, data: str, mode: str = 'w') -> None:\n",
    "        \"\"\"\n",
    "        Writes data to disk in chunks to handle large strings efficiently.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the file where data will be written.\n",
    "            data (str): The string data to write to the file.\n",
    "            mode (str): The file opening mode (e.g., 'w' for write, 'a' for append). Defaults to 'w'.\n",
    "\n",
    "        Raises:\n",
    "            IOError: If an error occurs during the file writing process.\n",
    "        \"\"\"\n",
    "        logger.debug(f\"Writing data to disk in chunks: file_path='{file_path}', mode='{mode}', chunk_size='{self.chunk_size}'\")\n",
    "        try:\n",
    "            with open(file_path, mode, encoding='utf-8') as f:\n",
    "                for i in range(0, len(data), self.chunk_size):\n",
    "                    chunk: str = data[i:i + self.chunk_size]\n",
    "                    f.write(chunk)\n",
    "            logger.info(f\"Successfully wrote data to disk: {file_path}\")\n",
    "        except IOError as e:\n",
    "            logger.error(f\"Error writing to file '{file_path}': {e}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "    def read_data_from_disk(self, file_path: str, mode: str = 'r') -> str:\n",
    "        \"\"\"\n",
    "        Reads data from disk in chunks and returns the entire content as a string.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the file to read from.\n",
    "            mode (str): The file opening mode (e.g., 'r' for read). Defaults to 'r'.\n",
    "\n",
    "        Returns:\n",
    "            str: The entire content of the file.\n",
    "\n",
    "        Raises:\n",
    "            IOError: If an error occurs during the file reading process.\n",
    "        \"\"\"\n",
    "        logger.debug(f\"Reading data from disk in chunks: file_path='{file_path}', mode='{mode}', chunk_size='{self.chunk_size}'\")\n",
    "        content: str = \"\"\n",
    "        try:\n",
    "            with open(file_path, mode, encoding='utf-8') as f:\n",
    "                while True:\n",
    "                    chunk: str = f.read(self.chunk_size)\n",
    "                    if not chunk:\n",
    "                        break\n",
    "                    content += chunk\n",
    "            logger.info(f\"Successfully read data from disk: {file_path}\")\n",
    "            return content\n",
    "        except IOError as e:\n",
    "            logger.error(f\"Error reading from file '{file_path}': {e}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "    def read_data_from_disk_in_chunks(self, file_path: str, mode: str = 'r') -> Iterator[str]:\n",
    "        \"\"\"\n",
    "        Reads data from disk in chunks and yields each chunk as an iterator.\n",
    "\n",
    "        This method is memory-efficient for processing large files, as it does not load\n",
    "        the entire file into memory at once.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the file to read from.\n",
    "            mode (str): The file opening mode (e.g., 'r' for read). Defaults to 'r'.\n",
    "\n",
    "        Yields:\n",
    "            str: A chunk of data read from the file.\n",
    "\n",
    "        Raises:\n",
    "            IOError: If an error occurs during the file reading process.\n",
    "        \"\"\"\n",
    "        logger.debug(f\"Reading data from disk in chunks (iterator): file_path='{file_path}', mode='{mode}', chunk_size='{self.chunk_size}'\")\n",
    "        try:\n",
    "            with open(file_path, mode, encoding='utf-8') as f:\n",
    "                while True:\n",
    "                    chunk: str = f.read(self.chunk_size)\n",
    "                    if not chunk:\n",
    "                        break\n",
    "                    yield chunk\n",
    "            logger.info(f\"Finished reading data from disk in chunks (iterator): {file_path}\")\n",
    "        except IOError as e:\n",
    "            logger.error(f\"Error reading from file '{file_path}': {e}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "class ResourceManager:\n",
    "    \"\"\"\n",
    "    Manages and monitors system resources (CPU, disk, memory) for adaptive processing.\n",
    "\n",
    "    This class tracks resource usage and provides methods to determine if concurrency\n",
    "    should be reduced based on historical resource consumption.\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Initializes the ResourceManager with default thresholds and history size.\"\"\"\n",
    "        self.cpu_threshold: float = 80.0  # CPU usage threshold (%)\n",
    "        self.disk_threshold: float = 80.0  # Disk usage threshold (%)\n",
    "        self.memory_threshold: float = 80.0  # Memory usage threshold (%)\n",
    "        self.history_size: int = 10  # Number of recent measurements to consider\n",
    "        self.cpu_history: deque[float] = deque(maxlen=self.history_size)  # History of CPU usage\n",
    "        self.disk_history: deque[float] = deque(maxlen=self.history_size)  # History of disk usage\n",
    "        self.memory_history: deque[float] = deque(maxlen=self.history_size)  # History of memory usage\n",
    "        logger.debug(\"ResourceManager initialized.\")\n",
    "\n",
    "    def get_resource_usage(self) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Returns current CPU, disk, and memory usage as a dictionary.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: A dictionary containing CPU, disk, and memory usage percentages.\n",
    "        \"\"\"\n",
    "        cpu_percent: float = psutil.cpu_percent()\n",
    "        disk_usage: float = psutil.disk_usage('/').percent  # Assuming root partition\n",
    "        memory_usage: float = psutil.virtual_memory().percent\n",
    "        self.cpu_history.append(cpu_percent)\n",
    "        self.disk_history.append(disk_usage)\n",
    "        self.memory_history.append(memory_usage)\n",
    "        logger.debug(f\"Current resource usage: CPU={cpu_percent}%, Disk={disk_usage}%, Memory={memory_usage}%\")\n",
    "        return {\"cpu_percent\": cpu_percent, \"disk_percent\": disk_usage, \"memory_percent\": memory_usage}\n",
    "\n",
    "    def should_reduce_concurrency(self) -> bool:\n",
    "        \"\"\"\n",
    "        Determines if concurrency should be reduced based on the average resource usage history.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if concurrency should be reduced, False otherwise.\n",
    "        \"\"\"\n",
    "        avg_cpu: float = sum(self.cpu_history) / len(self.cpu_history) if self.cpu_history else 0\n",
    "        avg_disk: float = sum(self.disk_history) / len(self.disk_history) if self.disk_history else 0\n",
    "        avg_memory: float = sum(self.memory_history) / len(self.memory_history) if self.memory_history else 0\n",
    "        should_reduce: bool = avg_cpu > self.cpu_threshold or avg_disk > self.disk_threshold or avg_memory > self.memory_threshold\n",
    "        logger.debug(f\"Average resource usage: CPU={avg_cpu}%, Disk={avg_disk}%, Memory={avg_memory}%. Reduce concurrency: {should_reduce}\")\n",
    "        return should_reduce\n",
    "\n",
    "class AdaptiveThreadPoolExecutor(ThreadPoolExecutor):\n",
    "    \"\"\"\n",
    "    A thread pool executor that dynamically adjusts the number of worker threads\n",
    "    based on system resource usage.\n",
    "\n",
    "    This class helps in optimizing resource utilization by scaling the number of threads\n",
    "    up or down as needed.\n",
    "    \"\"\"\n",
    "    def __init__(self, initial_max_workers: int, resource_manager: 'ResourceManager', min_workers: int = 1, max_workers_limit: Optional[int] = None) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the AdaptiveThreadPoolExecutor.\n",
    "\n",
    "        Args:\n",
    "            initial_max_workers (int): The initial maximum number of worker threads.\n",
    "            resource_manager (ResourceManager): An instance of ResourceManager to monitor system resources.\n",
    "            min_workers (int): The minimum number of worker threads. Defaults to 1.\n",
    "            max_workers_limit (Optional[int]): The hard limit for the maximum number of worker threads.\n",
    "                                               If None, it defaults to the initial_max_workers.\n",
    "        \"\"\"\n",
    "        super().__init__(max_workers=initial_max_workers)\n",
    "        self.resource_manager: ResourceManager = resource_manager\n",
    "        self.initial_max_workers: int = initial_max_workers\n",
    "        self.min_workers: int = min_workers\n",
    "        self.max_workers_limit: Optional[int] = max_workers_limit\n",
    "        self._last_resize_time: float = time.time()\n",
    "        self._resize_interval: int = 5  # seconds\n",
    "        logger.debug(f\"AdaptiveThreadPoolExecutor initialized with initial_max_workers={initial_max_workers}, min_workers={min_workers}, max_workers_limit={max_workers_limit}\")\n",
    "\n",
    "    def adjust_pool_size(self) -> None:\n",
    "        \"\"\"\n",
    "        Dynamically adjusts the pool size based on resource usage.\n",
    "\n",
    "        This method checks the resource manager and either increases or decreases the\n",
    "        number of worker threads based on current and historical resource consumption.\n",
    "        \"\"\"\n",
    "        if time.time() - self._last_resize_time < self._resize_interval:\n",
    "            return\n",
    "\n",
    "        if self.resource_manager.should_reduce_concurrency():\n",
    "            if self._max_workers > self.min_workers:\n",
    "                self._max_workers = max(self.min_workers, self._max_workers // 2)\n",
    "                logger.warning(f\"Reducing thread pool size to {self._max_workers} due to high resource usage.\")\n",
    "                self._resize_pool()\n",
    "                self._last_resize_time = time.time()\n",
    "        elif self._max_workers < (self.max_workers_limit if self.max_workers_limit is not None else self.initial_max_workers):\n",
    "            target_workers: int = min((self.max_workers_limit if self.max_workers_limit is not None else self.initial_max_workers), self._max_workers * 2)\n",
    "            if target_workers > self._max_workers:\n",
    "                self._max_workers = target_workers\n",
    "                logger.info(f\"Increasing thread pool size to {self._max_workers}.\")\n",
    "                self._resize_pool()\n",
    "                self._last_resize_time = time.time()\n",
    "\n",
    "    def _resize_pool(self) -> None:\n",
    "        \"\"\"\n",
    "        Internal method to resize the thread pool by creating new threads or stopping existing ones.\n",
    "\n",
    "        Note: This is a simplified approach. A more robust solution might involve creating\n",
    "        a new executor and transferring tasks.\n",
    "        \"\"\"\n",
    "        # This simplified approach might not be the most efficient or robust in all scenarios.\n",
    "        # Consider more advanced techniques for production environments.\n",
    "        all_threads = list(self._threads)\n",
    "        for thread in all_threads:\n",
    "            if thread.is_alive():\n",
    "                # This is a forceful way to stop threads and might lead to issues if threads are in the middle of execution.\n",
    "                # A better approach would involve signaling threads to finish their work and exit gracefully.\n",
    "                try:\n",
    "                    thread.join(timeout=0.1)  # Give a short timeout for the thread to join\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error while trying to join thread: {e}\", exc_info=True)\n",
    "\n",
    "        self._threads.clear()\n",
    "        self._work_queue.queue.clear()\n",
    "        for _ in range(self._max_workers):\n",
    "            self._start_thread()\n",
    "        logger.debug(f\"Thread pool resized to {self._max_workers} workers.\")\n",
    "\n",
    "class EnvironmentConfig:\n",
    "    \"\"\"\n",
    "    Handles environment configuration and directory setup with disk-based logging.\n",
    "\n",
    "    This class manages settings like the Hugging Face token and creates necessary\n",
    "    directories for the application.\n",
    "    \"\"\"\n",
    "    def __init__(self, hf_token: Optional[str] = None, directories: Optional[Dict[str, str]] = None, disk_io_manager: 'DiskIOManager' = None) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the EnvironmentConfig.\n",
    "\n",
    "        Args:\n",
    "            hf_token (Optional[str]): The Hugging Face API token. Defaults to the 'HF_TOKEN' environment variable or a fallback.\n",
    "            directories (Optional[Dict[str, str]]): A dictionary of directory names and their paths.\n",
    "            disk_io_manager (Optional[DiskIOManager]): An instance of DiskIOManager for disk operations.\n",
    "        \"\"\"\n",
    "        self.hf_token: str = hf_token or os.getenv('HF_TOKEN', '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')\n",
    "        self.directories: Dict[str, str] = directories or {\n",
    "            'saved_model': './saved_models',\n",
    "            'datasets': './datasets',\n",
    "            'checkpoints': './checkpoints',\n",
    "            'cache': './cache',\n",
    "            'logs': './logs',\n",
    "            'offload': './offload',\n",
    "            'memory': './memory',\n",
    "            'code_library': './code_library',\n",
    "            'documents': './documents',\n",
    "            'images': './images',\n",
    "            'audio': './audio',\n",
    "            'video': './video'\n",
    "        }\n",
    "        self.disk_io_manager: DiskIOManager = disk_io_manager\n",
    "        self._validate_environment()\n",
    "        self._create_directories()\n",
    "        logger.debug(\"EnvironmentConfig initialized.\")\n",
    "\n",
    "    def _validate_environment(self) -> None:\n",
    "        \"\"\"Validates the environment configuration, logging warnings for missing settings.\"\"\"\n",
    "        if not self.hf_token:\n",
    "            logger.warning(\"HF_TOKEN environment variable is not set. Using default token.\")\n",
    "\n",
    "    def _create_directories(self, ) -> None:\n",
    "        \"\"\"Creates necessary directories if they do not exist, leveraging disk I/O.\"\"\"\n",
    "        logger.info(\"Starting directory creation process.\")\n",
    "        for dir_name, path in self.directories.items():\n",
    "            self._create_directory(dir_name, path)\n",
    "        logger.info(\"Directory creation process completed.\")\n",
    "\n",
    "    def _create_directory(self, dir_name: str, path: str) -> None:\n",
    "        \"\"\"\n",
    "        Creates a single directory with detailed logging and error handling.\n",
    "\n",
    "        Args:\n",
    "            dir_name (str): The name of the directory.\n",
    "            path (str): The path to the directory.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(path):\n",
    "            logger.debug(f\"Directory '{dir_name}' does not exist. Attempting to create: {path}\")\n",
    "            try:\n",
    "                os.makedirs(path, exist_ok=True)\n",
    "                logger.info(f\"Successfully created directory: {path}\")\n",
    "            except OSError as e:\n",
    "                logger.error(f\"Failed to create directory '{dir_name}' at '{path}': {e}\", exc_info=True)\n",
    "                raise\n",
    "        else:\n",
    "            logger.debug(f\"Directory '{dir_name}' already exists: {path}\")\n",
    "\n",
    "class ModelManager:\n",
    "    \"\"\"\n",
    "    Manages downloading and saving models from Hugging Face, prioritizing disk-based\n",
    "    operations and utilizing adaptive concurrency.\n",
    "    \"\"\"\n",
    "    def __init__(self, disk_io_manager: 'DiskIOManager' = None, resource_manager: 'ResourceManager' = None, thread_pool: 'AdaptiveThreadPoolExecutor' = None) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the ModelManager.\n",
    "\n",
    "        Args:\n",
    "            disk_io_manager (Optional[DiskIOManager]): An instance of DiskIOManager for disk operations.\n",
    "            resource_manager (Optional[ResourceManager]): An instance of ResourceManager for monitoring system resources.\n",
    "            thread_pool (Optional[AdaptiveThreadPoolExecutor]): An instance of AdaptiveThreadPoolExecutor for concurrent downloads.\n",
    "        \"\"\"\n",
    "        self.disk_io_manager: DiskIOManager = disk_io_manager or DiskIOManager()\n",
    "        self.resource_manager: ResourceManager = resource_manager or ResourceManager()\n",
    "        self.thread_pool: AdaptiveThreadPoolExecutor = thread_pool or AdaptiveThreadPoolExecutor(initial_max_workers=2, resource_manager=self.resource_manager)\n",
    "        logger.debug(\"ModelManager initialized.\")\n",
    "\n",
    "    def download_and_save_model(self, model_name: str, save_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Downloads a model and its tokenizer from Hugging Face and saves them locally.\n",
    "\n",
    "        This process prioritizes saving configurations to disk first and uses adaptive\n",
    "        concurrency to manage resource usage during download.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the model to download from Hugging Face.\n",
    "            save_path (str): The local path where the model and tokenizer will be saved.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If any error occurs during the download or save process.\n",
    "        \"\"\"\n",
    "        log_prefix: str = f\"download_and_save_model(model_name='{model_name}', save_path='{save_path}')\"\n",
    "        logger.info(f\"{log_prefix}: Starting model download and save process.\")\n",
    "\n",
    "        try:\n",
    "            # Save model config to disk first\n",
    "            config_path: str = os.path.join(save_path, 'config')\n",
    "            os.makedirs(config_path, exist_ok=True)\n",
    "            config = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=config_path, local_files_only=False)\n",
    "            config.save_pretrained(config_path)\n",
    "            logger.debug(f\"{log_prefix}: Model configuration saved to disk at {config_path}\")\n",
    "\n",
    "            # Save tokenizer config to disk first\n",
    "            tokenizer_path: str = os.path.join(save_path, 'tokenizer')\n",
    "            os.makedirs(tokenizer_path, exist_ok=True)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=tokenizer_path, local_files_only=False)\n",
    "            tokenizer.save_pretrained(tokenizer_path)\n",
    "            logger.debug(f\"{log_prefix}: Tokenizer configuration saved to disk at {tokenizer_path}\")\n",
    "\n",
    "            # Offload actual model weights download and save to disk\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=save_path, local_files_only=False)\n",
    "            model.save_pretrained(save_path)\n",
    "            logger.info(f\"{log_prefix}: Model and tokenizer successfully downloaded and saved to disk at {save_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.critical(f\"{log_prefix}: Critical failure during model download and save: {e}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "# Initialize components\n",
    "resource_manager: ResourceManager = ResourceManager()\n",
    "disk_io_manager: DiskIOManager = DiskIOManager()\n",
    "env_config: EnvironmentConfig = EnvironmentConfig(disk_io_manager=disk_io_manager)\n",
    "adaptive_thread_pool: AdaptiveThreadPoolExecutor = AdaptiveThreadPoolExecutor(initial_max_workers=4, resource_manager=resource_manager)\n",
    "model_manager: ModelManager = ModelManager(disk_io_manager=disk_io_manager, resource_manager=resource_manager, thread_pool=adaptive_thread_pool)\n",
    "\n",
    "# Constants\n",
    "MODEL_ID: str = \"Qwen/Qwen2.5-Coder-0.5B-Instruct\"\n",
    "WEB_SEARCH_URL: str = \"https://www.google.com/search?q={query}\"\n",
    "#===============================================================================#\n",
    "#                       Digital Intelligence Tools                              #\n",
    "#           Purpose-Built Framework for Digital Entity Operations               #\n",
    "#              Optimized for Autonomous Agent Interactions                      #\n",
    "#===============================================================================#\n",
    "\n",
    "# Centralized metrics and logging file\n",
    "METRICS_LOG_FILE: str = \"tool_metrics.log\"\n",
    "\n",
    "def log_tool_metrics(tool_name: str, start_time: float, end_time: float, inputs: Dict, outputs: str, error: Optional[str] = None) -> None:\n",
    "    \"\"\"Logs detailed metrics for each tool execution.\"\"\"\n",
    "    duration: float = end_time - start_time\n",
    "    metrics: Dict = {\n",
    "        \"tool_name\": tool_name,\n",
    "        \"start_time\": start_time,\n",
    "        \"end_time\": end_time,\n",
    "        \"duration\": duration,\n",
    "        \"inputs\": inputs,\n",
    "        \"outputs\": outputs,\n",
    "        \"error\": error\n",
    "    }\n",
    "    try:\n",
    "        with open(METRICS_LOG_FILE, \"a\") as f:\n",
    "            json.dump(metrics, f)\n",
    "            f.write(\"\\n\")\n",
    "        logger.debug(f\"Metrics for '{tool_name}' logged to {METRICS_LOG_FILE}\")\n",
    "    except IOError as e:\n",
    "        logger.error(f\"Error logging metrics for '{tool_name}': {e}\", exc_info=True)\n",
    "\n",
    "def get_tool_feedback(tool_name: str) -> str:\n",
    "    \"\"\"Retrieves and processes feedback for a specific tool.\"\"\"\n",
    "    feedback_content: str = \"\"\n",
    "    try:\n",
    "        with open(METRICS_LOG_FILE, \"r\") as f:\n",
    "            all_metrics = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "        tool_metrics = [m for m in all_metrics if m[\"tool_name\"] == tool_name]\n",
    "        if not tool_metrics:\n",
    "            return f\"No metrics found for tool '{tool_name}'.\"\n",
    "\n",
    "        # Basic analysis - can be extended with more advanced NLP techniques\n",
    "        total_executions: int = len(tool_metrics)\n",
    "        successful_executions: int = sum(1 for m in tool_metrics if not m.get(\"error\"))\n",
    "        error_rate: float = (1 - successful_executions / total_executions) * 100 if total_executions > 0 else 0\n",
    "\n",
    "        feedback_content += f\"Feedback for tool '{tool_name}':\\n\"\n",
    "        feedback_content += f\"Total executions: {total_executions}\\n\"\n",
    "        feedback_content += f\"Successful executions: {successful_executions}\\n\"\n",
    "        feedback_content += f\"Error rate: {error_rate:.2f}%\\n\"\n",
    "\n",
    "        if error_rate > 0:\n",
    "            error_messages = [m[\"error\"] for m in tool_metrics if m.get(\"error\")]\n",
    "            feedback_content += f\"Common errors:\\n\"\n",
    "            for error in set(error_messages):\n",
    "                feedback_content += f\"- {error}\\n\"\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        return f\"Metrics log file '{METRICS_LOG_FILE}' not found.\"\n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"Error decoding JSON from metrics log: {e}\", exc_info=True)\n",
    "        return f\"Error reading metrics for tool '{tool_name}'.\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error getting feedback for tool '{tool_name}': {e}\", exc_info=True)\n",
    "        return f\"Error processing feedback for tool '{tool_name}'.\"\n",
    "\n",
    "    return feedback_content\n",
    "\n",
    "@tool\n",
    "def visit_webpage(url: str, cache_dir: str = './cache', disk_io_manager: Optional['DiskIOManager'] = None) -> str:\n",
    "    \"\"\"\n",
    "    Visits a webpage, saves the raw content to disk, converts it to markdown, and returns the markdown string.\n",
    "\n",
    "    This function prioritizes disk offloading to handle large web pages efficiently.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the webpage to visit.\n",
    "        cache_dir (str): The directory to store the raw webpage content. Defaults to './cache'.\n",
    "        disk_io_manager (Optional[DiskIOManager]): An instance of DiskIOManager for disk operations.\n",
    "\n",
    "    Returns:\n",
    "        str: The markdown-formatted content of the webpage, or an error message if fetching fails.\n",
    "    \"\"\"\n",
    "    tool_name: str = \"visit_webpage\"\n",
    "    start_time: float = time.time()\n",
    "    log_prefix: str = f\"{tool_name}(url='{url}')\"\n",
    "    logger.info(f\"{log_prefix}: Attempting to visit webpage.\")\n",
    "    disk_io = disk_io_manager or DiskIOManager()\n",
    "    output: str = \"\"\n",
    "    error_msg: Optional[str] = None\n",
    "    try:\n",
    "        response: requests.Response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "\n",
    "        # Save raw content to disk immediately in chunks\n",
    "        raw_content_path: str = os.path.join(cache_dir, f\"{urlparse(url).netloc}_{hashlib.md5(url.encode()).hexdigest()[:8]}.html\")\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        disk_io.write_data_to_disk(raw_content_path, response.text)\n",
    "        logger.debug(f\"{log_prefix}: Raw content saved to disk: {raw_content_path}\")\n",
    "\n",
    "        # Process content from disk in chunks and convert to markdown\n",
    "        markdown_chunks: List[str] = []\n",
    "        for chunk in disk_io.read_data_from_disk_in_chunks(raw_content_path):\n",
    "            markdown_chunk: str = markdownify(chunk).strip()\n",
    "            markdown_chunks.append(markdown_chunk)\n",
    "        markdown_content: str = \"\\n\\n\".join(markdown_chunks)\n",
    "        markdown_content = re.sub(r\"\\n{3,}\", \"\\n\\n\", markdown_content)  # Normalize multiple newlines\n",
    "\n",
    "        output = markdown_content\n",
    "        logger.info(f\"{log_prefix}: Successfully fetched and converted webpage content.\")\n",
    "    except RequestException as e:\n",
    "        error_msg = f\"Error fetching the webpage: {e}\"\n",
    "        logger.error(f\"{log_prefix}: RequestException occurred while accessing {url}: {e}\")\n",
    "        output = error_msg\n",
    "    except Exception as e:\n",
    "        error_msg = f\"An unexpected error occurred: {e}\"\n",
    "        logger.critical(f\"{log_prefix}: An unexpected error occurred while accessing {url}: {e}\", exc_info=True)\n",
    "        output = error_msg\n",
    "    finally:\n",
    "        end_time: float = time.time()\n",
    "        log_tool_metrics(tool_name, start_time, end_time, {\"url\": url, \"cache_dir\": cache_dir}, output, error_msg)\n",
    "        return output\n",
    "\n",
    "@tool\n",
    "def execute_python_code(code_string: str) -> str:\n",
    "    \"\"\"\n",
    "    Executes a string of Python code and returns the output.\n",
    "\n",
    "    Args:\n",
    "        code_string (str): The Python code to execute.\n",
    "\n",
    "    Returns:\n",
    "        str: The output of the executed code, or an error message.\n",
    "    \"\"\"\n",
    "    tool_name: str = \"execute_python_code\"\n",
    "    start_time: float = time.time()\n",
    "    log_prefix: str = f\"{tool_name}(code_string='{code_string[:50]}...')\"\n",
    "    logger.info(f\"{log_prefix}: Attempting to execute Python code.\")\n",
    "    output: str = \"\"\n",
    "    error_msg: Optional[str] = None\n",
    "    try:\n",
    "        process = subprocess.Popen(['python', '-c', code_string],\n",
    "                                   stdout=subprocess.PIPE,\n",
    "                                   stderr=subprocess.PIPE,\n",
    "                                   text=True)\n",
    "        stdout, stderr = process.communicate(timeout=15)  # Add a timeout to prevent indefinite execution\n",
    "        if stderr:\n",
    "            error_msg = f\"Error executing code:\\n{stderr}\"\n",
    "            logger.error(f\"{log_prefix}: Error executing code: {stderr}\")\n",
    "            output = error_msg\n",
    "        else:\n",
    "            output = stdout\n",
    "            logger.info(f\"{log_prefix}: Successfully executed code.\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        error_msg = \"Error: Code execution timed out.\"\n",
    "        logger.error(f\"{log_prefix}: Code execution timed out.\")\n",
    "        output = error_msg\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error executing code: {e}\"\n",
    "        logger.error(f\"{log_prefix}: Unexpected error executing code: {e}\", exc_info=True)\n",
    "        output = error_msg\n",
    "    finally:\n",
    "        end_time: float = time.time()\n",
    "        log_tool_metrics(tool_name, start_time, end_time, {\"code_string\": code_string}, output, error_msg)\n",
    "        return output\n",
    "\n",
    "@tool\n",
    "def read_file(file_path: str, disk_io_manager: Optional['DiskIOManager'] = None) -> str:\n",
    "    \"\"\"\n",
    "    Reads the content of a file from disk.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file to read.\n",
    "        disk_io_manager (Optional[DiskIOManager]): An instance of DiskIOManager for disk operations.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the file, or an error message.\n",
    "    \"\"\"\n",
    "    tool_name: str = \"read_file\"\n",
    "    start_time: float = time.time()\n",
    "    log_prefix: str = f\"{tool_name}(file_path='{file_path}')\"\n",
    "    logger.info(f\"{log_prefix}: Attempting to read file.\")\n",
    "    disk_io = disk_io_manager or DiskIOManager()\n",
    "    output: str = \"\"\n",
    "    error_msg: Optional[str] = None\n",
    "    try:\n",
    "        content = disk_io.read_data_from_disk(file_path, mode='r')\n",
    "        output = content\n",
    "        logger.info(f\"{log_prefix}: Successfully read file.\")\n",
    "    except IOError as e:\n",
    "        error_msg = f\"Error reading file: {e}\"\n",
    "        logger.error(f\"{log_prefix}: Error reading file: {e}\", exc_info=True)\n",
    "        output = error_msg\n",
    "    finally:\n",
    "        end_time: float = time.time()\n",
    "        log_tool_metrics(tool_name, start_time, end_time, {\"file_path\": file_path}, output, error_msg)\n",
    "        return output\n",
    "\n",
    "@tool\n",
    "def write_file(file_path: str, content: str, mode: str = 'w', disk_io_manager: Optional['DiskIOManager'] = None) -> None:\n",
    "    \"\"\"\n",
    "    Writes content to a file on disk.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file to write.\n",
    "        content (str): The content to write to the file.\n",
    "        mode (str): The file opening mode (e.g., 'w' for write, 'a' for append). Defaults to 'w'.\n",
    "        disk_io_manager (Optional[DiskIOManager]): An instance of DiskIOManager for disk operations.\n",
    "    \"\"\"\n",
    "    tool_name: str = \"write_file\"\n",
    "    start_time: float = time.time()\n",
    "    log_prefix: str = f\"{tool_name}(file_path='{file_path}', mode='{mode}')\"\n",
    "    logger.info(f\"{log_prefix}: Attempting to write to file.\")\n",
    "    disk_io = disk_io_manager or DiskIOManager()\n",
    "    output: str = \"\"\n",
    "    error_msg: Optional[str] = None\n",
    "    try:\n",
    "        disk_io.write_data_to_disk(file_path, content, mode=mode)\n",
    "        logger.info(f\"{log_prefix}: Successfully wrote to file.\")\n",
    "    except IOError as e:\n",
    "        error_msg = f\"Error writing to file: {e}\"\n",
    "        logger.error(f\"{log_prefix}: Error writing to file: {e}\", exc_info=True)\n",
    "        raise\n",
    "    finally:\n",
    "        end_time: float = time.time()\n",
    "        log_tool_metrics(tool_name, start_time, end_time, {\"file_path\": file_path, \"content\": content, \"mode\": mode}, output, error_msg)\n",
    "\n",
    "@tool\n",
    "def search_internet(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Searches the internet for the given query using a simplified approach.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query.\n",
    "\n",
    "    Returns:\n",
    "        str: A summary of the search results or an error message.\n",
    "    \"\"\"\n",
    "    tool_name: str = \"search_internet\"\n",
    "    start_time: float = time.time()\n",
    "    log_prefix: str = f\"{tool_name}(query='{query}')\"\n",
    "    logger.info(f\"{log_prefix}: Attempting to search the internet.\")\n",
    "    output: str = \"\"\n",
    "    error_msg: Optional[str] = None\n",
    "    try:\n",
    "        from duckduckgo_search import ddg\n",
    "        results = ddg(query, max_results=5)\n",
    "        if results:\n",
    "            summary = \"\\n\".join([f\"Title: {r['title']}\\nLink: {r['href']}\\nSnippet: {r['body']}\" for r in results])\n",
    "            output = summary\n",
    "            logger.info(f\"{log_prefix}: Successfully retrieved search results.\")\n",
    "        else:\n",
    "            output = \"No relevant search results found.\"\n",
    "            logger.info(f\"{log_prefix}: No relevant search results found.\")\n",
    "    except ImportError as e:\n",
    "        error_msg = \"Error: The 'duckduckgo-search' library is required for this tool.\"\n",
    "        logger.error(f\"{log_prefix}: duckduckgo-search library is not installed.\")\n",
    "        output = error_msg\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error during internet search: {e}\"\n",
    "        logger.error(f\"{log_prefix}: Error during internet search: {e}\", exc_info=True)\n",
    "        output = error_msg\n",
    "    finally:\n",
    "        end_time: float = time.time()\n",
    "        log_tool_metrics(tool_name, start_time, end_time, {\"query\": query}, output, error_msg)\n",
    "        return output\n",
    "\n",
    "@tool\n",
    "def list_files_in_directory(path: str = '.', disk_io_manager: Optional['DiskIOManager'] = None) -> str:\n",
    "    \"\"\"\n",
    "    Lists all files and directories in the specified path.\n",
    "\n",
    "    Args:\n",
    "        path (str): The directory path to list. Defaults to the current directory.\n",
    "        disk_io_manager (Optional[DiskIOManager]): An instance of DiskIOManager for disk operations.\n",
    "\n",
    "    Returns:\n",
    "        str: A list of files and directories, or an error message.\n",
    "    \"\"\"\n",
    "    tool_name: str = \"list_files_in_directory\"\n",
    "    start_time: float = time.time()\n",
    "    log_prefix: str = f\"{tool_name}(path='{path}')\"\n",
    "    logger.info(f\"{log_prefix}: Attempting to list files in directory.\")\n",
    "    output: str = \"\"\n",
    "    error_msg: Optional[str] = None\n",
    "    try:\n",
    "        items = os.listdir(path)\n",
    "        output = \"\\n\".join(items)\n",
    "        logger.info(f\"{log_prefix}: Successfully listed files in directory: {path}\")\n",
    "    except FileNotFoundError:\n",
    "        error_msg = f\"Error: Directory not found: {path}\"\n",
    "        logger.error(f\"{log_prefix}: Directory not found: {path}\")\n",
    "        output = error_msg\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error listing directory: {e}\"\n",
    "        logger.error(f\"{log_prefix}: Error listing directory: {e}\", exc_info=True)\n",
    "        output = error_msg\n",
    "    finally:\n",
    "        end_time: float = time.time()\n",
    "        log_tool_metrics(tool_name, start_time, end_time, {\"path\": path}, output, error_msg)\n",
    "        return output\n",
    "\n",
    "@tool\n",
    "def get_file_metadata(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves metadata for a given file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file.\n",
    "\n",
    "    Returns:\n",
    "        str: A JSON string containing file metadata (size, modification time), or an error message.\n",
    "    \"\"\"\n",
    "    tool_name: str = \"get_file_metadata\"\n",
    "    start_time: float = time.time()\n",
    "    log_prefix: str = f\"{tool_name}(file_path='{file_path}')\"\n",
    "    logger.info(f\"{log_prefix}: Attempting to get file metadata.\")\n",
    "    output: str = \"\"\n",
    "    error_msg: Optional[str] = None\n",
    "    try:\n",
    "        metadata = {\n",
    "            \"size\": os.path.getsize(file_path),\n",
    "            \"modified_time\": time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(os.path.getmtime(file_path)))\n",
    "        }\n",
    "        output = json.dumps(metadata, indent=4)\n",
    "        logger.info(f\"{log_prefix}: Successfully retrieved metadata for: {file_path}\")\n",
    "    except FileNotFoundError:\n",
    "        error_msg = f\"Error: File not found: {file_path}\"\n",
    "        logger.error(f\"{log_prefix}: File not found: {file_path}\")\n",
    "        output = error_msg\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error getting file metadata: {e}\"\n",
    "        logger.error(f\"{log_prefix}: Error getting file metadata: {e}\", exc_info=True)\n",
    "        output = error_msg\n",
    "    finally:\n",
    "        end_time: float = time.time()\n",
    "        log_tool_metrics(tool_name, start_time, end_time, {\"file_path\": file_path}, output, error_msg)\n",
    "        return output\n",
    "\n",
    "@tool\n",
    "def download_file(url: str, save_path: str, disk_io_manager: Optional['DiskIOManager'] = None) -> str:\n",
    "    \"\"\"\n",
    "    Downloads a file from a URL and saves it to the specified path.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the file to download.\n",
    "        save_path (str): The local path where the file will be saved.\n",
    "        disk_io_manager (Optional[DiskIOManager]): An instance of DiskIOManager for disk operations.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating success or an error message.\n",
    "    \"\"\"\n",
    "    tool_name: str = \"download_file\"\n",
    "    start_time: float = time.time()\n",
    "    log_prefix: str = f\"{tool_name}(url='{url}', save_path='{save_path}')\"\n",
    "    logger.info(f\"{log_prefix}: Attempting to download file.\")\n",
    "    disk_io = disk_io_manager or DiskIOManager()\n",
    "    output: str = \"\"\n",
    "    error_msg: Optional[str] = None\n",
    "    try:\n",
    "        response = requests.get(url, stream=True, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        with open(save_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=disk_io.chunk_size):\n",
    "                file.write(chunk)\n",
    "        output = f\"File successfully downloaded to: {save_path}\"\n",
    "        logger.info(f\"{log_prefix}: File successfully downloaded to: {save_path}\")\n",
    "    except RequestException as e:\n",
    "        error_msg = f\"Error downloading file: {e}\"\n",
    "        logger.error(f\"{log_prefix}: Error downloading file: {e}\")\n",
    "        output = error_msg\n",
    "    except IOError as e:\n",
    "        error_msg = f\"Error saving downloaded file: {e}\"\n",
    "        logger.error(f\"{log_prefix}: Error saving downloaded file: {e}\", exc_info=True)\n",
    "        output = error_msg\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Unexpected error during file download: {e}\"\n",
    "        logger.error(f\"{log_prefix}: Unexpected error during file download: {e}\", exc_info=True)\n",
    "        output = error_msg\n",
    "    finally:\n",
    "        end_time: float = time.time()\n",
    "        log_tool_metrics(tool_name, start_time, end_time, {\"url\": url, \"save_path\": save_path}, output, error_msg)\n",
    "        return output\n",
    "\n",
    "@tool\n",
    "def make_directory(path: str) -> str:\n",
    "    \"\"\"\n",
    "    Creates a new directory at the specified path.\n",
    "\n",
    "    Args:\n",
    "        path (str): The path where the new directory should be created.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating success or an error message.\n",
    "    \"\"\"\n",
    "    tool_name: str = \"make_directory\"\n",
    "    start_time: float = time.time()\n",
    "    log_prefix: str = f\"{tool_name}(path='{path}')\"\n",
    "    logger.info(f\"{log_prefix}: Attempting to create directory.\")\n",
    "    output: str = \"\"\n",
    "    error_msg: Optional[str] = None\n",
    "    try:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        output = f\"Directory successfully created at: {path}\"\n",
    "        logger.info(f\"{log_prefix}: Directory successfully created at: {path}\")\n",
    "    except OSError as e:\n",
    "        error_msg = f\"Error creating directory: {e}\"\n",
    "        logger.error(f\"{log_prefix}: Error creating directory: {e}\", exc_info=True)\n",
    "        output = error_msg\n",
    "    finally:\n",
    "        end_time: float = time.time()\n",
    "        log_tool_metrics(tool_name, start_time, end_time, {\"path\": path}, output, error_msg)\n",
    "        return output\n",
    "\n",
    "@tool\n",
    "def delete_file(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Deletes the file at the specified path.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file to delete.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating success or an error message.\n",
    "    \"\"\"\n",
    "    tool_name: str = \"delete_file\"\n",
    "    start_time: float = time.time()\n",
    "    log_prefix: str = f\"{tool_name}(file_path='{file_path}')\"\n",
    "    logger.info(f\"{log_prefix}: Attempting to delete file.\")\n",
    "    output: str = \"\"\n",
    "    error_msg: Optional[str] = None\n",
    "    try:\n",
    "        os.remove(file_path)\n",
    "        output = f\"File successfully deleted: {file_path}\"\n",
    "        logger.info(f\"{log_prefix}: File successfully deleted: {file_path}\")\n",
    "    except FileNotFoundError:\n",
    "        error_msg = f\"Error: File not found: {file_path}\"\n",
    "        logger.error(f\"{log_prefix}: File not found: {file_path}\")\n",
    "        output = error_msg\n",
    "    except OSError as e:\n",
    "        error_msg = f\"Error deleting file: {e}\"\n",
    "        logger.error(f\"{log_prefix}: Error deleting file: {e}\", exc_info=True)\n",
    "        output = error_msg\n",
    "    finally:\n",
    "        end_time: float = time.time()\n",
    "        log_tool_metrics(tool_name, start_time, end_time, {\"file_path\": file_path}, output, error_msg)\n",
    "        return output\n",
    "\n",
    "# Example of agentic operation and user interaction (conceptual)\n",
    "def agent_loop():\n",
    "    \"\"\"Demonstrates the AI agent's autonomous operation and response to user input.\"\"\"\n",
    "    print(\"Agent is starting...\")\n",
    "    while True:\n",
    "        # Simulate autonomous decision making\n",
    "        action = \"search_internet\"\n",
    "        query = \"latest AI trends\"\n",
    "        print(f\"Agent is performing action: {action} with query: {query}\")\n",
    "        results = search_internet(query)\n",
    "        print(f\"Search results:\\n{results}\")\n",
    "\n",
    "        # Get feedback on the tool\n",
    "        feedback = get_tool_feedback(\"search_internet\")\n",
    "        print(f\"Feedback on search_internet tool:\\n{feedback}\")\n",
    "\n",
    "        # Simulate user interrupt\n",
    "        user_input = input(\"User input (or type 'continue'): \")\n",
    "        if user_input.lower() != 'continue':\n",
    "            print(f\"Agent responding to user input: {user_input}\")\n",
    "            # Implement logic to process user input contextually\n",
    "        else:\n",
    "            print(\"Agent continuing autonomous operation.\")\n",
    "\n",
    "        time.sleep(10) # Simulate time passing\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example tool usage\n",
    "    print(visit_webpage(WEB_SEARCH_URL.format(query=\"artificial intelligence\")))\n",
    "    print(execute_python_code(\"print('Hello from executed code!')\"))\n",
    "    print(read_file(\"smolagent_demo.ipynb\"))\n",
    "    write_file(\"test_output.txt\", \"This is a test.\")\n",
    "    print(search_internet(\"large language models\"))\n",
    "    print(list_files_in_directory())\n",
    "    print(get_file_metadata(\"smolagent_demo.ipynb\"))\n",
    "    print(download_file(\"https://www.example.com\", \"example.html\"))\n",
    "    print(make_directory(\"new_directory\"))\n",
    "    print(delete_file(\"test_output.txt\"))\n",
    "\n",
    "    # Start the agent loop (conceptual demonstration)\n",
    "    # agent_loop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
