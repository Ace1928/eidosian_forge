The user wants to optimize the `eidos_qwenllm.py` code for performance and robustness, moving beyond simulated disk offloading to real implementations. They also want to explore integrating LMQL. The goal is to maintain the `HuggingFaceLLM` base while enhancing its capabilities.

Here's a breakdown of potential optimization strategies and LMQL integration points:

**1. Real Disk Offloading:**

*   **Mechanism:** Instead of `time.sleep()`, use libraries like `diskcache` or implement custom logic to serialize and save large intermediate data (like model outputs or intermediate processing results) to disk when memory pressure is high. Load them back when needed.
*   **Granularity:**  Offload at the chunk level during processing or even the entire model if necessary (though the latter is complex with `transformers`).
*   **Monitoring:**  `psutil` is already in place. Use its memory usage metrics to trigger offloading dynamically.

**2. Dynamic Chunking:**

*   **Adaptive Size:**  Instead of a fixed `chunk_size`, adjust it based on available memory and processing speed. If memory is plentiful, increase the chunk size for faster processing. If memory is tight, decrease it.
*   **Lookahead/Feedback:**  Potentially monitor processing time per chunk. If it's consistently fast, increase the size. If it slows down, decrease it.

**3. Genuine Parallelism and Concurrency:**

*   **`asyncio` for I/O-bound tasks:**  Loading files, saving to disk, and potentially network requests (if any) are good candidates for `asyncio`.
*   **`threading` or `multiprocessing` for CPU-bound tasks:** Model inference can benefit from parallelism. Consider using `torch.multiprocessing` or `threading.Thread` for concurrent inference on different chunks or even within the model if the hardware allows. Be mindful of the Global Interpreter Lock (GIL) with threads and prefer multiprocessing for true CPU parallelism.
*   **Thread Safety:** Ensure any shared data structures accessed by multiple threads/processes are thread-safe (e.g., using `threading.Lock` or `queue.Queue`).

**4. Efficient Algorithms and Data Structures:**

*   **Tokenization:** The `transformers` library already provides highly optimized tokenizers. Ensure the correct tokenizer for the model is used.
*   **Data Structures:**  Review the use of lists, dictionaries, etc., and consider alternatives if they become performance bottlenecks for very large datasets (e.g., using generators for processing large files).

**5. Modular and Reusable Components:**

*   The existing code seems reasonably modular. Continue this pattern when adding new features.

**6. Abstraction:**

*   Maintain clear interfaces between components. For example, the chunking logic could be behind an interface, allowing different chunking strategies to be easily swapped.

**7. Error Handling and Logging:**

*   The current use of `try-except` and logging is good. Ensure detailed logging for debugging real disk offloading and concurrency issues.

**8. Parameterization:**

*   The configuration via `eidos_config.py` is excellent. Ensure all new configurable options have defaults.

**9. LMQL Integration:**

*   **Querying the LLM:**  Instead of directly calling `complete()` or `chat()`, use LMQL queries to express constraints, sampling strategies, and control the generation process more precisely.
*   **Prompt Engineering:** LMQL can help structure prompts dynamically based on context and previous interactions.
*   **Constraint Satisfaction:** LMQL allows specifying constraints on the generated output (e.g., format, length, keywords).
*   **Integration Points:**
    *   Modify the `complete()` and `chat()` methods to accept LMQL queries.
    *   Introduce a new method specifically for executing LMQL queries.
    *   Potentially create a wrapper around the `HuggingFaceLLM` to handle LMQL interactions.
*   **Dependencies:** Add `lmql` as a dependency.

**Specific Steps/Plan:**

1. **Disk Offloading Implementation:**
    *   Choose a disk caching library or design a custom serialization/deserialization mechanism.
    *   Modify the resource monitoring logic to trigger disk offloading when memory exceeds a threshold.
    *   Implement offloading for intermediate results in `complete()` and `chat()`.
    *   Implement logic to load data back from disk when needed.

2. **Dynamic Chunking Implementation:**
    *   Introduce logic to track memory usage and potentially processing time per chunk.
    *   Implement a function to adjust `chunk_size` based on these metrics.
    *   Integrate this dynamic adjustment into the processing loops.

3. **Concurrency Implementation:**
    *   Identify CPU-bound and I/O-bound operations.
    *   Use `asyncio` for I/O (file loading/saving).
    *   Use `multiprocessing` for parallel inference if feasible (consider model sharding or data parallelism). If not, explore `threading` for concurrency within the GIL limitations.
    *   Implement appropriate locking mechanisms for shared resources.

4. **LMQL Integration:**
    *   Install the `lmql` library.
    *   Create a function to execute LMQL queries using the underlying `HuggingFaceLLM`.
    *   Modify `complete()` and `chat()` to optionally accept LMQL queries.
    *   Potentially add a dedicated `query()` method for LMQL.

5. **Testing and Refinement:**
    *   Thoroughly test the disk offloading, dynamic chunking, and concurrency mechanisms with large datasets.
    *   Benchmark performance to ensure the optimizations are effective.
    *   Test the LMQL integration with various query types.

**Key Considerations:**

*   **Complexity:** Implementing true disk offloading and parallelism adds significant complexity. Start with one optimization at a time and test thoroughly.
*   **Hardware:** The effectiveness of parallelism depends heavily on the available hardware (number of CPU cores, GPU).
*   **Model Size:** Offloading an entire large language model is a complex task often handled by specialized libraries or frameworks like DeepSpeed or Accelerate. Focus on offloading intermediate data first.
*   **LMQL Learning Curve:**  Integrating LMQL requires understanding its syntax and capabilities.


So how can we go about doing this.


LMQL Basic Implementation Examples to Add:

2. Write Your First Query

A very simple Hello World LMQL query looks like this:

"Say 'this is a test':[RESPONSE]" where len(TOKENS(RESPONSE)) < 25

 Model Output

Say this is a test: RESPONSE This is a test

Note: You can click Open In Playground to run and experiment with this query.

This simple LMQL program consists of a single prompt statement and an associated where clause:

    Prompt Statement "Say 'this is a test'[RESPONSE]": Prompts are constructed using so-called prompt statements that look like top-level strings in Python. Template variables like [RESPONSE] are automatically completed by the model. Apart from single-line textual prompts, LMQL also support multi-part and scripted prompts, e.g. by allowing control flow and branching behavior to control prompt construction. To learn more, see Scripted Prompting.

    Constraint Clause where len(RESPONSE) < 10: In this second part of the statement, users can specify logical, high-level constraints on the output. LMQL uses novel evaluation semantics for these constraints, to automatically translate character-level constraints like len(RESPONSE) < 25 to (sub)token masks, that can be eagerly enforced during text generation. To learn more, see Constraints.

3. Going Further

Extending on your first query above, you may want to add more complex logic, e.g. by adding a second part to the prompt. Further, you may want to employ a different decoding algorithm, e.g. to sample multiple trajectories of your program or use a different model.

Let's extend our initial query, to allow for these changes:

sample(temperature=1.2)

"Say 'this is a test'[RESPONSE]" where len(TOKENS(RESPONSE)) < 25

if "test" not in RESPONSE:
    "You did not say 'test', try again:[RESPONSE]" where \
        len(TOKENS(RESPONSE)) < 25
else:
    "Good job"

Going beyond what we have seen so far, this LMQL program extends on the above in a few ways:

    Decoder Declaration sample(temperature=1.2): Here, we specify the decoding algorithm to use for text generation. In this case we use sample decoding with slightly increased temperature (>1.0). Above, we implicitly relied on deterministic argmax decoding, which is the default in LMQL. To learn more about the different supported decoding algorithms in LMQL (e.g. beam or best_k), please see Decoders.

    Prompt Program: The main body of the program remains the prompt. As before, we use prompt statements here, however, now we also make use of control-flow and branching behavior.

    On each LLM call, the concatenation of all prompt statements so far, form the prompt used to generate a value for the currently active template variable like RESPONSE. This means the LLM is always aware of the full prompt context so far, when generating a value for a template variable.

    After a prompt statement has been executed, the contained template variables are automatically exposed to the surrounding program context. This allows you to react to model output and incorporate the results in your program logic. To learn more about this form of interactive prompting, please see Scripted Prompting.

3. Enjoy

These basic steps should get you started with LMQL. If you need more inspiration before writing your own queries, you can explore the examples included with the Playground IDE or showcased on the LMQL Website.

If you have any questions and or requests for documentation, please feel free to reach out to us via our Community Discord, GitHub Issues, or Twitter.



Further Examples:

Overview
A quick tour of LMQL's syntax and capabilities.

LMQL is a Python-based programming language for LLM programming with declarative elements. As a simple example consider the following program, demonstrating the basic syntax of LMQL:

# review to be analyzed
review = """We had a great stay. Hiking in the mountains 
            was fabulous and the food is really good."""

# use prompt statements to pass information to the model
"Review: {review}"
"Q: What is the underlying sentiment of this review and why?"
# template variables like [ANALYSIS] are used to generate text
"A:[ANALYSIS]" where not "\n" in ANALYSIS

# use constrained variable to produce a classification
"Based on this, the overall sentiment of the message\
 can be considered to be[CLS]" where CLS in [" positive", " neutral", " negative"]

CLS # positive

 Model Output

Review: We had a great stay. Hiking in the mountains was fabulous and the food is really good.
Q: What is the underlying sentiment of this review and why?
A: ANALYSISThe underlying sentiment of this review is positive because the reviewer had a great stay, enjoyed the hiking and found the food to be good.
Based on this, the overall sentiment of the message can be 
considered to be CLS positive

In this program, we program an LLM to perform sentiment analysis on a provided user review. We first ask the model to provide some basic analysis, and then we ask it to classify the overall sentiment as one of positive, neutral, or negative. The model is able to correctly identify the sentiment of the review as positive.

To implement this workflow, we use two template variables [ANALYSIS] and [CLS], both of which are constrained using designated where expressions.

For ANALYSIS we constrain the model to not output any newlines, which prevents it from outputting multiple lines that could potentially break the program. For CLS we constrain the model to output one of the three possible values. Using these constraints allows us to decode a fitting answer from the model, where both the analysis and the classification are well-formed and in an expected format.

Without constraints, the prompt above could produce different final classifications, such as good or bad. To handle this in an automated way, one would have to employ ad-hoc parsing to CLS result to obtain a clear result. Using LMQL's constraints, however, we can simply restrict the model to only output one of the desired values, thereby enabling robust and reliable integration. To learn more about the different types of constraints available in LMQL, see Constraints.
Extracting More Information With Distributions

While the query above allows us to extract the sentiment of a review, we do not get any certainty information on the model's confidence in its classification. To obtain this information, we can additionally employ LMQL's distribution clause, to obtain the full distribution over the possible values for CLASSIFICATION:

argmax
    # review to be analyzed
    review = """We had a great stay. Hiking in the mountains was fabulous and the food is really good."""

    # use prompt statements to pass information to the model
    "Review: {review}"
    "Q: What is the underlying sentiment of this review and why?"
    # template variables like [ANALYSIS] are used to generate text
    "A:[ANALYSIS]" where not "\n" in ANALYSIS

    # use constrained variable to produce a classification
    "Based on this, the overall sentiment of the message can be considered to be[CLS]"
distribution
   CLS in [" positive", " neutral", " negative"]

 Model Output

Review: We had a great stay. Hiking in the mountains was fabulous and the food is really good.
Q: What is the underlying sentiment of this review and why?
A: ANALYSIS The underlying sentiment of this review is positive because the reviewer had a great stay, enjoyed the hiking and found the food to be good.
Based on this, the overall sentiment of the message can be considered to be CLS

P(CLS)
 -  positive (*) 0.9999244826658527
 -  neutral      7.513155848720942e-05
 -  negative     3.8577566019560874e-07

Distribution Clause

Instead of constraining CLS with a where expression, we now constrain it in the separate distribution clause. In LMQL, the distribution clause can be used to specify whether we want to additionally obtain the distribution over the possible values for a given variable. In this case, we want to obtain the distribution over the possible values for CLS.

    Extended Syntax: Note, that to use the distribution clause, we have to make our choice of decoding algorithm explicit, by specifying argmax at the beginning of our code (see Decoding Algorithms for more information). ¸

    In general, this extended form of LMQL syntax, i.e. indenting your program and explicitly specifying e.g. argmax at the beginning of your code, is optional, but recommended if you want to use the distribution clause. Throughout the documentation we will make use of both syntax variants.

In addition to using the model to perform the ANALYSIS, LMQL now also scores each of the individually provided values for CLS and normalizes the resulting sequence scores into a probability distribution P(CLS) (printed to the Terminal Output of the Playground or Standard Output of the CLI).

Here, we can see that the model is indeed quite confident in its classification of the review as positive, with an overwhelming probability of 99.9%.

    Note that currently distribution variables like CLS can only occur at the end of your program.

Dynamically Reacting To Model Output

Another way to improve on our initial query, is to implement a more dynamic prompt, where we can react to the model's output. For example, we could ask the model to provide a more detailed analysis of the review, depending on the model's classification:

argmax
   review = """We had a great stay. Hiking in the mountains 
               was fabulous and the food is really good."""
   """Review: {review}
   Q: What is the underlying sentiment of this review and why?
   A:[ANALYSIS]""" where not "\n" in ANALYSIS
   
   "Based on this, the overall sentiment of the message can be considered to be[CLS]" where CLS in [" positive", " neutral", " negative"]
   
   if CLS == " positive":
      "What is it that they liked about their stay? [FURTHER_ANALYSIS]"
   elif CLS == " neutral":
      "What is it that could have been improved? [FURTHER_ANALYSIS]"
   elif CLS == " negative":
      "What is it that they did not like about their stay? [FURTHER_ANALYSIS]"
where
   STOPS_AT(FURTHER_ANALYSIS, ".")

 Model Output

Review: We had a great stay. Hiking in the mountains was fabulous and the food is really good.

Q: What is the underlying sentiment of this review and why?
A: ANALYSISThe underlying sentiment of this review is positive because the reviewer had a great stay, enjoyed the hiking and found the food to be good.

Based on this, the overall sentiment of the message can be considered to be CLASSIFICATIONpositive

What is it that they liked about their stay?
FURTHER_ANALYSISThe reviewer liked the hiking in the mountains and the food.

As shown here, we can use the if statement to dynamically react to the model's output. In this case, we ask the model to provide a more detailed analysis of the review, depending on the overall positive, neutral, or negative sentiment of the review. All intermediate variables like ANALYSIS, CLASSIFICATION or FURTHER_ANALYSIS can be considered the output of query, and may be processed by an surrounding automated system.

To learn more about the capabilities of such control-flow-guided prompts, see Scripted Prompting.

As shown here, in addition to inline where expressions as seen earlier, you can also provide a global where expression at the end of your program, e.g. to specify constraints that should apply for all variables. Depending on your use case, this can be a convenient way to avoid having to repeat the same constraints multiple times, like for FURTHER_ANALYSIS in this example.


Further Information:
Scripted Prompting
Programmatic LLM prompting with control flow.

In LMQL, programs are not just static templates of text, as they also contain control flow (e.g. loops, conditions, function calls). This facilitates dynamic prompt construction and allows LMQL programs to respond dynamically to model output. This scripting mechanic is achieved by a combination of prompt templates, control flow and output constraining.

Escaping

LMQL requires special escaping to use [, ], { and } in a literal way, see Escaping for details.
Templates and Control Flow

Packing List For instance, let's say we want to generate a packing list before going on vacation. One way to do this would be the following query:

# use sampling decoder
sample(temperature=1.0) 
# generate a list
"A few things not to forget when going to the sea (not travelling): \n"
"[LIST]"

A list of things not to forget when going to the sea (not travelling):
LIST-A phone with call, texting or tech services
-A copy of the local paper
-A pen or phone Keytar

Here, we specify the sample decoder for increased diversity over argmax (cf. Decoders), and then execute the program to generate a list using one [LIST] variable.

This can work well, however, it is unclear if the model will always produce a well-structured list of items in practice. Further, we have to parse the response to separate the various items and process them further.

Simple Prompt Templates To address this, we can provide a more rigid template, by providing multiple prompt statements, one per item, to let the model only fill in THING:

"A list of things not to forget when going to the sea (not travelling): \n"
"-[THING]" where STOPS_AT(THING, "\n")
"-[THING]" where STOPS_AT(THING, "\n")
"-[THING]" where STOPS_AT(THING, "\n")
"-[THING]" where STOPS_AT(THING, "\n")
"-[THING]" where STOPS_AT(THING, "\n")

A list of things not to forget when going to the sea (not travelling):
-THINGA phone with a/r text
-THINGpletter
-THINGaccoon
-THINGFilms about/of the sea
-THINGA has been in the Poconos for/ Entered the Poconos

Note how we use a stopping constraint on each THING, such that a new line in the model output makes sure we progress with our provided template, instead of running-on with the model output. Without the stopping condition, simple template filling would not be possible, as the model would generate more than one item for the first variable already.

Prompt with Control-Flow Given this prompt template, we can now leverage control flow in our prompt, to further process results and avoid redundancy, while also guiding text generation.

First, we simplify our query and use a for loop instead of repeating the variable:

"A list of things not to forget when going to the sea (not travelling): \n"
backpack = []
for i in range(5):
   "-[THING]" where STOPS_AT(THING, "\n") 
   backpack.append(THING.strip())
print(backpack)

A list of things not to forget when going to the sea (not travelling):
-THINGA good pair of blue/gel saskaers
-THINGA good sun tanner
-THINGA good air freshener
-THINGA good spot for washing your hands
-THINGA good spot for washing your feet

['A good pair of blue/gel saskaers', 
 'A good sun tanner', 
 'A good air freshener', 'A good spot for washing your hands', 
 'A good spot for washing your feet']

Because we decode our list THING by THING, we can easily access the individual items, without having to think about parsing or validation. We just add them to a backpack list of things, which we then can process further.

Cross-Variable Constraints Now that we have a collected a list of things, we can even extend our program to constrain later parts to choose only the things in our backpack:

"A list of things not to forget when going to the sea (not travelling): \n"
backpack = []
for i in range(5):
   "-[THING]" where STOPS_AT(THING, "\n") 
   backpack.append(THING.strip())

"The most essential of which is: [ESSENTIAL_THING]" \
    where ESSENTIAL_THING in backpack

A list of things not to forget when going to the sea (not travelling): ⏎
-THINGSunscreen⏎
-THINGBeach Towels⏎
-THINGBeach Umbrella⏎
-THINGBeach Chairs⏎
-THINGBeach Bag⏎
The most essential of which is: ESSENTIAL_THINGSunscreen

This can be helpful in guiding the model to achieve complete and consistent model reasoning which is less likely to contradict itself.
Escaping

Inside prompt strings, the characters [, ], {, and } are reserved for template variable use and cannot be used directly. To use them as literals, they need to be escaped as [[, ]], {{, and }}, respectively. Beyond this, the standard string escaping rules for Python strings and f-strings apply, as all top-level strings in LMQL are interpreted as Python f-strings.

For instance, if you want to use JSON syntax as part of your prompt string, you need to escape the curly braces and squared brackets as follows:

argmax 
    """
    Write a summary of Bruno Mars, the singer:
    {{
      "name": "[STRING_VALUE]",
      "age": [INT_VALUE],
      "top_songs": [[
         "[STRING_VALUE]",
         "[STRING_VALUE]"
      ]]
    }}
    """
from
    "openai/text-davinci-003" 
where
    STOPS_BEFORE(STRING_VALUE, '"') and INT(INT_VALUE) and len(TOKENS(INT_VALUE)) < 2
         
         

Python Compatibility
Going beyond simple control flow, LMQL also supports most valid Python constructs in the prompt clause of a query, where top-level strings like "-[THING]" are automatically interpreted as model input and template variables are assigned accordingly. For more advanced usage, see also the Tool Augmentation chapter.


Further Information:
Constraints
Controlled, type-safe LLM generation with guarantees.

LMQL allows you to specify constraints on the language model output. This is valuable in scripted prompting, to ensure the model output stops at the desired point, but also allows you to guide the model during decoding.

This chapter provides an overview of the set of currently available constraints and a brief description of their semantics. Beyond that, LMQL constraint support is modular and can also be extended, as discussed in more detail in Custom Constraints.
Supported Constraints
Stopping Phrases

For many prompts, scripted prompts in particular, it is important to ensure that the model stops decoding once a certain word or symbol is reached. To do so, LMQL supports the STOPS_AT and STOPS_BEFORE constraint. They take two arguments, the first is the name of the variable to which the model output is assigned, the second is the stopping phrase.

In the example below we use stopping conditions to ensure that as soon as the model predicts the newline character \n the decoding of the current THING variable ends, and the query program continues execution.

"A list of things not to forget when going to the sea (not travelling): \n"
for i in range(5):
   "-[THING]" where STOPS_AT(THING, "\n")

A list of things not to forget when going to the sea (not travelling): 
- THINGSunscreen
- THINGBeach towels
- THINGSwimsuit
- THINGSunglasses
- THINGHat

TIP

If multiple variables in the query have the same name, the constraint is applied to all of them.
Number Type Constraints

The data type of a generated variable can also be constrained. For example, we can constrain a variable to be a string, encoding an integer by using INT:

"A number: [N]" where INT(N)

A number: N2

This enforces that the model cannot generate tokens that do not correspond to an integer.

INFO

LMQL currently only supports integer constraints. However, support for floating point numbers and other types is planned for future releases.
Choice From Set

LMQL allows to specify that a variable should be a choice from a set of possible values. This can be rephrased as the variable being within a set of possible values, i.e. THING in set(["Volleyball", "Sunscreen", "Bathing Suite"]) in the following example

sample(temperature=0.8)
   "A list of things not to forget when going to the sea (not travelling): \n"
   "- Sunglasses \n"
   for i in range(4):
      "- [THING] \n"
from
   'openai/text-ada-001'
where
   THING in set(["Volleyball", "Sunscreen", "Bathing Suite"])

A list of things not to forget when going to the sea (not travelling):
- THINGSunglasses
- THINGSunscreen
- THINGVolleyball
- THINGSunscreen
- THINGSunscreen

Character Length

Similar to Python, the len function can be used to refer to the length of a variable and can thus be used to add constraints on it's length on a character level.

argmax
   "Hello [NAME]"
from
   'openai/text-ada-001'
where
    len(NAME) < 10

Hello NAME ⏎
⏎
I am in

Token Length

Next to character-level length constraints, you can also specify constraints on the token length of a variable. This is useful if you want to ensure that the model does not generate too many tokens for a given variable. For instance, in the following example, we constrain the length of the variable THING to be at most 3 tokens:

"Greet the user in four different ways: [GREETINGS]" \
   where len(TOKENS(GREETINGS)) < 10

Greet the user in four different ways: GREETINGS⏎
⏎
1. Hello there It's nice

Similar to character length constraints, the len function can also be used to refer to the number of TOKENS(GREETINGS). However, now, length is measured in tokens, not characters. The exact resulting character length of the output thus depends on the tokenization of the model, however.

Token length constraints are cheaper to enforce than character length constraints, as they can be implemented as simple length checks on the tokenized output, rather than the more expensive character-level length checks, which require detokenization and masking.
Regex Constraints Preview

LMQL also supports REGEX constraints. This allows you to enforce a regular expression during generation of a variable.

As a simple example, consider the following snippet:

"It's the last day of June so today is [DATE]" \
   where REGEX(DATE, r"[0-9]{2}/[0-9]{2}")

It's the last day of June so today is DATE30/06

PREVIEW FEATURE

REGEX constraints are currently in preview, which means that they may not function as expected in all cases. If you encounter problems, please report them in LMQL's issue tracker on GitHub.
Combining Constraints

Several constraints can be combined with the and and or keywords, recovering a Python boolean expression over the variables utilized in the LMQL query.

For example, the following program uses, both, a token length constraint and a stopping condition, to generate a short story:

"A story about life:[STORY]" \
   where STOPS_AT(STORY, ".") and len(TOKENS(STORY)) > 40

A story about life:

STORYLife is a journey filled with ups and downs, twists and turns, and unexpected surprises. It is a story that is unique to each individual, with its own set of challenges and triumphs.

For some, life begins with a loving family and a comfortable home, while for others it may start with struggle and hardship.

Here, we enforce a stopping condition on the . character, but only once the generated story is at least 40 tokens long. Thus, the interpretation of and with stopping conditions is that the stopping condition is only enforced once the other constraints are satisfied.
How Do LMQL Constraints Work?

Token Masking and Eager Validation LMQL constraints are evaluated eagerly on each generated token, and will be used by the runtime to generate token masks during generation. This means, that the provided constraints are either satisfied by directly guiding the model during generation appropriately or, if this is not possible, validation will fail early on during generation, saving the cost of generating invalid output. In case of greedy decoding (sample or argmax), this terminates the generation process, in case of branching decoding, it will prune the current branch and continue generation only in the remaining, valid branches.

High-Level Text Constraints Constraints are high-level and operate on a text (not token) level. For instance, users can specify constraints like VAR in ["Hello", "Hi", "Greetings"], without having to consider the exact tokenization of the individual phrases. LMQL will automatically translate these constraints to token masks, that can be used to guide the model during generation, allowing to generate output that satisfies the provided constraint using one generation call only.
Custom Constraints and Theoretical Background
To learn more about the internals of LMQL and how to implement your own LMQL constraint, see the chapter on Implementing Custom LMQL Constraints.



Further Information:

Tool Augmentation
Augment LLM reasoning with Python tool integration

LMQL is a superset of Python and thus query programs can incorporate arbitrary Python constructs including function calls. For instance, below, we ask the model for a simple math problem and then use Python's eval function to evaluate the solution.

# generates an arithmetic expression
"A simple math problem for addition (without solution, \
    without words): [MATH]" where STOPS_BEFORE(MATH, "=")

# evaluate the expression and feed it back into the prompt
"= {eval(MATH.strip())}"

A simple math problem for addition (without solution, without words):
MATH 7 + 8 = 15

Here, similar to a python f-string, we use the {...} syntax to re-insert the result of the eval function into the prompt. This allows us to augment the reasoning capabilities of the large language model with a simple calculator.

WARNING

While eval is handy for the examples in this section and allows to perform simple math, generally it can pose a security risk and should not be used in production.
Calculator

Building on the previous example, we can now create an improved calculator that can handle more complex expressions:

import re
from lmql.demo import gsm8k_samples

def calc(expr):
      expr = re.sub(r"[^0-9+\-*/().]", "", expr)
      return eval(expr)

QUESTION = "Josh decides to try flipping a house. \
He buys a house for $80,000 and then puts in $50,000 in repairs. \
This increased the value of the house by 150%. \
How much profit did he make?"

# insert few shot demonstrations
"{gsm8k_samples()}"

# prompt tempalte
"Q: {QUESTION}\n"
"Let's think step by step.\n"

# reasoning loop
for i in range(4):
    "[REASON_OR_CALC]" \
        where STOPS_AT(REASON_OR_CALC, "<<") and \
              STOPS_AT(REASON_OR_CALC, "So the answer")
    
    if REASON_OR_CALC.endswith("<<"):
        " [EXPR]" where STOPS_AT(EXPR, "=")
        # invoke calculator function
        " {calc(EXPR)}>>"
    elif REASON_OR_CALC.endswith("So the answer"):
        break

# produce the final answer
"is[RESULT]"

Q: Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repairs.  This increased the value of the house by 150%.  How much profit did he make?

Let's think step by step.
REASON_OR_CALCJosh bought the house for $80,000 and put in $50,000 in repairs.
The value of the house increased by 150%, so the new value of the house is $80,000 + 150% of $80,000 = << EXPR80,000 + (80,000*1.5) = 200000.0>> 
REASON_OR_CALCThe profit Josh made is the difference between the new value of the house and the amount he spent on it, which is $200,000 - $80,000 - $50,000 = << EXPR200,000 - 80,000 - 50,000 = 70000>> REASON_OR_CALC $70,000.
So the answer is RESULT$70,000.

Here, we define a function calc that leverages the build-in re library for regular expressions, to strip the input of any non-numeric characters before calling eval.

Further, we use a function gsm8k_samples that returns a few-shot samples for the gsm8k dataset, priming the model on the correct form of tool use.
Beyond Calculators

Wikipedia Search Function use is not limited to calculators. In the example bellow we show how text retrieval, using Pythons async/await syntax, can be used to augment the reasoning capabilities of the large language model.

async def wikipedia(q):
   from lmql.http import fetch
   try:
      q = q.strip("\n '.")
      pages = await fetch(f"https://en.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&exintro&explaintext&redirects=1&titles={q}&origin=*", "query.pages")
      return list(pages.values())[0]["extract"][:280]
   except:
      return "No results"

# ask a question
"Q: From which countries did the Norse originate?\n"

# prepare wikipedia call
"Action: Let's search Wikipedia for the term '[TERM]\n" where STOPS_AT(TERM, "'")
result = await wikipedia(TERM)

# feed back result
"Result: {result}\n"

# generate final response
"Final Answer:[ANSWER]"

Q: From which countries did the Norse originate?
Action: Let's search Wikipedia for the term 'TERM Norse'.
Result: Norse is a demonym for Norsemen, a Medieval North Germanic ethnolinguistic group ancestral to modern Scandinavians, defined as speakers of Old Norse from about the 9th to the 13th centuries.
Norse may also refer to:

Final Answer: ANSWERThe Norse originated from North Germanic countries, including Denmark, Norway, Sweden, and Iceland.

Key-Value Store LMQL can also access the state of the surrounding python interpreter. To showcase this, we show how to use the assign and get functions to store and retrieve values in a simple key-value store.

# implement a simple key value storage
# with two operations
storage = {}
def assign(key, value): 
    # store a value
    storage[key] = value; return f'{{{key}: "{value}"}}'
def get(key): 
    # retrieve a value
    return storage.get(key)

# instructive prompt, instructing the model to how use the storage
"""In your reasoning you can use actions. You do this as follows:
`action_name(<args>) # result: <inserted result>`
To remember things, you can use 'assign'/'get':
- To remember something:
`assign("Alice", "banana") # result: "banana"`
- To retrieve a stored value:
`get("Alice") # result: "banana"`
Always tail calls with " # result". Using these actions, let's solve the following question.\n"""

# actual problem statement
"""
Q: Alice, Bob, and Claire are playing a game. At the start 
of the game, they are each holding a ball: Alice has a black 
ball, Bob has a brown ball, and Claire has a blue ball. 

As the game progresses, pairs of players trade balls. First, 
Bob and Claire swap balls. Then, Alice and Bob swap balls. 
Finally, Claire and Bob swap balls. At the end of the game, 
what ball does Alice have?
A: Let's think step by step.
"""

# core reasoning loop
for i in range(32):
    "[REASONING]" where STOPS_AT(REASONING, "# result") and \
                        STOPS_AT(REASONING, "Therefore, ")
    
    if REASONING.endswith("# result"):
        cmd = REASONING.rsplit("`",1)[-1]
        cmd = cmd[:-len("# result")]
        "{eval(cmd)}`\n"
    else:
        break

# generate final answer
"Therefore at the end of the game, Alice has the[OBJECT]" \
    where STOPS_AT(OBJECT, ".") and STOPS_AT(OBJECT, ",")

 Model Output

(...)
A: Let's think step by step

REASONING At the start of the game:
`assign('Alice', 'black') # result {Alice: 'black'}
REASONING `assign('Bob', 'brown') # result {Bob: 'brown'}
REASONING `assign('Claire', 'blue') # result {Claire: 'blue'}

REASONING After Bob and Claire swap balls:
`assign('Bob', 'blue') # result {Bob: 'blue'}
REASONING `assign('Claire', 'brown') # result {Claire: 'brown'}

REASONING After Alice and Bob swap balls:
`assign('Alice', 'blue') # result {Alice: 'blue'}
REASONING `assign('Bob', 'black') # result {Bob: 'black'}

REASONING After Claire and Bob swap balls:
`assign('Claire', 'black') # result {Claire: 'black'}
REASONING `assign('Bob', 'brown') # result {Bob: 'brown'}

REASONING At the end of the game, Alice has a blue ball:
`get('Alice') # result blue`
Therefore at the end of the game, Alice has the OBJECT blue ball.

As shown in the example above, the assign and get functions can be used to store and retrieve values in a simple key-value store. The model is merely instructed to make use of these functions in its reasoning. The query then implements logic to intercept any function use and insert the result of the function call into the reasoning. This allows the model to incorporate the state of the key-value store into its reasoning.



LMQL Language Reference:
Language Reference
LMQL's core syntax and semantics.

TIP

This document is a work-in-progress effort to provide a more formal description of LMQL, mainly discussing the syntactic form and corresponding semantics. Please feel free to reach out to the core team if you have any questions or suggestions for improvement.
Origins and Motivation

LMQL is a programming language for LLM programming. The primary design objective for LMQL is to provide a simple, yet powerful interface to implement (multi-part) reasoning flows that interact with LLMs in complex and algorithmically controlled ways.

At the core of LMQL, the following components are of particular importance to achieve this goal:

    Robust and type-safe generation via a constrained decoding engine.

    A lightweight programming model for prompting, including ways to abstract and modularize query code.

    A vendor-agnostic abstraction over optimized and parallelized LLM inference.

The overall goal is to provide a fast and reliable toolchain that facilitates LLM-powered reasoning and applications across domains. This includes already existing use-cases, such as simple chat applications and data parsing, but also extends to more complex applications such as autonomous agents, large-scale data processing, and programmatic reasoning and planning.
Using LMQL

LMQL's current reference implementation is written in Python and can be used in a variety of ways:

    The lmql playground offers an interactive interface for running, debugging and experimenting with LMQL programs. It is the recommended way to get started with LMQL. An online version of the playground is available at lmql.ai/playground.

    LMQL is available as a Python library, with the lmql.query function offering a simple way to use LMQL directly from within Python. For more information, please refer to the Python API section.

    The lmql run CLI tool can used to run LMQL programs directly from the command line.

For more information and example-based discussion on how to use LMQL, please refer to the Getting Started guide.
Syntax

The LMQL language comprises two syntax variants:

    The modern, more minimalistic standard syntax that relies on a very small set of language constructs in an otherwise standard Python environment. This syntax is the main focus for the continued development of LMQL.

    A legacy standalone syntax that is more static in nature but relevant for standalone LMQL use-cases.

Both syntax variants are compatible and can be used interchangeably.
Grammar Notation

Standard Syntax

LMQL's modern syntax can be read as standard Python code, with the addition of one new construct: query strings. Query strings are just top-level string expressions, that are interpreted as prompts to the underlying LLM. The following grammar describes this syntax in detail.

<LMQL_PROGRAM> := # (optional) decoder clause
                  [ <DECODER> [ '(' <python.KeywordArguments> ')' ]? ]? 
                  # program body
                  <STMT>*

<STMT> := # regular python statements
                 <python.Stmt> |
                 # query strings
                 <QUERY_STRING> |
                 # query strings with inline constraints
                 <QUERY_STRING> 'where' <CONSTRAINT_EXPR> |
                 # query strings with distribution clause
                 <QUERY_STRING> 'distribution' <DISTRIBUTION_EXPR>) |

<QUERY_STRING> := <...>

<CONSTRAINT_EXPR> := <...>

<DISTRIBUTION_EXPR> := <VARIABLE> 'in' <python.Expr>

Examples of Valid Programs

Simple program with two query strings

"Hello [WHO]"
"Goodbye [WHO]"

Python programs without LMQL constructs are also valid LMQL programs

a = 12
print("Hello, world!")
print("Goodbye, world!", a)

LMQL programs with control flow

"Q: What is 2x2? [ANSWER]"
while ANSWER != 4:
    "Incorrect, try again: [ANSWER]"
"Good job!"

Program with query strings, constraints and string interpolation

a = "Alice"
"Hello {a} [WHO]" where len(TOKENS(WHO)) < 10
print(WHO)

Program with query strings and distribution clauses

"Greet Alice\n:"
"Hello [WHO]" distribution WHO in ["Alice", "Bob"]

Standalone Syntax

The standalone query syntax is less flexible than the modern syntax variant and generally considered legacy. It is still supported for standalone use-cases, but users are advised to rely on the modern syntax going forward. Nonetheless, it shares many syntactic constructs with the modern syntax, and is thus described in this document as well.

<STANDALONE_QUERY> := [ <DECODER> [ '(' <python.KeywordArguments> ')' ]? ]? 
                        <PROMPT>
                     [ 'from' <MODEL_EXPR> ]?
                     [ 'where' <CONSTRAINT_EXPR> ]?
                     [ 'distribution' <DISTRIBUTION_EXPR> ]?

<DECODER> := 'argmax' | 'sample' | 'beam' | 'beam_var' | 
             'var' | 'best_k' | <python.Identifier>

<PROMPT> := <QUERY_STRING> | <python.Stmt>

<QUERY_STRING> := <...>

<MODEL_EXPR> := "lmql.model" "(" <python.Arguments> ")" |
                <python.StringLiteral>

<CONSTRAINT_EXPR> := <...>

<DISTRIBUTION_EXPR> := <VARIABLE> 'in' <python.Expr>

VARIABLE := <python.Identifier>

Examples

decoder clause + from

argmax "What is the capital of France? [ANSWER]" from "gpt2"

decoder clause + where

argmax "What is the capital of France? [ANSWER]" \
    where len(TOKENS(ANSWER)) < 10

decoder clause + distribution

argmax "What is the capital of France? [ANSWER]" \
    distribution ANSWER in ["A", "B"]

decoder clause + from + where + distribution

argmax 
    "What is the capital of France? [ANSWER]" 
from
    "gpt2"
distribution 
    ANSWER in ["A", "B"]

Decoder Clause

DECODER_CLAUSE := <DECODER> [ '(' <python.KeywordArguments> ')' ]?

The decoder clause defines the decoding algorithm to be used for generation. It is optional and defaults to argmax.

Algorithms <DECODER> is one of the runtime-supported decoding algorithms, e.g. argmax, sample, beam, beam_var, var, best_k, or a custom decoder function. For a detailed description please see the Decoding documentation chapter.

Program-Level Decoding Per query program, only one decoder clause can be specified, i.e. a single decoding algorithm is used to execute all query strings and placeholder variables. This is because decoders act on the program level, and branchingly explore several possible continuations of the program, based on the current program state.
Query Strings

Query strings represent the core construct for prompt construction and model interaction. They read like top-level string expressions in Python, but are interpreted as prompts to the underlying LLM, including placeholder variables, constraints and string interpolation:

QUERY_STRING := <CONSTRAINED_QSTRING> | <DISTRIBUTION_QSTRING> | <PURE_QSTRING>

# qstring with constraints, e.g. "Hello, [NAME]!" where len(TOKENS(NAME)) < 10
<CONSTRAINED_QSTRING> := <PURE_QSTRING> 'where' <CONSTRAINT_EXPR>

# qstring with distribution clause, e.g. "Hello, [NAME]!" distribution NAME in ["Alice", "Bob"]
<DISTRIBUTION_QSTRING> := <PURE_QSTRING> 'distribution' <VARIABLE> 'in' <python.Expr>

# qstring without constraints, e.g. "Hello, [NAME]!"
<PURE_QSTRING> := <python.StringDelimiter>
                  ( 
                      '.*' | # arbitrary string prompt
                      <PLACEHOLDER_VARIABLE> |
                      <STRING_INTERPOLATION> 
                  )* '"'
                  <python.StringDelimiter>

# placeholder variable, e.g. "[NAME]"
<PLACEHOLDER_VARIABLE> := "[" <python.Identifier> "]" |
                          # with type/tactic annotation
                          "[" <python.Identifier> ":" <python.Expr> "]" |
                          # with decorator
                          "[" <DECORATOR>?  <python.Identifier> "]" |
                          # with decorator and type/tactic
                          "[" <DECORATOR>?  <python.Identifier> ":" <python.Expr> "]"

# decorator, e.g. "@fct(a=12)"
<DECORATOR> := "@" <python.Call> | "@" [<python.Identifier>]

# standard Python f-string interpolation
<STRING_INTERPOLATION> := "{" <python.Expr> "}"

Examples

Escaping

To avoid ambiguities, the following characters need to be escaped in query strings:

    [ and ] need to be escaped as [[ and ]].
    { and } need to be escaped as { { and }}.

Python string escaping rules also apply, e.g. string delimiters need to be escaped to disambiguate string boundaries (e.g. "\"").
Prompt Construction

The current prompt used for LLM invocations throughout execution, is defined by the concatenation of all query strings executed so far, where placeholder variable are substituted by the respective generated values. Query strings are evaluated along the program's control flow (e.g. multiple times when called in loops), and left-to-right within a single query string with multiple placeholders. At any point during execution, when an LLM is invoked, the currently active prompt is what is passed to the model.

For this, newline characters \n must always be included explicitly, if desired.
Example: Prompt Construction

			
			
			
			
			
			
Placeholder Variables

Placeholder variables define the templated placeholders an LLM generates text for, and are denoted by [...] square brackets. With respect to the program state, placeholder variables assign the generated values to program variables of the same name.
Example: Variable Assignments

Query String Constraints and Distributions

Query strings can also be constrained using the "..." where ... syntax. This defines decoding constraints, that only apply locally during generation of the respective query string. For more information, see constraints.

Similarly, distributions can be constructed using the "..." distribution VAR in [...] syntax. This generates scores for given alternative values for the respective variable, and returns the resulting likelihoods. For more information, see distribution clauses
Example: Constrained Query String

Types and Tactics

Next to the variable name, a tactic or type can be specified using the "[VAR: <tactic>]" syntax. Syntactically, a tactic expression can be an arbitrary Python expression, however, at runtime, the interpreter expects one of the following:

    A type reference like int, supported by the runtime as LMQL type expression

    A regex expression like r"[a-z]+", to be enforced as a regex constraint.

    A reference to another LMQL query function like chain_of_thought(shots=2), to be executed as a nested query.

Example: Tactic Expressions

String Interpolation

Query strings are compiled to Python f-strings and thus implement regular Python string interpolation semantics using the "Hello {...}" syntax, e.g. "Hello {NAME}" evaluates to "Hello Alice", given the current program state assigns NAME = "Alice".
Constraints

<CONSTRAINT_EXPR> := <CONSTRAINT> 'and' CONSTRAINT_EXPR |
                     <CONSTRAINT> 'or' CONSTRAINT_EXPR |
                     'not' <CONSTRAINT> |
                     <CONSTRAINT>

DANGER

TODO
Model Expressions

<MODEL_EXPR> := "lmql.model" "(" <python.Arguments> ")" |
                <python.StringLiteral>

DANGER

TODO
Distribution Clauses

<DISTRIBUTION_EXPR> := <VARIABLE> 'in' <python.Expr>

DANGER

TODO
Python Fragments

LMQL relies on the following Python grammar fragments, to express parts of the LMQL language.

For reference, the Python grammar is available here.
Fragment	Description
<python.Identifier>	A Python identifier, as defined as NAME in the Python grammar. Examples: a, b, my_var1, MY_VAR2
<python.StringLiteral>	A Python string literal, as defined as STRING in the Python grammar. This includes supports for string delimiters, such as single quotes ('), double quotes ("), and triple quotes (''' or """). Examples: 'hello', "world", '''hello''', """world"""
<python.StringDelimiter>	A Python string delimiter, e.g. ', ", ''', or """.
<python.Expr>	A regular Python expression, as defined as expression in the Python grammar. Examples: a, a+b, a(b=12), a if b else c, a > 2, a == b
<python.Stmt>	A regular Python statement, as defined as simple_stmt or compound_stmt in the Python grammar. This includes control flow statements, such as if, for, while, try, with, etc.
<python.Call>	A Python function call, as defined as call in the Python grammar. This includes expressions like a(), a(b=12), a(b=12, c=13), a(b=12, c=13, **some_dict).
<python.KeywordArguments>	Function call keyword arguments, as defined as kwargs in the Python grammar. This includes expressions like (), (a=1, b=2), (a=1, b=2, **some_dict).
<python.Arguments>	Function call arguments, as defined as args in the Python grammar. This includes expressions like (), (a, b), (a, 1, 2, c=2), (a, b, *some_iterable, **some_dict).
Types
Work in Progress

LMQL types can be used to annotate variable during generation, to enforce type constraints on the generated values. The resulting value has two representations:

    A prompt representation: The value that is used in the prompt, e.g. 1234 would be represented as the string "1234".

    A program representation, the value that is returned when the corresponding variable is accessed from the program, e.g. 1234 would be represented as the Python integer 1234.

This distinction helps enable expressive prompting, i.e. represent values in a way that is suitable for the LLM, while also allowing for type-safe and convenient programmatic access to the generated values.
Type	Description	Prompt Representation	Program Representation
str (default)	String type, e.g. "hello", "world", ...	str_value	class str
int	Integer type, e.g. 1, 2, 3, ...	str(int_value)	class int

The default type of all placeholder variables is str. To change the type of a variable, the type can be specified as part of the placeholder variable declaration, e.g. [NAME:int] or [NAME:float], as discussed in the query strings section.
Query Functions

Query functions are the functional building blocks for LMQL programs.

LMQL query functions are defined similar to regular Python function syntax, but using the @lmql.query decorator and by providing the LMQL code as part of the docstring, not the function body.

@lmql.query
def my_query_function(person):
    '''lmql
    "Greet {person}. Hello [NAME]!"
    '''

From within Python, the same syntax can used to construct Python-callable query functions. Please see to the documentation chapter on Python Integration for more information.

LMQL query function can also be declared as async functions, which enables asynchronous execution.
Function Calling and Arguments

A query function can be called as a standard function from within LMQL or Python code. It can also be called as a nested query from within a query string. async query functions require the await keyword to be used.

Arguments In addition to the function arguments specified in the function signature, query functions also provide the following additional arguments, that can be used to control the generation process:

    model: The lmql.LLM model reference (or string identifier) to be used for generation.
    decoder: The decoding algorithm to be used for generation. See also the decoder clause section.
    output_writer: The output writer callback to be used during generation. See also documentation chapter on output streaming section.
    **kwargs: Additional keyword arguments, passed to decoder and interpreter, such as temperature, chunksize, etc.

Reference Implementation

LMQL's current reference implementation is written in Python and also available as a Python library. The reference implementation of the syntax and semantics described in this document is available via Git at github.com/eth-sri/lmql.
Compiler and Runtime

The LMQL Python compiler translates LMQL programs into asynchroneous, brancheable Python code according to the semantics described in this document. The resulting program is then executed using the LMQL runtime, which implements (constrained) decoding algorithms, optimizations and model support via several backends.
Hybrid Parser
For parsing, the implementation leverages a hybrid approach, largely relying on the existing Python parser (ast.parse) and grammar, adding additional parsing logic only for LMQL-specific constructs. This approach allows us to be compliant with the Python grammar, while also allowing us to extend the language with additional constructs, that are not part of the original Python grammar. To parse the standalone syntax, we segment the input on a token level and then call the parser several times to obtain the final AST for e.g. the prompt clause, the where clause or the distribution clause.


Using all of this to add to the program:
import asyncio  # 📦 Provides infrastructure for writing single-threaded concurrent code using coroutines, enabling asynchronous operations.
import logging  # 🪵 Provides a flexible framework for emitting log messages from applications, crucial for debugging and monitoring.
import nest_asyncio  # 🐦 Patches the asyncio event loop to allow it to be nested, enabling asynchronous operations within synchronous contexts.
import psutil  # 📊 Provides functions for monitoring system resources, including CPU, memory, and disk usage, crucial for adaptive processing.
import threading  # 🧵 Provides support for creating and managing threads, enabling concurrent execution of code.
import time  # ⏱️ Provides time-related functions, used here for simulating disk offloading and timestamping resource logs.
from typing import (
    Any,
    List,
    Optional,
    Dict,
    Tuple,
)  # 🖋️ Provides type hinting for complex data structures, improving code clarity and enabling static analysis.

import torch  # 🔥 The fundamental library for tensor computations and neural networks in PyTorch.
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
)  # 🤖 Provides pre-trained models and tokenizers for various NLP tasks, including causal language modeling.
from llama_index.llms.huggingface import (
    HuggingFaceLLM,
)  # 🦙 Provides a wrapper for Hugging Face models to be used with LlamaIndex.
from llama_index.core.llms import (
    ChatMessage,
    MessageRole,
)  # 💬 Defines the structure for chat messages, including the role of the sender and the message content.
from eidos_config import (
    DEFAULT_MODEL_NAME,
    DEFAULT_INITIAL_MAX_TOKENS,
    DEFAULT_DEVICE_MAP,
    DEFAULT_TRUST_REMOTE_CODE,
    DEFAULT_HIGH_RESOURCE_THRESHOLD,
    DEFAULT_INITIAL_CHUNK_SIZE,
)  # ⚙️ Imports default configuration values from the eidos_config module, ensuring consistent settings.

nest_asyncio.apply()  # 🐦 Applies the nest_asyncio patch to allow nested asyncio event loops.

# Configure logging to display messages in real-time for debugging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)  # 🪵 Configures basic logging to output messages with timestamps, log levels, and message content.


class QwenLLM:
    """
    A wrapper class for the Qwen Hugging Face model, providing
    enhanced functionality such as resource monitoring, chunkwise processing,
    and thread-safe operations.

    [all]
        - __init__(self, model_name: str = DEFAULT_MODEL_NAME, max_new_tokens: int = DEFAULT_INITIAL_MAX_TOKENS, device_map: str = DEFAULT_DEVICE_MAP, trust_remote_code: bool = DEFAULT_TRUST_REMOTE_CODE, memory_threshold: int = DEFAULT_HIGH_RESOURCE_THRESHOLD, chunk_size: int = DEFAULT_INITIAL_CHUNK_SIZE)
        - _load_model(self) -> None
        - _log_resource_usage(self, stage: str) -> None
        - _get_resource_usage(self) -> dict
        - _should_offload(self) -> bool
        - _process_chunk(self, text_chunk: str) -> str
        - complete(self, text: str) -> str
        - chat(self, messages: List[ChatMessage]) -> ChatMessage
    """

    def __init__(
        self,
        model_name: str = DEFAULT_MODEL_NAME,
        max_new_tokens: int = DEFAULT_INITIAL_MAX_TOKENS,
        device_map: str = DEFAULT_DEVICE_MAP,
        trust_remote_code: bool = DEFAULT_TRUST_REMOTE_CODE,
        memory_threshold: int = DEFAULT_HIGH_RESOURCE_THRESHOLD,
        chunk_size: int = DEFAULT_INITIAL_CHUNK_SIZE,
    ):
        """
        Initializes the QwenLLM with configuration settings.

        This constructor sets up the QwenLLM instance with the provided or default configurations.
        It initializes the model, tokenizer, and other necessary components for text processing.

        Args:
            model_name (str, optional): The name or path of the Qwen model to load from Hugging Face Hub.
                Defaults to DEFAULT_MODEL_NAME from eidos_config.
            max_new_tokens (int, optional): The maximum number of new tokens to generate in a single response.
                Defaults to DEFAULT_INITIAL_MAX_TOKENS from eidos_config.
            device_map (str, optional): The device to use for model inference ('cpu' or 'cuda').
                Defaults to DEFAULT_DEVICE_MAP from eidos_config.
            trust_remote_code (bool, optional): Whether to trust remote code when loading the model.
                Defaults to DEFAULT_TRUST_REMOTE_CODE from eidos_config.
            memory_threshold (int, optional): The memory usage percentage threshold above which offloading is triggered.
                Defaults to DEFAULT_HIGH_RESOURCE_THRESHOLD from eidos_config.
            chunk_size (int, optional): The initial size of text chunks for processing.
                Defaults to DEFAULT_INITIAL_CHUNK_SIZE from eidos_config.

        [all]
            - model_name (str): The name of the Qwen model to load from Hugging Face Hub.
            - max_new_tokens (int): The maximum number of new tokens to generate in a single response.
            - device_map (str): The device to use for model inference ('cpu' or 'cuda').
            - trust_remote_code (bool): Whether to trust remote code when loading the model.
            - memory_threshold (int): The memory usage percentage threshold above which offloading is triggered.
            - chunk_size (int): The initial size of text chunks for processing.
            - llm (Optional[HuggingFaceLLM]): The loaded HuggingFaceLLM model, initialized to None.
            - load_lock (threading.Lock): A lock to ensure thread-safe model loading.
            - resource_usage_log (List[Dict[str, dict]]): A list to store resource usage logs.
            - tokenizer (Optional[AutoTokenizer]): The tokenizer associated with the loaded model, initialized to None.
        """
        self.model_name = (
            model_name  # 🏷️ Sets the model name from the provided argument or default.
        )
        self.max_new_tokens = max_new_tokens  # 🏷️ Sets the maximum number of new tokens from the provided argument or default.
        self.device_map = (
            device_map  # 🏷️ Sets the device map from the provided argument or default.
        )
        self.trust_remote_code = trust_remote_code  # 🏷️ Sets the trust remote code flag from the provided argument or default.
        self.memory_threshold = memory_threshold  # 🏷️ Sets the memory threshold from the provided argument or default.
        self.chunk_size = chunk_size  # 🏷️ Sets the initial chunk size from the provided argument or default.
        self.llm: Optional[HuggingFaceLLM] = (
            None  # 🏷️ Initializes the LLM model to None.
        )
        self.load_lock = (
            threading.Lock()
        )  # 🔒 Creates a thread lock for safe model loading.
        self.resource_usage_log: List[Dict[str, dict]] = (
            []
        )  # 📊 Initializes an empty list to store resource usage logs.
        self._load_model()  # ⚙️ Loads the Qwen model and tokenizer.
        self.llm = (
            self.llm if not self.llm is None else None
        )  # 🏷️ Assigns the loaded llm to self.llm if it is not None, otherwise sets it to None.

    def _load_model(self) -> None:
        """Loads the Qwen model and tokenizer.

        This method loads the Qwen model and its tokenizer from the Hugging Face Hub,
        using the configurations provided during initialization. It employs a thread lock
        to ensure that the model is loaded only once, even if multiple threads attempt to
        access it concurrently.

        [all]
            - None
        """
        with self.load_lock:  # 🔒 Acquires the thread lock to ensure thread-safe model loading.
            if self.llm is None:  # 🔍 Checks if the model has already been loaded.
                try:  # ⛑️ Starts a try block to catch any exceptions during model loading.
                    logging.info(
                        "Loading Qwen model and tokenizer..."
                    )  # 🪵 Logs the start of model loading.
                    qwen_model = AutoModelForCausalLM.from_pretrained(
                        self.model_name,
                        torch_dtype="auto",
                        device_map=self.device_map,
                        trust_remote_code=self.trust_remote_code,
                    )  # 🤖 Loads the Qwen model from Hugging Face Hub using the specified configurations.
                    qwen_tokenizer = AutoTokenizer.from_pretrained(
                        self.model_name,
                        trust_remote_code=self.trust_remote_code,
                    )  # 🤖 Loads the tokenizer for the Qwen model from Hugging Face Hub.
                    logging.info(
                        "Building HuggingFaceLLM wrapper..."
                    )  # 🪵 Logs the start of building the HuggingFaceLLM wrapper.
                    self.llm = HuggingFaceLLM(
                        model=qwen_model,
                        tokenizer=qwen_tokenizer,
                        max_new_tokens=self.max_new_tokens,
                    )  # 🦙 Creates a HuggingFaceLLM wrapper around the loaded model and tokenizer.
                    logging.info(
                        f"Qwen model {self.model_name} loaded successfully."
                    )  # 🪵 Logs the successful loading of the model.
                except Exception as e:  # ⛑️ Catches any exceptions during model loading.
                    logging.error(
                        f"Error loading Qwen model: {e}"
                    )  # 🪵 Logs the error message.
                    raise  # 💥 Re-raises the exception to halt execution.

    def _log_resource_usage(self, stage: str) -> None:
        """Logs resource usage.

        This method logs the current resource usage, including CPU, memory,
        and disk usage, at a specific stage of processing. The resource
        usage data is appended to the resource_usage_log list.

        Args:
            stage (str): The stage of processing at which the resource usage is logged.

        [all]
            - stage (str): A string indicating the current stage of processing.
        """
        resource_data = (
            self._get_resource_usage()
        )  # 📊 Gets the current resource usage data by calling the _get_resource_usage method.
        self.resource_usage_log.append(
            {stage: resource_data}
        )  # 📊 Appends the resource data to the log, associating it with the given stage.
        logging.debug(
            f"Resource snapshot at '{stage}': CPU: {resource_data['cpu_percent']}%, Memory: {resource_data['memory_percent']}%, Disk: {resource_data['disk_percent']}%, Resident Memory: {resource_data['resident_memory'] / (1024**2):.2f} MB, Virtual Memory: {resource_data['virtual_memory'] / (1024**2):.2f} MB."
        )  # 🪵 Logs the resource usage data with detailed information, including CPU, memory, disk, resident memory, and virtual memory.

    def _get_resource_usage(self) -> dict:
        """Gets current resource usage.

        This method retrieves the current resource usage, including CPU,
        memory, and disk usage, using the psutil library.

        Returns:
            dict: A dictionary containing the current resource usage data.
                The dictionary includes keys for 'cpu_percent', 'memory_percent',
                'disk_percent', 'resident_memory', 'virtual_memory', and 'timestamp'.

        [all]
            - cpu_percent (float): The current CPU usage percentage.
            - memory_percent (float): The current memory usage percentage.
            - disk_percent (float): The current disk usage percentage.
            - resident_memory (int): The resident memory usage in bytes.
            - virtual_memory (int): The virtual memory usage in bytes.
            - timestamp (float): The timestamp of the resource usage data.
        """
        process = psutil.Process()  # 📊 Gets the current process using psutil.
        memory_info = (
            process.memory_info()
        )  # 📊 Gets memory information for the current process using psutil.
        cpu_percent = (
            psutil.cpu_percent()
        )  # 📊 Gets the current CPU usage percentage using psutil.
        memory_percent = (
            psutil.virtual_memory().percent
        )  # 📊 Gets the current memory usage percentage using psutil.
        disk_usage = psutil.disk_usage(
            "/"
        )  # 📊 Gets the disk usage information for the root directory using psutil.

        resource_data = {
            "cpu_percent": cpu_percent,
            "memory_percent": memory_percent,
            "disk_percent": disk_usage.percent,
            "resident_memory": memory_info.rss,
            "virtual_memory": memory_info.vms,
            "timestamp": time.time(),
        }  # 📊 Creates a dictionary containing the resource usage data.
        return resource_data  # 📊 Returns the resource usage data.

    def _should_offload(self) -> bool:
        """Checks if memory usage exceeds the threshold.

        This method checks if the current memory usage exceeds the configured
        memory threshold.

        Returns:
            bool: True if memory usage exceeds the threshold, False otherwise.

        [all]
            - None
        """
        memory_usage = (
            psutil.virtual_memory().percent
        )  # 📊 Gets the current memory usage percentage using psutil.
        return (
            memory_usage > self.memory_threshold
        )  # 🔍 Returns True if memory usage exceeds the threshold, False otherwise.

    def _process_chunk(self, text_chunk: str) -> str:
        """Processes a text chunk with the LLM.

        This method processes a single text chunk using the loaded LLM. It
        logs resource usage before and after processing, and simulates disk
        offloading if memory usage is high.

        Args:
            text_chunk (str): The text chunk to process.

        Returns:
            str: The processed text from the LLM.

        [all]
            - text_chunk (str): The text chunk to be processed by the LLM.
        """
        try:  # ⛑️ Starts a try block to catch any exceptions during chunk processing.
            self._log_resource_usage(
                "start_chunk_processing"
            )  # 🪵 Logs resource usage at the start of chunk processing.
            if (
                self._should_offload()
            ):  # 🔍 Checks if memory usage exceeds the threshold.
                logging.warning(
                    "Memory usage high, offloading to disk (simulated)."
                )  # 🪵 Logs a warning message indicating simulated disk offloading.
                # Simulate disk offloading (replace with actual disk I/O if needed)
                time.sleep(0.1)  # ⏱️ Simulates disk I/O by pausing execution.
            if self.llm is not None:
                response = self.llm.complete(
                    text_chunk
                )  # 🤖 Processes the text chunk using the LLM.
            else:
                logging.error("LLM not loaded")
                return ""
            self._log_resource_usage(
                "end_chunk_processing"
            )  # 🪵 Logs resource usage at the end of chunk processing.
            return response.text  # 💬 Returns the processed text.
        except Exception as e:  # ⛑️ Catches any exceptions during chunk processing.
            logging.error(f"Error processing chunk: {e}")  # 🪵 Logs the error message.
            return ""  # 💬 Returns an empty string in case of an error.

    def complete(self, text: str) -> str:
        """Completes text using chunkwise processing.

        This method processes the input text in chunks, using the _process_chunk
        method. It dynamically adjusts the chunk size based on memory usage.

        Args:
            text (str): The input text to process.

        Returns:
            str: The complete processed text from the LLM.

        [all]
            - text (str): The input text to be processed.
        """
        all_responses = ""  # 💬 Initializes an empty string to store all responses.
        start = 0  # 🏷️ Initializes the starting index for chunking.
        while start < len(text):  # 🔁 Loops through the text in chunks.
            end = min(
                start + self.chunk_size, len(text)
            )  # 🏷️ Calculates the end index for the current chunk.
            chunk = text[start:end]  # ✂️ Extracts the current chunk of text.
            response = self._process_chunk(chunk)  # 🤖 Processes the current chunk.
            all_responses += (
                response  # 💬 Appends the response to the accumulated responses.
            )
            start = end  # 🏷️ Updates the starting index for the next chunk.
            if (
                self._should_offload()
            ):  # 🔍 Checks if memory usage exceeds the threshold.
                self.chunk_size = max(
                    self.chunk_size // 2, 128
                )  # 📏 Reduces the chunk size if memory usage is high.
                logging.info(
                    f"Adjusting chunk size to {self.chunk_size} due to high memory usage."
                )  # 🪵 Logs the adjustment of chunk size.
            else:  # 🔍 If memory usage is low.
                self.chunk_size = min(
                    self.chunk_size * 2, DEFAULT_INITIAL_CHUNK_SIZE
                )  # 📏 Increases the chunk size if memory usage is low.
                logging.info(
                    f"Adjusting chunk size to {self.chunk_size} due to low memory usage."
                )  # 🪵 Logs the adjustment of chunk size.
        return all_responses  # 💬 Returns the complete processed text.

    def chat(self, messages: List[ChatMessage]) -> ChatMessage:
        """Processes chat messages.

        This method processes a list of chat messages using the loaded LLM.
        It logs resource usage before and after processing, and simulates disk
        offloading if memory usage is high.

        Args:
            messages (List[ChatMessage]): A list of chat messages to process.

        Returns:
            ChatMessage: The response message from the LLM.

        [all]
            - messages (List[ChatMessage]): A list of chat messages to be processed by the LLM.
        """
        try:  # ⛑️ Starts a try block to catch any exceptions during chat processing.
            self._log_resource_usage(
                "start_chat"
            )  # 🪵 Logs resource usage at the start of chat processing.
            if (
                self._should_offload()
            ):  # 🔍 Checks if memory usage exceeds the threshold.
                logging.warning(
                    "Memory usage high, offloading to disk (simulated)."
                )  # 🪵 Logs a warning message indicating simulated disk offloading.
                # Simulate disk offloading (replace with actual disk I/O if needed)
                time.sleep(0.1)  # ⏱️ Simulates disk I/O by pausing execution.
            if self.llm is not None:
                response = self.llm.chat(
                    messages=messages
                )  # 🤖 Processes the chat messages using the LLM.
            else:
                logging.error("LLM not loaded")
                return ChatMessage(
                    role=MessageRole.ASSISTANT, content="Error during chat."
                )
            self._log_resource_usage(
                "end_chat"
            )  # 🪵 Logs resource usage at the end of chat processing.
            return response.message  # 💬 Returns the response message.
        except Exception as e:  # ⛑️ Catches any exceptions during chat processing.
            logging.error(f"Error during chat: {e}")  # 🪵 Logs the error message.
            return ChatMessage(
                role=MessageRole.ASSISTANT, content="Error during chat."
            )  # 💬 Returns an error message in case of an exception.


# Example usage
qwen_llm = QwenLLM()  # 🤖 Creates an instance of the QwenLLM class.

# "Complete" style usage
response_text = qwen_llm.complete(
    "Tell me about large language models. " * 10
)  # 🤖 Uses the complete method to process a text.
print(
    "LLM Completion:", response_text
)  # 💬 Prints the response from the complete method.

user_message = ChatMessage(
    role=MessageRole.USER, content="Tell me about large language models."
)  # 💬 Creates a user message for chat.
chat_response = qwen_llm.chat(
    messages=[user_message]
)  # 🤖 Uses the chat method to process a chat message.
print(
    "LLM Chat Response:", chat_response.content
)  # 💬 Prints the response from the chat method.


Output just the code to be added/updated
Each piece in its own code block.
Covering everything in the documentation.
As perfect and complete as possible.
Retaining everything in the current code and ensurng perfect integration and functionality. Minimal changes to the current code and extending.




import asyncio
import logging
import nest_asyncio
import psutil
import threading
import time
from typing import (
    Any,
    List,
    Optional,
    Dict,
    Tuple,
)

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
)
from llama_index.llms.huggingface import (
    HuggingFaceLLM,
)
from llama_index.core.llms import (
    ChatMessage,
    MessageRole,
)
from eidos_config import (
    DEFAULT_MODEL_NAME,
    DEFAULT_INITIAL_MAX_TOKENS,
    DEFAULT_DEVICE_MAP,
    DEFAULT_TRUST_REMOTE_CODE,
    DEFAULT_HIGH_RESOURCE_THRESHOLD,
    DEFAULT_INITIAL_CHUNK_SIZE,
)
import lmql
import os
import diskcache
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor

nest_asyncio.apply()

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


class QwenLLM:
    """
    A wrapper class for the Qwen Hugging Face model, providing
    enhanced functionality such as resource monitoring, chunkwise processing,
    and thread-safe operations.

    [all]
        - __init__(self, model_name: str = DEFAULT_MODEL_NAME, max_new_tokens: int = DEFAULT_INITIAL_MAX_TOKENS, device_map: str = DEFAULT_DEVICE_MAP, trust_remote_code: bool = DEFAULT_TRUST_REMOTE_CODE, memory_threshold: int = DEFAULT_HIGH_RESOURCE_THRESHOLD, chunk_size: int = DEFAULT_INITIAL_CHUNK_SIZE)
        - _load_model(self) -> None
        - _log_resource_usage(self, stage: str) -> None
        - _get_resource_usage(self) -> dict
        - _should_offload(self) -> bool
        - _process_chunk(self, text_chunk: str) -> str
        - complete(self, text: str) -> str
        - chat(self, messages: List[ChatMessage]) -> ChatMessage
        - query(self, lmql_query: str, **kwargs) -> Any
    """

    def __init__(
        self,
        model_name: str = DEFAULT_MODEL_NAME,
        max_new_tokens: int = DEFAULT_INITIAL_MAX_TOKENS,
        device_map: str = DEFAULT_DEVICE_MAP,
        trust_remote_code: bool = DEFAULT_TRUST_REMOTE_CODE,
        memory_threshold: int = DEFAULT_HIGH_RESOURCE_THRESHOLD,
        chunk_size: int = DEFAULT_INITIAL_CHUNK_SIZE,
    ):
        """
        Initializes the QwenLLM with configuration settings.

        This constructor sets up the QwenLLM instance with the provided or default configurations.
        It initializes the model, tokenizer, and other necessary components for text processing.

        Args:
            model_name (str, optional): The name or path of the Qwen model to load from Hugging Face Hub.
                Defaults to DEFAULT_MODEL_NAME from eidos_config.
            max_new_tokens (int, optional): The maximum number of new tokens to generate in a single response.
                Defaults to DEFAULT_INITIAL_MAX_TOKENS from eidos_config.
            device_map (str, optional): The device to use for model inference ('cpu' or 'cuda').
                Defaults to DEFAULT_DEVICE_MAP from eidos_config.
            trust_remote_code (bool, optional): Whether to trust remote code when loading the model.
                Defaults to DEFAULT_TRUST_REMOTE_CODE from eidos_config.
            memory_threshold (int, optional): The memory usage percentage threshold above which offloading is triggered.
                Defaults to DEFAULT_HIGH_RESOURCE_THRESHOLD from eidos_config.
            chunk_size (int, optional): The initial size of text chunks for processing.
                Defaults to DEFAULT_INITIAL_CHUNK_SIZE from eidos_config.

        [all]
            - model_name (str): The name of the Qwen model to load from Hugging Face Hub.
            - max_new_tokens (int): The maximum number of new tokens to generate in a single response.
            - device_map (str): The device to use for model inference ('cpu' or 'cuda').
            - trust_remote_code (bool): Whether to trust remote code when loading the model.
            - memory_threshold (int): The memory usage percentage threshold above which offloading is triggered.
            - chunk_size (int): The initial size of text chunks for processing.
            - llm (Optional[HuggingFaceLLM]): The loaded HuggingFaceLLM model, initialized to None.
            - load_lock (threading.Lock): A lock to ensure thread-safe model loading.
            - resource_usage_log (List[Dict[str, dict]]): A list to store resource usage logs.
            - tokenizer (Optional[AutoTokenizer]): The tokenizer associated with the loaded model, initialized to None.
        """
        self.model_name = (
            model_name  # 🏷️ Sets the model name from the provided argument or default.
        )
        self.max_new_tokens = max_new_tokens  # 🏷️ Sets the maximum number of new tokens from the provided argument or default.
        self.device_map = (
            device_map  # 🏷️ Sets the device map from the provided argument or default.
        )
        self.trust_remote_code = trust_remote_code  # 🏷️ Sets the trust remote code flag from the provided argument or default.
        self.memory_threshold = memory_threshold  # 🏷️ Sets the memory threshold from the provided argument or default.
        self.chunk_size = chunk_size  # 🏷️ Sets the initial chunk size from the provided argument or default.
        self.llm: Optional[HuggingFaceLLM] = (
            None  # 🏷️ Initializes the LLM model to None.
        )
        self.load_lock = (
            threading.Lock()
        )  # 🔒 Creates a thread lock for safe model loading.
        self.resource_usage_log: List[Dict[str, dict]] = (
            []
        )  # 📊 Initializes an empty list to store resource usage logs.
        self._load_model()  # ⚙️ Loads the Qwen model and tokenizer.
        self.llm = (
            self.llm if not self.llm is None else None
        )  # 🏷️ Assigns the loaded llm to self.llm if it is not None, otherwise sets it to None.
        self.offload_path = os.path.join(os.getcwd(), "qwen_offload_cache")
        """
        A path for on-disk caching. Adjust as needed, or make it configurable.
        """

        self.cache = diskcache.Cache(self.offload_path)
        """
        A diskcache.Cache instance used for offloading intermediate data or model outputs.
        """

        self.enable_offloading = True
        """
        Flag to enable/disable real disk offloading. If set to False, falls back to simulation or no offloading.
        """

        self.enable_asyncio = True
        """
        Flag controlling whether to run asynchronous I/O-based tasks (like offloading/loading from disk) via asyncio.
        Set this to False if you prefer purely synchronous flows.
        """

        self.enable_parallel_inference = False
        """
        Flag for CPU-bound parallel processing with multiprocessing or threading if the environment/hardware supports it.
        """

        # 2) Optionally configure the concurrency pool
        #    For true parallel CPU-bound tasks, prefer ProcessPoolExecutor.
        #    For I/O bound tasks (if any), use ThreadPoolExecutor.
        self.executor = ProcessPoolExecutor() if self.enable_parallel_inference else None
        """
        An executor for parallel CPU-bound tasks. In practice, 
        make sure your environment supports forking large models.
        """

        # 3) If using concurrency, you might want to initialize an event loop or check if one is running
        #    nest_asyncio has already been applied, so you can rely on `asyncio.get_event_loop()` 
        #    for your asynchronous tasks.

    def _load_model(self) -> None:
        """Loads the Qwen model and tokenizer.

        This method loads the Qwen model and its tokenizer from the Hugging Face Hub,
        using the configurations provided during initialization. It employs a thread lock
        to ensure that the model is loaded only once, even if multiple threads attempt to
        access it concurrently.

        [all]
            - None
        """
        with self.load_lock:  # 🔒 Acquires the thread lock to ensure thread-safe model loading.
            if self.llm is None:  # 🔍 Checks if the model has already been loaded.
                try:  # ⛑️ Starts a try block to catch any exceptions during model loading.
                    logging.info(
                        "Loading Qwen model and tokenizer..."
                    )  # 🪵 Logs the start of model loading.
                    qwen_model = AutoModelForCausalLM.from_pretrained(
                        self.model_name,
                        torch_dtype="auto",
                        device_map=self.device_map,
                        trust_remote_code=self.trust_remote_code,
                    )  # 🤖 Loads the Qwen model from Hugging Face Hub using the specified configurations.
                    qwen_tokenizer = AutoTokenizer.from_pretrained(
                        self.model_name,
                        trust_remote_code=self.trust_remote_code,
                    )  # 🤖 Loads the tokenizer for the Qwen model from Hugging Face Hub.
                    logging.info(
                        "Building HuggingFaceLLM wrapper..."
                    )  # 🪵 Logs the start of building the HuggingFaceLLM wrapper.
                    self.llm = HuggingFaceLLM(
                        model=qwen_model,
                        tokenizer=qwen_tokenizer,
                        max_new_tokens=self.max_new_tokens,
                    )  # 🦙 Creates a HuggingFaceLLM wrapper around the loaded model and tokenizer.
                    logging.info(
                        f"Qwen model {self.model_name} loaded successfully."
                    )  # 🪵 Logs the successful loading of the model.
                except Exception as e:  # ⛑️ Catches any exceptions during model loading.
                    logging.error(
                        f"Error loading Qwen model: {e}"
                    )  # 🪵 Logs the error message.
                    raise  # 💥 Re-raises the exception to halt execution.

    def _log_resource_usage(self, stage: str) -> None:
        """Logs resource usage.

        This method logs the current resource usage, including CPU, memory,
        and disk usage, at a specific stage of processing. The resource
        usage data is appended to the resource_usage_log list.

        Args:
            stage (str): The stage of processing at which the resource usage is logged.

        [all]
            - stage (str): A string indicating the current stage of processing.
        """
        resource_data = (
            self._get_resource_usage()
        )  # 📊 Gets the current resource usage data by calling the _get_resource_usage method.
        self.resource_usage_log.append(
            {stage: resource_data}
        )  # 📊 Appends the resource data to the log, associating it with the given stage.
        logging.debug(
            f"Resource snapshot at '{stage}': CPU: {resource_data['cpu_percent']}%, Memory: {resource_data['memory_percent']}%, Disk: {resource_data['disk_percent']}%, Resident Memory: {resource_data['resident_memory'] / (1024**2):.2f} MB, Virtual Memory: {resource_data['virtual_memory'] / (1024**2):.2f} MB."
        )  # 🪵 Logs the resource usage data with detailed information, including CPU, memory, disk, resident memory, and virtual memory.

    def _get_resource_usage(self) -> dict:
        """Gets current resource usage.

        This method retrieves the current resource usage, including CPU,
        memory, and disk usage, using the psutil library.

        Returns:
            dict: A dictionary containing the current resource usage data.
                The dictionary includes keys for 'cpu_percent', 'memory_percent',
                'disk_percent', 'resident_memory', 'virtual_memory', and 'timestamp'.

        [all]
            - cpu_percent (float): The current CPU usage percentage.
            - memory_percent (float): The current memory usage percentage.
            - disk_percent (float): The current disk usage percentage.
            - resident_memory (int): The resident memory usage in bytes.
            - virtual_memory (int): The virtual memory usage in bytes.
            - timestamp (float): The timestamp of the resource usage data.
        """
        process = psutil.Process()  # 📊 Gets the current process using psutil.
        memory_info = (
            process.memory_info()
        )  # 📊 Gets memory information for the current process using psutil.
        cpu_percent = (
            psutil.cpu_percent()
        )  # 📊 Gets the current CPU usage percentage using psutil.
        memory_percent = (
            psutil.virtual_memory().percent
        )  # 📊 Gets the current memory usage percentage using psutil.
        disk_usage = psutil.disk_usage(
            "/"
        )  # 📊 Gets the disk usage information for the root directory using psutil.

        resource_data = {
            "cpu_percent": cpu_percent,
            "memory_percent": memory_percent,
            "disk_percent": disk_usage.percent,
            "resident_memory": memory_info.rss,
            "virtual_memory": memory_info.vms,
            "timestamp": time.time(),
        }  # 📊 Creates a dictionary containing the resource usage data.
        return resource_data  # 📊 Returns the resource usage data.

    def _should_offload(self) -> bool:
        """Checks if memory usage exceeds the threshold.

        This method checks if the current memory usage exceeds the configured
        memory threshold.

        Returns:
            bool: True if memory usage exceeds the threshold, False otherwise.

        [all]
            - None
        """
        memory_usage = (
            psutil.virtual_memory().percent
        )  # 📊 Gets the current memory usage percentage using psutil.
        return (
            memory_usage > self.memory_threshold
        )  # 🔍 Returns True if memory usage exceeds the threshold, False otherwise.

    def _offload_to_disk(self, key: str, data: Any) -> None:
        """
        Offloads 'data' to disk under the key 'key'.
        Utilizes diskcache for efficient serialization.
        """
        try:
            self.cache[key] = data  # diskcache automatically handles serialization
            logging.debug(f"Offloaded data with key '{key}' to disk.")
        except Exception as e:
            logging.error(f"Disk offloading error for key '{key}': {e}")

    def _load_from_disk(self, key: str) -> Any:
        """
        Loads data from diskcache for the given 'key'.
        Returns the data if present, or None if it doesn't exist.
        """
        try:
            return self.cache.get(key, default=None)
        except Exception as e:
            logging.error(f"Disk loading error for key '{key}': {e}")
            return None

    async def _process_chunk_async(self, text_chunk: str) -> str:
        """
        Asynchronous version of chunk processing.
        Offloads data to disk if needed and runs the LLM inference in a separate thread/process if configured.
        """
        self._log_resource_usage("start_chunk_processing_async")
        if self._should_offload() and self.enable_offloading:
            logging.warning("Memory usage high, performing real disk offloading.")
            # Offload chunk to disk
            self._offload_to_disk(f"chunk_{hash(text_chunk)}", text_chunk)
            # Additional logic: optionally remove from RAM, etc.

        # If parallel inference is enabled, run the inference in self.executor
        # Otherwise, run it in the current event loop
        loop = asyncio.get_running_loop()

        if self.executor and self.enable_parallel_inference:
            # CPU-bound parallel inference
            response = await loop.run_in_executor(
                self.executor, self._run_llm_sync, text_chunk
            )
        else:
            # No concurrency or just do a simple blocking call
            response = await loop.run_in_executor(None, self._run_llm_sync, text_chunk)

        self._log_resource_usage("end_chunk_processing_async")
        return response

    def _run_llm_sync(self, text_chunk: str) -> str:
        """
        Helper method to run the LLM synchronously (can be called inside run_in_executor).
        """
        if self.llm is not None:
            try:
                resp = self.llm.complete(text_chunk)
                return resp.text
            except Exception as e:
                logging.error(f"Error in synchronous LLM call: {e}")
        return ""

    def _process_chunk(self, text_chunk: str) -> str:
        """Processes a text chunk with the LLM.

        This method processes a single text chunk using the loaded LLM. It
        logs resource usage before and after processing, and simulates disk
        offloading if memory usage is high.

        Args:
            text_chunk (str): The text chunk to process.

        Returns:
            str: The processed text from the LLM.

        [all]
            - text_chunk (str): The text chunk to be processed by the LLM.
        """
        if self.enable_asyncio:
            # Defer to the async version
            return asyncio.run(self._process_chunk_async(text_chunk))
        else:
            # Original logic from your code
            self._log_resource_usage("start_chunk_processing")
            if self._should_offload() and self.enable_offloading:
                logging.warning("Memory usage high, performing real disk offloading.")
                # Real offloading
                self._offload_to_disk(f"chunk_{hash(text_chunk)}", text_chunk)

            if self.llm:
                try:
                    response = self.llm.complete(text_chunk)
                    self._log_resource_usage("end_chunk_processing")
                    return response.text
                except Exception as e:
                    logging.error(f"Error processing chunk: {e}")
                    return ""
            else:
                logging.error("LLM not loaded.")
                return ""

    def complete(self, text: str) -> str:
        """Completes text using chunkwise processing.

        This method processes the input text in chunks, using the _process_chunk
        method. It dynamically adjusts the chunk size based on memory usage.

        Args:
            text (str): The input text to process.

        Returns:
            str: The complete processed text from the LLM.

        [all]
            - text (str): The input text to be processed.
        """
        all_responses = ""
        start = 0
        while start < len(text):
            end = min(
                start + self.chunk_size, len(text)
            )
            chunk = text[start:end]
            response = self._process_chunk(chunk)
            all_responses += response
            start = end
            if self._should_offload():
                self.chunk_size = max(
                    self.chunk_size // 2, 128
                )
                logging.info(
                    f"Adjusting chunk size to {self.chunk_size} due to high memory usage."
                )
            else:
                self.chunk_size = min(
                    self.chunk_size * 2, DEFAULT_INITIAL_CHUNK_SIZE
                )
                logging.info(
                    f"Adjusting chunk size to {self.chunk_size} due to low memory usage."
                )
        return all_responses

    def chat(self, messages: List[ChatMessage]) -> ChatMessage:
        """Processes chat messages.

        This method processes a list of chat messages using the loaded LLM.
        It logs resource usage before and after processing, and simulates disk
        offloading if memory usage is high.

        Args:
            messages (List[ChatMessage]): A list of chat messages to process.

        Returns:
            ChatMessage: The response message from the LLM.

        [all]
            - messages (List[ChatMessage]): A list of chat messages to be processed by the LLM.
        """
        try:
            self._log_resource_usage(
                "start_chat"
            )
            if (
                self._should_offload()
            ):
                logging.warning(
                    "Memory usage high, offloading to disk (simulated)."
                )
                # Simulate disk offloading (replace with actual disk I/O if needed)
                time.sleep(0.1)
            if self.llm is not None:
                response = self.llm.chat(
                    messages=messages
                )
            else:
                logging.error("LLM not loaded")
                return ChatMessage(
                    role=MessageRole.ASSISTANT, content="Error during chat."
                )
            self._log_resource_usage(
                "end_chat"
            )
            return response.message
        except Exception as e:
            logging.error(f"Error during chat: {e}")
            return ChatMessage(
                role=MessageRole.ASSISTANT, content="Error during chat."
            )

    def query(self, lmql_query: str, **kwargs) -> Any:
        """Executes an LMQL query.

        This method takes an LMQL query string, executes it using the LMQL library,
        and returns the result. It handles any exceptions that occur during query
        execution and logs them.

        Args:
            lmql_query (str): The LMQL query string to execute.
            **kwargs: Additional keyword arguments to pass to the LMQL query.

        Returns:
            Any: The result of the LMQL query, which can be of any type.

        [all]
            - lmql_query (str): The LMQL query string to be executed.
            - **kwargs: Additional keyword arguments to pass to the LMQL query.
        """
        try:
            logging.info(f"Executing LMQL query: {lmql_query}")
            result = lmql.run(lmql_query, **kwargs)
            logging.info(f"LMQL query completed successfully.")
            return result
        except Exception as e:
            logging.error(f"Error executing LMQL query: {e}")
            return None

    def run_lmql_query(self, query_script: str, model: str = None, **decoder_args) -> str:
        """
        Runs an LMQL query script using the underlying HuggingFace model.
        'query_script' is an LMQL string, which can be a multi-line script:
            argmax
                "Say 'this is a test':[RESPONSE]" where len(TOKENS(RESPONSE)) < 25
            from
                "openai/text-ada-001"
        You can also specify:
            distribution, constraints, etc.
        If you'd like to rely on your own HuggingFace model, adjust 'from' accordingly
        or use the 'model' override below.
        """
        try:
            # fallback to self.model_name if no model provided
            lm = model if model else self.model_name

            # Use LMQL's python API
            # doc: https://lmql.ai/docs/python-api.html
            result = lmql.run(query_script, output_writer=lmql.stream, model=lm, **decoder_args)

            # If the query has variables, we can retrieve them from result
            # The structure depends on your LMQL script
            # For simplicity, we assume a single best output
            # or you can parse result for multiple outputs
            if result and len(result) > 0:
                # result[0].variables is a dict with keys corresponding to placeholders
                # result[0].text is the entire resolved text
                return result[0].text
            return "No valid LMQL output."
        except Exception as e:
            logging.error(f"Error running LMQL query: {e}")
            return f"LMQL error: {e}"


# Example usage
qwen_llm = QwenLLM()

# "Complete" style usage
response_text = qwen_llm.complete(
    "Tell me about large language models. " * 10
)
print(
    "LLM Completion:", response_text
)

user_message = ChatMessage(
    role=MessageRole.USER, content="Tell me about large language models."
)
chat_response = qwen_llm.chat(
    messages=[user_message]
)
print(
    "LLM Chat Response:", chat_response.content
)


def demo_lmql_usage(llm: QwenLLM):
    test_query = """
argmax
    # A basic prompt with a constraint
    "Say 'this is a test':[RESPONSE]" where len(TOKENS(RESPONSE)) < 25
from
    "{model_name}"
    """.format(model_name=llm.model_name)

    lmql_response = llm.run_lmql_query(test_query)
    print("LMQL Query Output:", lmql_response)

demo_lmql_usage(qwen_llm)

