\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{microtype}
\usepackage{hyperref}

% Define a robust style for Python listings with automatic line breaks.
\lstdefinestyle{MyPythonStyle}{
    language=Python,
    basicstyle=\ttfamily\small,
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=10pt,
    backgroundcolor=\color{white},
    showspaces=false,
    showstringspaces=false,
    breaklines=true,
    breakatwhitespace=true,
    tabsize=4,
    captionpos=b,
    frame=single,
    rulecolor=\color{black},
    upquote=true,
    aboveskip=10pt,
    belowskip=10pt,
}
\lstset{style=MyPythonStyle}

\begin{document}
\sloppy

\section{Example: A Comprehensive PMP Program}
The Python program below demonstrates a complete PMP application that:
\begin{itemize}
    \item Normalizes text using robust, adaptive methods.
    \item Tokenizes the normalized text.
    \item Computes detailed metrics and renders a polished, visually appealing graph.
\end{itemize}
This implementation encapsulates internal states and process functions within modular components,
which are composed via dependency injection to form a coherent, adaptive system.

\begin{lstlisting}[language=Python, caption={A Comprehensive Text Processing Pipeline in PMP}]
import unicodedata
from typing import Tuple, List, Dict, Any, Optional
import matplotlib.pyplot as plt

"""
Processual Modular Programming (PMP) Paradigm:
Robust Text Analysis from Real-World Sources

This script demonstrates PMP via a text processing pipeline designed for 
unstructured, real-world text. The pipeline is engineered for robustness and 
performance, effectively processing chaotic text from diverse sources. It shows:
    - Well-defined interfaces for modular interoperability.
    - Encapsulated states within modules for clarity and maintainability.
    - Process functions as explicit state transformations.
    - Dynamic adaptation mechanisms that refine processing based on input.
    - Modular composition via dependency injection for flexibility and scalability.
"""

# Module 1: Advanced Text Normalization Module
class TextNormalizer:
    """
    PMP module for advanced text normalization. Incorporates Unicode normalization,
    whitespace handling, and adaptive rule application.

    Attributes:
        state (Dict[str, Any]): Internal state including 'rules', 'notes', and 
            'adaptation_strategy'.
    """
    def __init__(
        self,
        rules: Optional[List[str]] = None,
        adaptation_strategy: Optional[Dict[str, Any]] = None,
    ) -> None:
        """
        Initializes with normalization rules and an adaptation strategy.
        Defaults to Unicode NFC normalization and whitespace stripping.
        """
        default_rules = ["NFC", "strip_whitespace"]
        self.state: Dict[str, Any] = {
            "rules": rules if rules is not None else default_rules,
            "notes": [],
            "adaptation_strategy": (
                adaptation_strategy if adaptation_strategy is not None 
                else {"short_text_threshold": 15}
            ),
        }

    def process(self, input_text: str) -> Tuple[str, Dict[str, Any]]:
        """
        Processes input text with normalization and dynamic state adaptation.
        """
        normalized_text = self.normalize(input_text)
        text_length = len(normalized_text)
        short_text_threshold = self.state["adaptation_strategy"].get(
            "short_text_threshold", 15
        )
        note = f"Short text detected ({text_length} chars), consider content sparsity."
        if text_length < short_text_threshold:
            if note not in self.state["notes"]:
                self.state["notes"].append(note)
        else:
            if note in self.state["notes"]:
                self.state["notes"].remove(note)
        return normalized_text, self.state.copy()

    def normalize(self, text: str) -> str:
        """
        Applies the configured normalization rules to the text.
        """
        processed_text = text
        if "strip_whitespace" in self.state["rules"]:
            processed_text = processed_text.strip()
        if "NFC" in self.state["rules"]:
            processed_text = unicodedata.normalize("NFC", processed_text)
        return processed_text

# Module 2: Intelligent Text Tokenizer Module
class Tokenizer:
    """
    PMP module for text tokenization with configurable delimiters.

    Attributes:
        state (Dict[str, Any]): Internal state including 'delimiter', 'token_count',
            and 'tokenization_strategy'.
    """
    def __init__(
        self,
        delimiter: str = " ",
        tokenization_strategy: Optional[Dict[str, Any]] = None,
    ) -> None:
        """
        Initializes the Tokenizer with a delimiter and optional strategy.
        """
        self.state: Dict[str, Any] = {
            "delimiter": delimiter,
            "token_count": 0,
            "tokenization_strategy": (
                tokenization_strategy if tokenization_strategy is not None 
                else {}
            ),
        }

    def process(self, text: str) -> Tuple[List[str], Dict[str, Any]]:
        """
        Tokenizes input text and updates token count.
        """
        tokens = self.tokenize(text)
        self.state["token_count"] = len(tokens)
        return tokens, self.state.copy()

    def tokenize(self, text: str) -> List[str]:
        """
        Splits text into tokens using the configured delimiter.
        """
        return text.split(self.state["delimiter"])

# Module 3: Comprehensive Metrics Collector and Visualizer Module
class MetricsCollector:
    """
    PMP module for computing text metrics and visualizing the token length distribution.

    Attributes:
        metrics (Dict[str, Any]): Contains computed metrics such as average, maximum, and 
            minimum token lengths, token count, vocabulary size, and the token list.
    """
    def __init__(self) -> None:
        self.metrics: Dict[str, Any] = {}

    def process(self, tokens: List[str]) -> Dict[str, Any]:
        """
        Computes various metrics based on tokens and updates internal state.
        """
        if not tokens:
            self.metrics = {
                "average_token_length": 0,
                "max_token_length": 0,
                "min_token_length": 0,
                "token_count": 0,
                "vocabulary_size": 0,
                "tokens": tokens,
            }
        else:
            token_lengths = [len(token) for token in tokens]
            vocabulary = set(tokens)
            self.metrics = {
                "average_token_length": sum(token_lengths) / len(token_lengths),
                "max_token_length": max(token_lengths),
                "min_token_length": min(token_lengths),
                "token_count": len(tokens),
                "vocabulary_size": len(vocabulary),
                "tokens": tokens,
            }
        return self.metrics.copy()

    def visualize(self) -> None:
        """
        Generates and displays a bar chart of token length distribution.
        """
        tokens = self.metrics.get("tokens", [])
        lengths = [len(token) for token in tokens] if tokens else []
        if not lengths:
            print("No tokens available for visualization.")
            return

        plt.figure(figsize=(12, 7))
        bars = plt.bar(
            range(len(lengths)),
            lengths,
            color="mediumseagreen",
            edgecolor="darkgreen",
            alpha=0.85,
        )
        for bar in bars:
            yval = bar.get_height()
            plt.text(
                bar.get_x() + bar.get_width() / 2,
                yval + 0.5,
                round(yval, 1),
                ha="center",
                va="bottom",
                fontsize=8,
                color="dimgray",
            )
        plt.xlabel("Token Index", fontsize=14, color="dimgray")
        plt.ylabel("Token Length (Characters)", fontsize=14, color="dimgray")
        plt.title(
            "Distribution of Token Lengths in Processed Text",
            fontsize=18,
            fontweight="bold",
            color="midnightblue",
        )
        plt.xticks(fontsize=11, color="gray")
        plt.yticks(fontsize=11, color="gray")
        plt.grid(axis="y", linestyle="--", alpha=0.7)
        plt.tight_layout()
        ax = plt.gca()
        ax.spines["top"].set_visible(False)
        ax.spines["right"].set_visible(False)
        ax.set_facecolor("whitesmoke")
        plt.show()

# PMP Pipeline: Composition via Dependency Injection.
def PMP_pipeline(input_text: str) -> Dict[str, Any]:
    """
    Orchestrates normalization, tokenization, and metrics analysis 
    in the PMP pipeline.

    Returns a dictionary with:
        - 'normalized_text'
        - 'tokens'
        - 'normalizer_state'
        - 'tokenizer_state'
        - 'metrics'
    """
    normalizer = TextNormalizer()
    tokenizer = Tokenizer()
    metrics_collector = MetricsCollector()
    normalized_text, normalizer_state = normalizer.process(input_text)
    tokens, tokenizer_state = tokenizer.process(normalized_text)
    metrics = metrics_collector.process(tokens)
    metrics_collector.visualize()
    return {
        "normalized_text": normalized_text,
        "tokens": tokens,
        "normalizer_state": normalizer_state,
        "tokenizer_state": tokenizer_state,
        "metrics": metrics,
    }

if __name__ == "__main__":
    """
    Main execution demonstrating the PMP pipeline with various text sources.
    """
    print("PMP Pipeline Demonstration: Real-World Text Analysis\n" + "=" * 60)
    # Example 1: Social Media Text
    raw_text_1 = (
        "OMG! Just saw the most amazing sunset #nofilter #blessed. "
        "Gonna grab a coffee now! What's everyone else up to?"
    )
    result_1 = PMP_pipeline(raw_text_1)
    print("\nExample 1: Social Media Text Processing")
    print("-" * 60)
    print("Normalized Text:", f"'{result_1['normalized_text']}'")
    print("Tokenized Output:", result_1["tokens"])
    print("Normalizer State:", result_1["normalizer_state"])
    print("Tokenizer State:", result_1["tokenizer_state"])
    print("Computed Metrics:", 
          {k: v for k, v in result_1["metrics"].items() if k != "tokens"})

    # Example 2: News Headline
    raw_text_2 = ("Breaking News: Global Markets React to Unexpected "
                  "Economic Data Release.")
    result_2 = PMP_pipeline(raw_text_2)
    print("\nExample 2: News Headline Processing")
    print("-" * 60)
    print("Normalized Text:", f"'{result_2['normalized_text']}'")
    print("Tokenized Output:", result_2["tokens"])
    print("Normalizer State:", result_2["normalizer_state"])
    print("Tokenizer State:", result_2["tokenizer_state"])
    print("Computed Metrics:", 
          {k: v for k, v in result_2["metrics"].items() if k != "tokens"})

    # Example 3: Online Review Text
    raw_text_3 = ("The new restaurant was okay, I guess. Service was kinda slow, "
                  "but the food was decent")
    result_3 = PMP_pipeline(raw_text_3)
    print("\nExample 3: Online Review Text Processing")
    print("-" * 60)
    print("Normalized Text:", f"'{result_3['normalized_text']}'")
    print("Tokenized Output:", result_3["tokens"])
    print("Normalizer State:", result_3["normalizer_state"])
    print("Tokenizer State:", result_3["tokenizer_state"])
    print("Computed Metrics:", 
          {k: v for k, v in result_3["metrics"].items() if k != "tokens"})

    # Example 4: Telegraphic Text
    raw_text_4 = "Urgent meeting tmrw 9am sharp!"
    result_4 = PMP_pipeline(raw_text_4)
    print("\nExample 4: Telegraphic Text & Dynamic Adaptation")
    print("-" * 60)
    print("Normalized Text:", f"'{result_4['normalized_text']}'")
    print("Tokenized Output:", result_4["tokens"])
    print("Normalizer State:", result_4["normalizer_state"])
    print("Tokenizer State:", result_4["tokenizer_state"])
    print("Computed Metrics:", 
          {k: v for k, v in result_4["metrics"].items() if k != "tokens"})

    print("\n" + "=" * 60 + "\nEnd of PMP Demonstration: Real-World Text Analysis")
\end{lstlisting}

This comprehensive example illustrates the PMP approach by:
\begin{itemize}
    \item Encapsulating internal states and process functions in well-documented, self-contained modules.
    \item Demonstrating dynamic adaptation via runtime state updates.
    \item Implementing dependency injection through modular composition.
    \item Delivering polished, graphical feedback of token metrics.
\end{itemize}

\end{document}
