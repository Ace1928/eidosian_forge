{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user wants to replace the selected markdown content in the `eidos_planning_book.ipynb` file with a new markdown cell that describes the modularized project structure. The new markdown content is provided in the instructions. I need to format the provided content as a single markdown cell, similar to the original selection.```\n",
    "# Start of Selection\n",
    "<div style=\"font-family: 'SF Pro Display', sans-serif; line-height: 1.5; color: #f8f8f2; margin-bottom: 20px; background-color: #2D3748; border-radius: 10px; padding: 20px; box-shadow: 0 8px 20px rgba(0, 0, 0, 0.6);\">\n",
    "    <div style=\"display: flex; align-items: center; border-bottom: 2px solid #4A5568; padding-bottom: 15px; margin-bottom: 20px;\">\n",
    "        <span style=\"font-size: 2.2em; color: #81E6D9; margin-right: 15px;\">&#x1F4BB;</span>\n",
    "        <h2 style=\"color: #FFFFFF; font-weight: bold; font-size: 1.7em; margin: 0;\">Modularizing Eidos: Project Structure</h2>\n",
    "    </div>\n",
    "    <p style=\"font-size: 1.0em; color: #CBD5E0; margin-bottom: 15px;\">\n",
    "        To modularize the provided code while preserving its functionality and separating concerns such as LLM core operations, NLP analysis, math logic, text processing, input processing, and thought routing, we can reorganize the codebase into distinct modules. Below is a proposed project structure along with an outline of each module's primary classes and functions. For clarity and maintainability, all modules will reside in a single top-level directory called <code>eidos_project</code>.\n",
    "    </p>\n",
    "    <div style=\"margin-bottom: 20px;\">\n",
    "        <h3 style=\"color: #81E6D9; font-size: 1.3em; margin-bottom: 10px; border-bottom: 1px solid #4A5568; padding-bottom: 5px;\">Project Structure:</h3>\n",
    "        <pre style=\"background-color: #384356; color: #B0BEC5; padding: 10px; border-radius: 5px; font-size: 0.9em; overflow-x: auto;\">\n",
    "eidos_project/\n",
    "├── __init__.py\n",
    "├── main.py\n",
    "├── config.py\n",
    "├── llm_core.py\n",
    "├── nlp_module.py\n",
    "├── text_processing.py\n",
    "├── input_processing.py\n",
    "├── math_logic.py\n",
    "└── thought_routing.py\n",
    "        </pre>\n",
    "    </div>\n",
    "\n",
    "    <div style=\"margin-bottom: 20px;\">\n",
    "        <h3 style=\"color: #F6AD55; font-size: 1.3em; margin-bottom: 10px; border-bottom: 1px solid #4A5568; padding-bottom: 5px;\">1. <code style=\"color: #81E6D9;\">config.py</code></h3>\n",
    "        <p style=\"font-size: 1.0em; color: #CBD5E0; margin-bottom: 10px;\">Centralizes configuration settings used across modules.</p>\n",
    "        <pre style=\"background-color: #384356; color: #B0BEC5; padding: 10px; border-radius: 5px; font-size: 0.9em; overflow-x: auto;\">\n",
    "# config.py\n",
    "\n",
    "class Config:\n",
    "    DEFAULTS = {\n",
    "        \"model_name\": \"Qwen/Qwen-7B-Chat\",\n",
    "        \"device\": \"cuda\" if __import__(\"torch\").cuda.is_available() else \"cpu\",\n",
    "        \"initial_max_tokens\": 512,\n",
    "        \"max_cycles\": 3,\n",
    "        \"max_single_response_tokens\": 2048,\n",
    "        \"assessor_count\": 2,\n",
    "        # ... other default configuration values\n",
    "    }\n",
    "        </pre>\n",
    "    </div>\n",
    "\n",
    "    <div style=\"margin-bottom: 20px;\">\n",
    "        <h3 style=\"color: #68D391; font-size: 1.3em; margin-bottom: 10px; border-bottom: 1px solid #4A5568; padding-bottom: 5px;\">2. <code style=\"color: #81E6D9;\">llm_core.py</code></h3>\n",
    "        <p style=\"font-size: 1.0em; color: #CBD5E0; margin-bottom: 10px;\">Handles core Large Language Model interactions such as loading models, generating responses, streaming, and warming up models.</p>\n",
    "        <pre style=\"background-color: #384356; color: #B0BEC5; padding: 10px; border-radius: 5px; font-size: 0.9em; overflow-x: auto;\">\n",
    "# llm_core.py\n",
    "import os, sys, time, threading, torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
    "from config import Config\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LLMCore:\n",
    "    def __init__(self, config=None):\n",
    "        self.config = config or Config.DEFAULTS\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "\n",
    "    def load_model(self):\n",
    "        # Load model and tokenizer\n",
    "        model_name = self.config[\"model_name\"]\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=(torch.float16 if torch.cuda.is_available() else torch.float32),\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        logger.info(f\"Model '{model_name}' and tokenizer loaded.\")\n",
    "\n",
    "    def generate_response(self, messages, max_tokens):\n",
    "        # Generate a response based on messages input\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            self.load_model()\n",
    "\n",
    "        # Prepare prompt and inputs\n",
    "        if hasattr(self.tokenizer, \"apply_chat_template\"):\n",
    "            raw_text = self.tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "        else:\n",
    "            roles_to_prefix = {\"system\": \"[System]\", \"user\": \"[User]\", \"assistant\": \"[Assistant]\"}\n",
    "            raw_text = \"\\n\".join(f\"{roles_to_prefix.get(m['role'], m['role'].upper())}: {m['content']}\" for m in messages)\n",
    "\n",
    "        model_inputs = self.tokenizer([raw_text], return_tensors=\"pt\").to(self.config[\"device\"])\n",
    "        with torch.no_grad():\n",
    "            output = self.model.generate(\n",
    "                **model_inputs,\n",
    "                max_new_tokens=max_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.8,\n",
    "                repetition_penalty=1.05,\n",
    "            )\n",
    "        decoded_output = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        return {\"choices\": [{\"message\": {\"content\": decoded_output}}]}\n",
    "\n",
    "    def stream_chat(self, messages):\n",
    "        # Implements streaming chat logic\n",
    "        # (Could further separate token streaming similar to _generate_tokens)\n",
    "        pass\n",
    "\n",
    "    def warm_up_model(self, warm_up_prompt, second_prompt):\n",
    "        # Warm-up sequence using provided prompts\n",
    "        pass\n",
    "        </pre>\n",
    "    </div>\n",
    "\n",
    "    <div style=\"margin-bottom: 20px;\">\n",
    "        <h3 style=\"color: #A0AEC0; font-size: 1.3em; margin-bottom: 10px; border-bottom: 1px solid #4A5568; padding-bottom: 5px;\">3. <code style=\"color: #81E6D9;\">nlp_module.py</code></h3>\n",
    "        <p style=\"font-size: 1.0em; color: #CBD5E0; margin-bottom: 10px;\">Handles NLP-related functionalities like sentiment analysis, key phrase extraction, POS tagging, named entity recognition, and clustering.</p>\n",
    "        <pre style=\"background-color: #384356; color: #B0BEC5; padding: 10px; border-radius: 5px; font-size: 0.9em; overflow-x: auto;\">\n",
    "# nlp_module.py\n",
    "\n",
    "class NLPProcessor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def analyze_sentiment(self, text):\n",
    "        # Placeholder for sentiment analysis\n",
    "        return {\"sentiment\": 0.0}\n",
    "\n",
    "    def extract_key_phrases(self, text):\n",
    "        # Placeholder for key phrase extraction\n",
    "        return []\n",
    "\n",
    "    def extract_pos_tags(self, text):\n",
    "        # Placeholder for POS tagging\n",
    "        return []\n",
    "\n",
    "    def extract_named_entities(self, text):\n",
    "        # Placeholder for named entity recognition\n",
    "        return []\n",
    "\n",
    "    def advanced_sentiment_analysis(self, text):\n",
    "        # Placeholder for advanced sentiment analysis\n",
    "        return {\"sentiment\": 0.0}\n",
    "\n",
    "    def contextual_key_phrases(self, text):\n",
    "        # Placeholder for contextual key phrase extraction\n",
    "        return []\n",
    "\n",
    "    def cluster_responses(self, responses):\n",
    "        # Placeholder for clustering logic\n",
    "        return {\"cluster_1\": list(range(len(responses)))}\n",
    "        </pre>\n",
    "    </div>\n",
    "\n",
    "    <div style=\"margin-bottom: 20px;\">\n",
    "        <h3 style=\"color: #FC8181; font-size: 1.3em; margin-bottom: 10px; border-bottom: 1px solid #4A5568; padding-bottom: 5px;\">4. <code style=\"color: #81E6D9;\">text_processing.py</code></h3>\n",
    "        <p style=\"font-size: 1.0em; color: #CBD5E0; margin-bottom: 10px;\">Manages text and prompt creation, including critique and refinement plan prompts.</p>\n",
    "        <pre style=\"background-color: #384356; color: #B0BEC5; padding: 10px; border-radius: 5px; font-size: 0.9em; overflow-x: auto;\">\n",
    "# text_processing.py\n",
    "\n",
    "class PromptFactory:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def create_base_prompt(self, user_prompt):\n",
    "        return f\"<USER_PROMPT>\\\\n{user_prompt}\\\\n</USER_PROMPT>\\\\n\" \\\n",
    "               f\"<INSTRUCTIONS>\\\\nYou are an advanced AI assistant...\\\\n</INSTRUCTIONS>\\\\n\"\n",
    "\n",
    "    def add_previous_assessments(self, prompt, previous_assessments):\n",
    "        if previous_assessments:\n",
    "            prompt += \"<PREVIOUS_ASSESSMENTS>\\\\n\"\n",
    "            for i, assessment in enumerate(previous_assessments):\n",
    "                prompt += f\"<ASSESSMENT_{i+1}>\\\\n{assessment}\\\\n</ASSESSMENT_{i+1}>\\\\n\"\n",
    "            prompt += \"</PREVIOUS_ASSESSMENTS>\\\\n\"\n",
    "        return prompt\n",
    "\n",
    "    def add_cycle_info(self, prompt, cycle):\n",
    "        return prompt + f\"<CYCLE>\\\\n{cycle}\\\\n</CYCLE>\\\\n\"\n",
    "\n",
    "    def add_response_under_review(self, prompt, response):\n",
    "        return prompt + f\"<RESPONSE_UNDER_REVIEW>\\\\n{response}\\\\n</RESPONSE_UNDER_REVIEW>\\\\n\"\n",
    "\n",
    "    def create_critique_prompt(self, user_prompt, initial_response, previous_assessments, cycle):\n",
    "        prompt = self.create_base_prompt(user_prompt)\n",
    "        prompt = self.add_cycle_info(prompt, cycle)\n",
    "        prompt = self.add_response_under_review(prompt, initial_response)\n",
    "        prompt = self.add_previous_assessments(prompt, previous_assessments)\n",
    "        prompt += \"<CRITIQUE_INSTRUCTIONS>Provide specific feedback...</CRITIQUE_INSTRUCTIONS>\\\\n<PROMPT_END>\"\n",
    "        return prompt\n",
    "\n",
    "    def create_refinement_plan_prompt(self, user_prompt, initial_response, assessments):\n",
    "        prompt = self.create_base_prompt(user_prompt)\n",
    "        prompt = self.add_response_under_review(prompt, initial_response)\n",
    "        prompt += \"<ASSESSMENTS>\\\\n\"\n",
    "        for i, assessment in enumerate(assessments):\n",
    "            prompt += f\"<ASSESSMENT_{i+1}>\\\\n{assessment}\\\\n</ASSESSMENT_{i+1}>\\\\n\"\n",
    "        prompt += \"</ASSESSMENTS>\\\\n\"\n",
    "        prompt += \"<REFINEMENT_INSTRUCTIONS>Formulate a detailed plan...</REFINEMENT_INSTRUCTIONS>\\\\n<PROMPT_END>\"\n",
    "        return prompt\n",
    "\n",
    "    def create_refined_response_prompt(self, user_prompt, initial_response, refinement_plan):\n",
    "        prompt = self.create_base_prompt(user_prompt)\n",
    "        prompt = self.add_response_under_review(prompt, initial_response)\n",
    "        prompt += f\"<REFINEMENT_PLAN>\\\\n{refinement_plan}\\\\n</REFINEMENT_PLAN>\\\\n\"\n",
    "        prompt += \"<REFINED_RESPONSE_INSTRUCTIONS>Generate a refined response...</REFINED_RESPONSE_INSTRUCTIONS>\\\\n<PROMPT_END>\"\n",
    "        return prompt\n",
    "        </pre>\n",
    "    </div>\n",
    "\n",
    "    <div style=\"margin-bottom: 20px;\">\n",
    "        <h3 style=\"color: #F6AD55; font-size: 1.3em; margin-bottom: 10px; border-bottom: 1px solid #4A5568; padding-bottom: 5px;\">5. <code style=\"color: #81E6D9;\">input_processing.py</code></h3>\n",
    "        <p style=\"font-size: 1.0em; color: #CBD5E0; margin-bottom: 10px;\">Handles input parsing, validation, and routing of incoming messages to appropriate modules.</p>\n",
    "        <pre style=\"background-color: #384356; color: #B0BEC5; padding: 10px; border-radius: 5px; font-size: 0.9em; overflow-x: auto;\">\n",
    "# input_processing.py\n",
    "\n",
    "class InputProcessor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def process_messages(self, messages):\n",
    "        # Validate and preprocess messages if needed\n",
    "        return messages\n",
    "\n",
    "    def detect_interruption(self, messages, cycle_messages):\n",
    "        # Logic to detect new user input interrupting current cycle\n",
    "        return len(messages) > len(cycle_messages)\n",
    "        </pre>\n",
    "    </div>\n",
    "\n",
    "    <div style=\"margin-bottom: 20px;\">\n",
    "        <h3 style=\"color: #68D391; font-size: 1.3em; margin-bottom: 10px; border-bottom: 1px solid #4A5568; padding-bottom: 5px;\">6. <code style=\"color: #81E6D9;\">math_logic.py</code></h3>\n",
    "        <p style=\"font-size: 1.0em; color: #CBD5E0; margin-bottom: 10px;\">Contains any advanced math or logic operations; placeholder for extension.</p>\n",
    "        <pre style=\"background-color: #384356; color: #B0BEC5; padding: 10px; border-radius: 5px; font-size: 0.9em; overflow-x: auto;\">\n",
    "# math_logic.py\n",
    "\n",
    "class MathLogic:\n",
    "    @staticmethod\n",
    "    def compute_rating(response_text):\n",
    "        # Compute a simple rating (e.g., length-based)\n",
    "        return len(response_text)\n",
    "\n",
    "    # Add more math/logic functions as needed\n",
    "        </pre>\n",
    "    </div>\n",
    "\n",
    "    <div style=\"margin-bottom: 20px;\">\n",
    "        <h3 style=\"color: #A0AEC0; font-size: 1.3em; margin-bottom: 10px; border-bottom: 1px solid #4A5568; padding-bottom: 5px;\">7. <code style=\"color: #81E6D9;\">thought_routing.py</code></h3>\n",
    "        <p style=\"font-size: 1.0em; color: #CBD5E0; margin-bottom: 10px;\">Coordinates overall logic, integrating LLM core, NLP analysis, text processing, input handling, and other modules to perform the iterative refinement cycles.</p>\n",
    "        <pre style=\"background-color: #384356; color: #B0BEC5; padding: 10px; border-radius: 5px; font-size: 0.9em; overflow-x: auto;\">\n",
    "# thought_routing.py\n",
    "import logging\n",
    "from llm_core import LLMCore\n",
    "from nlp_module import NLPProcessor\n",
    "from text_processing import PromptFactory\n",
    "from input_processing import InputProcessor\n",
    "from math_logic import MathLogic\n",
    "from config import Config\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ThoughtRouter:\n",
    "    def __init__(self, config=None):\n",
    "        self.config = config or Config.DEFAULTS\n",
    "        self.llm_core = LLMCore(config=self.config)\n",
    "        self.nlp_processor = NLPProcessor(config=self.config)\n",
    "        self.prompt_factory = PromptFactory(config=self.config)\n",
    "        self.input_processor = InputProcessor(config=self.config)\n",
    "\n",
    "    def chat(self, messages, secondary_llm=None, **kwargs):\n",
    "        # Refactored high-level chat logic using modules\n",
    "        messages = self.input_processor.process_messages(messages)\n",
    "        # ... implement the chat iteration logic here, leveraging llm_core, prompt_factory, nlp_processor, etc.\n",
    "        # This would include cycles, generating responses, assessing, refining, NLP analysis, etc.\n",
    "        pass\n",
    "        </pre>\n",
    "    </div>\n",
    "\n",
    "    <div style=\"margin-bottom: 20px;\">\n",
    "        <h3 style=\"color: #FC8181; font-size: 1.3em; margin-bottom: 10px; border-bottom: 1px solid #4A5568; padding-bottom: 5px;\">8. <code style=\"color: #81E6D9;\">main.py</code></h3>\n",
    "        <p style=\"font-size: 1.0em; color: #CBD5E0; margin-bottom: 10px;\">Entry point for running the application.</p>\n",
    "        <pre style=\"background-color: #384356; color: #B0BEC5; padding: 10px; border-radius: 5px; font-size: 0.9em; overflow-x: auto;\">\n",
    "# main.py\n",
    "import logging\n",
    "from thought_routing import ThoughtRouter\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def main():\n",
    "    router = ThoughtRouter()\n",
    "    # Example messages\n",
    "    messages = [{\"role\": \"user\", \"content\": \"Hello, Eidos!\"}]\n",
    "    response = router.chat(messages)\n",
    "    print(response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "        </pre>\n",
    "    </div>\n",
    "\n",
    "    <p style=\"font-size: 0.9em; color: #B0BEC5; margin-top: 15px; font-style: italic;\">\n",
    "        **Notes:**\n",
    "        <ul>\n",
    "            <li>The above structure splits responsibilities across modules.</li>\n",
    "            <li>The actual detailed implementation of the iterative <code>chat</code> logic inside <code>ThoughtRouter.chat</code> is omitted for brevity, but it should orchestrate calls to the <code>LLMCore</code>, utilize prompt creation from <code>PromptFactory</code>, perform NLP analysis using <code>NLPProcessor</code>, and so on, closely following the logic provided in the original <code>chat</code> method.</li>\n",
    "            <li>Each module contains placeholder implementations where necessary. You can expand these methods with specific algorithmic details as needed.</li>\n",
    "            <li>This modular design improves maintainability, readability, and extensibility of the project.</li>\n",
    "        </ul>\n",
    "    </p>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
