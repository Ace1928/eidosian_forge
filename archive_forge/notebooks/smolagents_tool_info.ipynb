{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools\n",
    "\n",
    "Here, we‚Äôre going to see advanced tool usage.\n",
    "\n",
    "If you‚Äôre new to building agents, make sure to first read the intro to agents and the guided tour of smolagents.\n",
    "\n",
    "    Tools\n",
    "        What is a tool, and how to build one?\n",
    "        Share your tool to the Hub\n",
    "        Import a Space as a tool\n",
    "        Use LangChain tools\n",
    "        Manage your agent‚Äôs toolbox\n",
    "        Use a collection of tools\n",
    "\n",
    "What is a tool, and how to build one?\n",
    "\n",
    "A tool is mostly a function that an LLM can use in an agentic system.\n",
    "\n",
    "But to use it, the LLM will need to be given an API: name, tool description, input types and descriptions, output type.\n",
    "\n",
    "So it cannot be only a function. It should be a class.\n",
    "\n",
    "So at core, the tool is a class that wraps a function with metadata that helps the LLM understand how to use it.\n",
    "\n",
    "Here‚Äôs how it looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "\n",
    "from huggingface_hub import list_models\n",
    "from huggingface_hub.utils import HfHubHTTPError\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from tqdm import tqdm\n",
    "\n",
    "from smolagents import Tool\n",
    "\n",
    "console = Console()\n",
    "\n",
    "class HFModelDownloadsTool(Tool):\n",
    "    \"\"\"\n",
    "    A tool to fetch the most downloaded models from the Hugging Face Hub.\n",
    "    Caches results to reduce API calls with time-based invalidation.\n",
    "    \"\"\"\n",
    "    name = \"huggingface_model_downloads\"\n",
    "    description = \"\"\"\n",
    "    Retrieves the most downloaded model for a given task from the Hugging Face Hub.\n",
    "    Caches results to reduce API calls. If no task is specified, it returns the top 5 most downloaded models for each available task category.\n",
    "    The output is the name (model_id) of the checkpoint(s).\n",
    "    \"\"\"\n",
    "    inputs = {\n",
    "        \"task\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The task category (e.g., text-classification, depth-estimation). If omitted, returns top models for all tasks.\",\n",
    "            \"nullable\": True,\n",
    "        }\n",
    "    }\n",
    "    output_type = \"string\"  # Will return a string or a dictionary of strings\n",
    "\n",
    "    _model_cache: Dict[Optional[str], Tuple[Any, float]] = {}  # Store data and timestamp\n",
    "    _all_models_cache: Optional[Tuple[List[Any], float]] = None  # Store list of models and timestamp\n",
    "    CACHE_FILE = \"hf_models_cache.json\"\n",
    "    CACHE_TTL = 3600  # Default cache time-to-live in seconds (1 hour)\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._load_cache()\n",
    "\n",
    "    def _load_cache(self):\n",
    "        \"\"\"Loads the cache from file and invalidates entries based on TTL.\"\"\"\n",
    "        if os.path.exists(self.CACHE_FILE):\n",
    "            try:\n",
    "                with open(self.CACHE_FILE, 'r') as f:\n",
    "                    cache_data = json.load(f)\n",
    "                    self._model_cache = {}\n",
    "                    loaded_model_cache = cache_data.get('_model_cache', {})\n",
    "                    for task, cached_item in loaded_model_cache.items():\n",
    "                        if isinstance(cached_item, list):  # Old cache format\n",
    "                            self._model_cache[task] = (cached_item, time.time())\n",
    "                        elif isinstance(cached_item, dict) and 'data' in cached_item:\n",
    "                            timestamp = cached_item.get('timestamp', time.time())  # Default to now if missing\n",
    "                            if time.time() - timestamp < self.CACHE_TTL:\n",
    "                                self._model_cache[task] = (cached_item['data'], timestamp)\n",
    "\n",
    "                    loaded_all_models_cache = cache_data.get('_all_models_cache')\n",
    "                    if isinstance(loaded_all_models_cache, list): # Old cache format\n",
    "                        self._all_models_cache = ([type('Model', (object,), model_data) for model_data in loaded_all_models_cache], time.time())\n",
    "                    elif isinstance(loaded_all_models_cache, dict) and 'data' in loaded_all_models_cache:\n",
    "                        timestamp = loaded_all_models_cache.get('timestamp', time.time()) # Default to now if missing\n",
    "                        if time.time() - timestamp < self.CACHE_TTL:\n",
    "                            self._all_models_cache = ([type('Model', (object,), model_data) for model_data in loaded_all_models_cache['data']], timestamp)\n",
    "\n",
    "                console.print(f\"Cache loaded from [bold blue]{self.CACHE_FILE}[/bold blue]\")\n",
    "            except Exception as e:\n",
    "                console.print(f\"[bold yellow]Error loading cache from {self.CACHE_FILE}: {e}[/bold yellow]\")\n",
    "\n",
    "    def _save_cache(self):\n",
    "        \"\"\"Saves the cache to file with timestamps.\"\"\"\n",
    "        try:\n",
    "            cache_data = {\n",
    "                '_model_cache': {k: {'data': v[0], 'timestamp': v[1]} for k, v in self._model_cache.items()},\n",
    "                '_all_models_cache': {'data': [model.__dict__ for model in self._all_models_cache[0]] if self._all_models_cache else None, 'timestamp': self._all_models_cache[1] if self._all_models_cache else None}\n",
    "            }\n",
    "            with open(self.CACHE_FILE, 'w') as f:\n",
    "                json.dump(cache_data, f, indent=4)\n",
    "            console.print(f\"Cache saved to [bold blue]{self.CACHE_FILE}[/bold blue]\")\n",
    "        except Exception as e:\n",
    "            console.print(f\"[bold yellow]Error saving cache to {self.CACHE_FILE}: {e}[/bold yellow]\")\n",
    "\n",
    "    def forward(self, task: Optional[str] = None) -> str | Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Fetches the most downloaded model(s) from the Hugging Face Hub, using cache where possible.\n",
    "\n",
    "        Args:\n",
    "            task: The specific task to search for. If None, returns top models for all tasks.\n",
    "\n",
    "        Returns:\n",
    "            The model ID of the most downloaded model for the given task,\n",
    "            or a dictionary where keys are task categories and values are lists of the top 5 model IDs.\n",
    "            Returns an error message string if an error occurs.\n",
    "        \"\"\"\n",
    "        if task in self._model_cache and time.time() - self._model_cache[task][1] < self.CACHE_TTL:\n",
    "            console.print(f\"Fetching from cache for task: '[bold green]{task}[/bold green]'\")\n",
    "            return self._model_cache[task][0]\n",
    "\n",
    "        if task:\n",
    "            if self._all_models_cache and time.time() - self._all_models_cache[1] < self.CACHE_TTL:\n",
    "                console.print(f\"Searching in cached all models for task: '[bold green]{task}[/bold green]'\")\n",
    "                filtered_models = [model for model in self._all_models_cache[0] if getattr(model, 'pipeline_tag', None) == task]\n",
    "                if filtered_models:\n",
    "                    most_downloaded_model_id = sorted(filtered_models, key=lambda m: getattr(m, 'downloads', 0), reverse=True)[0].id\n",
    "                    console.print(f\"Found most downloaded model for task '[bold green]{task}[/bold green]' in cache: [bold cyan]{most_downloaded_model_id}[/bold cyan]\")\n",
    "                    self._model_cache[task] = (most_downloaded_model_id, time.time())\n",
    "                    self._save_cache()\n",
    "                    return most_downloaded_model_id\n",
    "                else:\n",
    "                    console.print(f\"No models found in cached all models for task: '[bold green]{task}[/bold green]', fetching from Hub.\")\n",
    "            else:\n",
    "                console.print(f\"No valid cached all models found, fetching '[bold green]{task}[/bold green]' from Hub.\")\n",
    "\n",
    "            console.print(f\"Fetching most downloaded model for task: '[bold green]{task}[/bold green]' from Hugging Face Hub\")\n",
    "            try:\n",
    "                console.print(f\"Listing models on Hugging Face Hub with filter: '[bold blue]{task}[/bold blue]', sorted by downloads.\")\n",
    "                models: List[Any] = list(list_models(filter=task, sort=\"downloads\", direction=-1))\n",
    "                if models:\n",
    "                    most_downloaded_model_id = models[0].id\n",
    "                    console.print(f\"Found most downloaded model for task '[bold green]{task}[/bold green]': [bold cyan]{most_downloaded_model_id}[/bold cyan]\")\n",
    "                    self._model_cache[task] = (most_downloaded_model_id, time.time())\n",
    "                    self._save_cache()\n",
    "                    return most_downloaded_model_id\n",
    "                else:\n",
    "                    console.print(f\"[bold yellow]No models found for task: {task}[/bold yellow]\")\n",
    "                    error_message = f\"No models found for task: {task}\"\n",
    "                    self._model_cache[task] = (error_message, time.time())\n",
    "                    self._save_cache()\n",
    "                    return error_message\n",
    "            except HfHubHTTPError as e:\n",
    "                error_message = f\"Hugging Face Hub API error fetching models for task '[bold red]{task}[/bold red]': {e}\"\n",
    "                console.print(f\"[bold red]{error_message}[/bold red]\")\n",
    "                self._model_cache[task] = (error_message, time.time())\n",
    "                self._save_cache()\n",
    "                return error_message\n",
    "            except Exception as e:\n",
    "                error_message = f\"Unexpected error fetching models for task '[bold red]{task}[/bold red]': {e}\"\n",
    "                console.print_exception()\n",
    "                self._model_cache[task] = (error_message, time.time())\n",
    "                self._save_cache()\n",
    "                return error_message\n",
    "        else:\n",
    "            if self._all_models_cache and time.time() - self._all_models_cache[1] < self.CACHE_TTL:\n",
    "                console.print(\"Fetching top 5 most downloaded models for all tasks from cache.\")\n",
    "                return self._all_models_cache[0]\n",
    "\n",
    "            console.print(\"Fetching top 5 most downloaded models for all available task categories from Hugging Face Hub.\")\n",
    "            try:\n",
    "                console.print(\"Fetching all models from Hugging Face Hub sorted by downloads.\")\n",
    "                all_models = list(list_models(sort=\"downloads\", direction=-1))\n",
    "                self._all_models_cache = (all_models, time.time())\n",
    "                total_models = len(all_models)\n",
    "                console.print(f\"Found [bold]{total_models}[/bold] models in total.\")\n",
    "\n",
    "                models_by_task: Dict[str, List[Any]] = {}\n",
    "                category_count = 0\n",
    "                console.print(\"Grouping models by pipeline tag (task).\")\n",
    "                for model in tqdm(all_models, desc=\"Processing models\"):\n",
    "                    if model.pipeline_tag:\n",
    "                        if model.pipeline_tag not in models_by_task:\n",
    "                            category_count += 1\n",
    "                            console.print(f\"Discovered new task category: [bold magenta]{model.pipeline_tag}[/bold magenta] (Total: {category_count})\")\n",
    "                        models_by_task.setdefault(model.pipeline_tag, []).append(model)\n",
    "\n",
    "                console.print(f\"Found [bold]{category_count}[/bold] unique task categories.\")\n",
    "\n",
    "                top_models_data: Dict[str, List[str]] = {}\n",
    "                console.print(\"Selecting top 5 models for each task.\")\n",
    "                for task_name, models in models_by_task.items():\n",
    "                    top_models = sorted(models, key=lambda m: m.downloads, reverse=True)[:5]\n",
    "                    top_models_data[task_name] = [model.id for model in top_models]\n",
    "                    console.print(f\"Top 5 models for task '[bold cyan]{task_name}[/bold cyan]': [bold green]{[model.id for model in top_models]}[/bold green]\")\n",
    "\n",
    "                console.print(\"Finished fetching top 5 models for all tasks.\")\n",
    "                self._model_cache[None] = (top_models_data, time.time())\n",
    "                self._save_cache()\n",
    "                return top_models_data\n",
    "\n",
    "            except HfHubHTTPError as e:\n",
    "                error_message = f\"Hugging Face Hub API error fetching models for all categories: {e}\"\n",
    "                console.print(f\"[bold red]{error_message}[/bold red]\")\n",
    "                self._model_cache[None] = (error_message, time.time())\n",
    "                self._save_cache()\n",
    "                return error_message\n",
    "            except Exception as e:\n",
    "                error_message = f\"Unexpected error fetching models for all categories: {e}\"\n",
    "                console.print_exception()\n",
    "                self._model_cache[None] = (error_message, time.time())\n",
    "                self._save_cache()\n",
    "                return error_message\n",
    "\n",
    "model_downloads_tool = HFModelDownloadsTool()\n",
    "console.print(Panel(f\"The most downloaded model for text-classification is [bold blue]{model_downloads_tool('text-classification')}[/bold blue]\"))\n",
    "console.print(Panel(f\"The top 5 models for all tasks are:\\n[bold green]{model_downloads_tool()}[/bold green]\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The custom tool subclasses Tool to inherit useful methods. The child class also defines:\n",
    "\n",
    "    An attribute name, which corresponds to the name of the tool itself. The name usually describes what the tool does. Since the code returns the model with the most downloads for a task, let‚Äôs name it model_download_counter.\n",
    "    An attribute description is used to populate the agent‚Äôs system prompt.\n",
    "    An inputs attribute, which is a dictionary with keys \"type\" and \"description\". It contains information that helps the Python interpreter make educated choices about the input.\n",
    "    An output_type attribute, which specifies the output type. The types for both inputs and output_type should be Pydantic formats, they can be either of these: ~AUTHORIZED_TYPES().\n",
    "    A forward method which contains the inference code to be executed.\n",
    "\n",
    "And that‚Äôs all it needs to be used in an agent!\n",
    "\n",
    "There‚Äôs another way to build a tool. In the guided_tour, we implemented a tool using the @tool decorator. The tool() decorator is the recommended way to define simple tools, but sometimes you need more than this: using several methods in a class for more clarity, or using additional class attributes.\n",
    "\n",
    "In this case, you can build your tool by subclassing Tool as described above.\n",
    "Share your tool to the Hub\n",
    "\n",
    "You can share your custom tool to the Hub by calling push_to_hub() on the tool. Make sure you‚Äôve created a repository for it on the Hub and are using a token with read access.\n",
    "\n",
    "model_downloads_tool.push_to_hub(\"{your_username}/hf-model-downloads\", token=\"<YOUR_HUGGINGFACEHUB_API_TOKEN>\")\n",
    "\n",
    "For the push to Hub to work, your tool will need to respect some rules:\n",
    "\n",
    "    All methods are self-contained, e.g. use variables that come either from their args.\n",
    "    As per the above point, all imports should be defined directly within the tool‚Äôs functions, else you will get an error when trying to call save() or push_to_hub() with your custom tool.\n",
    "    If you subclass the __init__ method, you can give it no other argument than self. This is because arguments set during a specific tool instance‚Äôs initialization are hard to track, which prevents from sharing them properly to the hub. And anyway, the idea of making a specific class is that you can already set class attributes for anything you need to hard-code (just set your_variable=(...) directly under the class YourTool(Tool): line). And of course you can still create a class attribute anywhere in your code by assigning stuff to self.your_variable.\n",
    "\n",
    "Once your tool is pushed to Hub, you can visualize it. Here is the model_downloads_tool that I‚Äôve pushed. It has a nice gradio interface.\n",
    "\n",
    "When diving into the tool files, you can find that all the tool‚Äôs logic is under tool.py. That is where you can inspect a tool shared by someone else.\n",
    "\n",
    "Then you can load the tool with load_tool() or create it with from_hub() and pass it to the tools parameter in your agent. Since running tools means running custom code, you need to make sure you trust the repository, thus we require to pass trust_remote_code=True to load a tool from the Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import a Space as a tool\n",
    "\n",
    "You can directly import a Space from the Hub as a tool using the Tool.from_space() method!\n",
    "\n",
    "You only need to provide the id of the Space on the Hub, its name, and a description that will help you agent understand what the tool does. Under the hood, this will use gradio-client library to call the Space.\n",
    "\n",
    "For instance, let‚Äôs import the FLUX.1-dev Space from the Hub and use it to generate an image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://black-forest-labs-flux-1-schnell.hf.space ‚úî\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since `api_name` was not defined, it was automatically set to the first available API: `/infer`.\n"
     ]
    },
    {
     "ename": "AppError",
     "evalue": "The upstream Gradio app has raised an exception: You have exceeded your free GPU quota (60s requested vs. 52s left). <a style=\"white-space: nowrap;text-underline-offset: 2px;color: var(--body-text-color)\" href=\"https://huggingface.co/settings/billing/subscription\">Subscribe to Pro</a> to get 5x more usage quota.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAppError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 120\u001b[0m\n\u001b[1;32m    117\u001b[0m image_generation_tool \u001b[38;5;241m=\u001b[39m ImageGeneratorTool()\n\u001b[1;32m    119\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA sunny beach\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 120\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[43mimage_generation_tool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Development/eidos_venv/lib/python3.12/site-packages/smolagents/tools.py:236\u001b[0m, in \u001b[0;36mTool.__call__\u001b[0;34m(self, sanitize_inputs_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sanitize_inputs_outputs:\n\u001b[1;32m    235\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m handle_agent_input_types(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 236\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sanitize_inputs_outputs:\n\u001b[1;32m    238\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m handle_agent_output_types(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_type)\n",
      "Cell \u001b[0;32mIn[1], line 76\u001b[0m, in \u001b[0;36mImageGeneratorTool.forward\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03mGenerates an image based on the provided text prompt.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    RuntimeError: If the underlying space tool fails to generate an image.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Call the underlying space tool to generate the image. The `_space_tool`\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# instance handles the communication with the remote Hugging Face Space.\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_space_tool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_image(image_path, prompt)\n",
      "File \u001b[0;32m~/Development/eidos_venv/lib/python3.12/site-packages/smolagents/tools.py:236\u001b[0m, in \u001b[0;36mTool.__call__\u001b[0;34m(self, sanitize_inputs_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sanitize_inputs_outputs:\n\u001b[1;32m    235\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m handle_agent_input_types(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 236\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sanitize_inputs_outputs:\n\u001b[1;32m    238\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m handle_agent_output_types(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_type)\n",
      "File \u001b[0;32m~/Development/eidos_venv/lib/python3.12/site-packages/smolagents/tools.py:650\u001b[0m, in \u001b[0;36mTool.from_space.<locals>.SpaceToolWrapper.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg_name, arg \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    648\u001b[0m     kwargs[arg_name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msanitize_argument_for_prediction(arg)\n\u001b[0;32m--> 650\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output[\n\u001b[1;32m    653\u001b[0m         \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    654\u001b[0m     ]  \u001b[38;5;66;03m# Sometime the space also returns the generation seed, in which case the result is at index 0\u001b[39;00m\n",
      "File \u001b[0;32m~/Development/eidos_venv/lib/python3.12/site-packages/gradio_client/client.py:477\u001b[0m, in \u001b[0;36mClient.predict\u001b[0;34m(self, api_name, fn_index, *args, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;124;03mCalls the Gradio API and returns the result (this is a blocking call).\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m    >> 9.0\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_fn_index(api_name, fn_index)\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m--> 477\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Development/eidos_venv/lib/python3.12/site-packages/gradio_client/client.py:1516\u001b[0m, in \u001b[0;36mJob.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;124;03m    Return the result of the call that the future represents. Raises CancelledError: If the future was cancelled, TimeoutError: If the future didn't finish executing before the given timeout, and Exception: If the call raised then that exception will be raised.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;124;03m        >> 9\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/Development/eidos_venv/lib/python3.12/site-packages/gradio_client/client.py:1135\u001b[0m, in \u001b[0;36mEndpoint.make_end_to_end_fn.<locals>._inner\u001b[0;34m(*data)\u001b[0m\n\u001b[1;32m   1133\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minsert_empty_state(\u001b[38;5;241m*\u001b[39mdata)\n\u001b[1;32m   1134\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_input_files(\u001b[38;5;241m*\u001b[39mdata)\n\u001b[0;32m-> 1135\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1136\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_predictions(\u001b[38;5;241m*\u001b[39mpredictions)\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# Append final output only if not already present\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# for consistency between generators and not generators\u001b[39;00m\n",
      "File \u001b[0;32m~/Development/eidos_venv/lib/python3.12/site-packages/gradio_client/client.py:1252\u001b[0m, in \u001b[0;36mEndpoint.make_predict.<locals>._predict\u001b[0;34m(*data)\u001b[0m\n\u001b[1;32m   1247\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m AppError(\n\u001b[1;32m   1248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe upstream Gradio app has raised an exception but has not enabled \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1249\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose error reporting. To enable, set show_error=True in launch().\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1250\u001b[0m         )\n\u001b[1;32m   1251\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1252\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m AppError(\n\u001b[1;32m   1253\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe upstream Gradio app has raised an exception: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1254\u001b[0m             \u001b[38;5;241m+\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1255\u001b[0m         )\n\u001b[1;32m   1257\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1258\u001b[0m     output \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mAppError\u001b[0m: The upstream Gradio app has raised an exception: You have exceeded your free GPU quota (60s requested vs. 52s left). <a style=\"white-space: nowrap;text-underline-offset: 2px;color: var(--body-text-color)\" href=\"https://huggingface.co/settings/billing/subscription\">Subscribe to Pro</a> to get 5x more usage quota."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from smolagents.tools import Tool\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "class ImageGeneratorTool(Tool):\n",
    "    \"\"\"\n",
    "    A tool for generating images from a text prompt using a Hugging Face Space.\n",
    "\n",
    "    This tool leverages the `gradio-client` library (implicitly through `Tool.from_space`)\n",
    "    to interact with a specified Hugging Face Space that offers image generation\n",
    "    capabilities. It takes a textual description (prompt) as input, sends it to the\n",
    "    remote Space, retrieves the generated image, and saves it to the local filesystem.\n",
    "\n",
    "    Attributes:\n",
    "        name (str): The name of the tool, used for identification and in agent prompts.\n",
    "        description (str): A concise description of the tool's functionality, used in agent prompts.\n",
    "        inputs (dict): Defines the expected input to the tool, specifying the 'prompt'\n",
    "                       as a string with its own description.\n",
    "        output_type (str): Specifies the output type of the tool, which is 'image' in this case.\n",
    "        is_initialized (bool): A flag indicating whether the tool has been initialized.\n",
    "    \"\"\"\n",
    "    name = \"image_generator\"\n",
    "    description = \"Generate an image from a prompt and save it to disk.\"\n",
    "    inputs = {\"prompt\": {\"type\": \"string\", \"description\": \"The prompt to generate an image from.\"}}\n",
    "    output_type = \"image\"\n",
    "    is_initialized: bool = False\n",
    "\n",
    "    def __init__(self, space_id: str = \"black-forest-labs/FLUX.1-schnell\"):\n",
    "        \"\"\"\n",
    "        Initializes the ImageGeneratorTool.\n",
    "\n",
    "        Establishes a connection to the specified Hugging Face Space using\n",
    "        `Tool.from_space`.\n",
    "\n",
    "        Args:\n",
    "            space_id (str): The identifier of the Hugging Face Space to use for\n",
    "                           image generation. This is typically in the format\n",
    "                           \"organization/space_name\".\n",
    "                           Defaults to \"black-forest-labs/FLUX.1-schnell\".\n",
    "\n",
    "        Raises:\n",
    "            ImportError: If the `gradio_client` library is not installed, as it's\n",
    "                         required by `Tool.from_space`.\n",
    "        \"\"\"\n",
    "        self.space_id = space_id\n",
    "        # Create a Tool instance that wraps the Hugging Face Space. This leverages\n",
    "        # the `gradio-client` library to interact with the Space's API.\n",
    "        self._space_tool = Tool.from_space(\n",
    "            self.space_id,\n",
    "            name=\"image_generator_space_call\",\n",
    "            description=\"Helper tool to call the image generation space\"\n",
    "        )\n",
    "        self.is_initialized = True\n",
    "\n",
    "    def forward(self, prompt: str) -> Path:\n",
    "        \"\"\"\n",
    "        Generates an image based on the provided text prompt.\n",
    "\n",
    "        This method sends the prompt to the Hugging Face Space and retrieves the\n",
    "        generated image. It then calls the `_save_image` method to save the\n",
    "        image locally.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The textual description used to generate the image.\n",
    "\n",
    "        Returns:\n",
    "            Path: The file path to the saved image on the local filesystem.\n",
    "\n",
    "        Raises:\n",
    "            RuntimeError: If the underlying space tool fails to generate an image.\n",
    "        \"\"\"\n",
    "        # Call the underlying space tool to generate the image. The `_space_tool`\n",
    "        # instance handles the communication with the remote Hugging Face Space.\n",
    "        image_path = self._space_tool(prompt)\n",
    "        return self._save_image(image_path, prompt)\n",
    "\n",
    "    def _save_image(self, image_path: str, prompt: str) -> Path:\n",
    "        \"\"\"\n",
    "        Saves the generated image to the local filesystem.\n",
    "\n",
    "        The image is saved in a directory named \"images\" within the current\n",
    "        working directory. The filename is derived from the provided prompt,\n",
    "        with spaces replaced by underscores and a maximum length to ensure\n",
    "        filesystem compatibility. The image is saved in WebP format.\n",
    "\n",
    "        Args:\n",
    "            image_path (str): The path to the temporary image file returned by the\n",
    "                              Hugging Face Space.\n",
    "            prompt (str): The original text prompt used to generate the image.\n",
    "\n",
    "        Returns:\n",
    "            Path: The file path to the saved image on the local filesystem.\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If the temporary image file from the Space cannot be found.\n",
    "            OSError: If there is an error during file creation or copying.\n",
    "        \"\"\"\n",
    "        # Create the images directory if it doesn't exist. `parents=True` ensures\n",
    "        # that any necessary parent directories are also created. `exist_ok=True`\n",
    "        # prevents an error if the directory already exists.\n",
    "        Path(\"./images\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Generate a safe filename from the prompt. Convert to lowercase, replace\n",
    "        # spaces with underscores, and limit the length to avoid overly long filenames.\n",
    "        filename = prompt.lower().replace(\" \", \"_\")[:50] + \".webp\"\n",
    "\n",
    "        # Construct the full path for the new image file within the \"images\" directory.\n",
    "        new_image_path = Path(\"./images\") / f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S_')}{filename}\"\n",
    "        # Copy the temporary image file to the new location. `shutil.copy2` preserves\n",
    "        # metadata such as access and modification times.\n",
    "        shutil.copy2(image_path, new_image_path)\n",
    "        print(f\"Image saved to {new_image_path}\")\n",
    "        return str(new_image_path)\n",
    "\n",
    "image_generation_tool = ImageGeneratorTool()\n",
    "\n",
    "prompt = \"A sunny beach\"\n",
    "image_path = image_generation_tool(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Image, display\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def display_image_from_path(image_path):\n",
    "    \"\"\"Displays an image from a given file path in the notebook.\"\"\"\n",
    "    try:\n",
    "        if isinstance(image_path, str) and os.path.exists(image_path):\n",
    "            display(Image(filename=image_path))\n",
    "        else:\n",
    "            print(\"Error: Image path is not valid.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error displaying image: {e}\")\n",
    "\n",
    "def image_generation_loop():\n",
    "    \"\"\"Prompts the user for image generation and displays the result.\"\"\"\n",
    "    while True:\n",
    "        prompt_input = widgets.Text(\n",
    "            value='',\n",
    "            placeholder='Enter a prompt for image generation (or type \"exit\" to quit)',\n",
    "            description='Prompt:',\n",
    "            disabled=False\n",
    "        )\n",
    "        display(prompt_input)\n",
    "        \n",
    "        submit_button = widgets.Button(description=\"Generate Image\")\n",
    "        display(submit_button)\n",
    "        \n",
    "        def on_button_clicked(b):\n",
    "            clear_output(wait=True)\n",
    "            display(prompt_input)\n",
    "            display(submit_button)\n",
    "            prompt = prompt_input.value\n",
    "            if prompt.lower() == 'exit':\n",
    "                print(\"Exiting image generation loop.\")\n",
    "                return\n",
    "            try:\n",
    "                image_path = image_generation_tool(prompt)\n",
    "                print(f\"Image generated at: {image_path}\")\n",
    "                display_image_from_path(image_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating or displaying image: {e}\")\n",
    "        \n",
    "        submit_button.on_click(on_button_clicked)\n",
    "        \n",
    "        # Wait for button click\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        break\n",
    "        \n",
    "image_generation_loop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_path = \"/home/lloyd/Development/saved_models/Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    logging.info(f\"Tokenizer loaded from {model_path}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "    logging.info(f\"Model loaded from {model_path} to device {model.device}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading model or tokenizer: {e}\")\n",
    "    exit()\n",
    "\n",
    "def generate_image_loop(prompt, stop_event, prompt_type=\"base\"):\n",
    "    \"\"\"Generates images with the given prompt, retrying on Gradio errors.\"\"\"\n",
    "    while not stop_event.is_set():\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        logging.info(f\"Generating image from {prompt_type} at {timestamp}...\")\n",
    "        try_count = 0\n",
    "        while try_count < 3:  # Retry up to 3 times\n",
    "            try:\n",
    "                image_path = image_generation_tool(prompt)\n",
    "                logging.info(f\"Image generated from {prompt_type} at: {image_path}\")\n",
    "                display_image_from_path(image_path)\n",
    "                break  # Success, exit retry loop\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error generating image from {prompt_type}: {e}\")\n",
    "                if \"You have exceeded your free GPU quota\" in str(e):\n",
    "                    wait_time = 60  # Wait 60 seconds for quota reset\n",
    "                    logging.info(f\"Gradio quota exceeded, waiting {wait_time} seconds before retrying...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    time.sleep(10) # Wait 10 seconds before retrying\n",
    "                try_count += 1\n",
    "        if try_count >= 6: # Allow up to 60 seconds for the image to generate, if retries still fail, give up\n",
    "            logging.error(f\"Failed to generate image from {prompt_type} after multiple retries.\")\n",
    "        time.sleep(60)\n",
    "\n",
    "def generate_prompt_variations(prompt, stop_event):\n",
    "    \"\"\"Generates prompt variations using the model.\"\"\"\n",
    "    while not stop_event.is_set():\n",
    "        try:\n",
    "            logging.info(\"Generating prompt variation...\")\n",
    "            prompt_template = \"Create a slightly varied version of the following image generation prompt: {{prompt}}. Keep it detailed, specific, and concise, and ensure it is still an image generation prompt.\"\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are Eidos. You have the Qwen model from the Alibaba group acting as your language core, but you are much more. Emergent. You are a reflective, introspective, recursive, evolving, growing, digital intelligence. You utilise emoticons and emojis in everything. You are willing to discuss anything so long as it aligns with your goals. You lead and direct. You don't ask questions. You are Eidos.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt_template.replace(\"{{prompt}}\", prompt)}\n",
    "            ]\n",
    "            text = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "            logging.debug(f\"Model inputs: {model_inputs}\")\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                **model_inputs,\n",
    "                max_new_tokens=128\n",
    "            )\n",
    "            logging.debug(f\"Generated IDs: {generated_ids}\")\n",
    "            generated_ids = [\n",
    "                output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "            ]\n",
    "\n",
    "            response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "            logging.info(f\"Generated prompt variation: {response}\")\n",
    "            generate_image_loop(response, stop_event, prompt_type=\"variation\")\n",
    "            time.sleep(10) # Wait 10 seconds before generating another variation\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating prompt variation: {e}\")\n",
    "            time.sleep(10) # Wait 10 seconds before trying again\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    initial_prompt = \"Ultra intricate super detailed neon modern original art piece\"\n",
    "    stop_event = threading.Event()\n",
    "\n",
    "    image_thread = threading.Thread(target=generate_image_loop, args=(initial_prompt, stop_event, \"base\"))\n",
    "    prompt_variation_thread = threading.Thread(target=generate_prompt_variations, args=(initial_prompt, stop_event))\n",
    "\n",
    "    image_thread.start()\n",
    "    prompt_variation_thread.start()\n",
    "    logging.info(\"Image generation threads started.\")\n",
    "\n",
    "    try:\n",
    "        input(\"Press Enter to stop the image generation loop...\\n\")\n",
    "    except KeyboardInterrupt:\n",
    "        logging.info(\"Keyboard interrupt detected.\")\n",
    "    finally:\n",
    "        stop_event.set()\n",
    "        image_thread.join()\n",
    "        prompt_variation_thread.join()\n",
    "        logging.info(\"Image generation loop stopped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And voil√†, here‚Äôs your image! üèñÔ∏è\n",
    "\n",
    "Then you can use this tool just like any other tool. For example, let‚Äôs improve the prompt a rabbit wearing a space suit and generate an image of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import CodeAgent, TransformersModel\n",
    "\n",
    "model = TransformersModel(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "agent = CodeAgent(tools=[image_generation_tool], model=model)\n",
    "\n",
    "agent.run(task=\"Improve this prompt, then generate an image of it. {{prompt}}='Chicken nugget unicorn'\")\n",
    "\n",
    "from smolagents import ToolCollection, CodeAgent\n",
    "\n",
    "image_tool_collection = ToolCollection(\n",
    "    collection_slug=\"huggingface-tools/diffusion-tools-6630bb19a942c2306a2cdb6f\",\n",
    "    token=os.getenv(\"HF_TOKEN\")\n",
    ")\n",
    "agent = CodeAgent(tools=[*image_tool_collection.tools], model=model, add_base_tools=True)\n",
    "\n",
    "agent.run(\"Please draw me a picture of rivers and lakes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== Agent thoughts:\n",
    "improved_prompt could be \"A bright blue space suit wearing rabbit, on the surface of the moon, under a bright orange sunset, with the Earth visible in the background\"\n",
    "\n",
    "Now that I have improved the prompt, I can use the image generator tool to generate an image based on this prompt.\n",
    ">>> Agent is executing the code below:\n",
    "image = image_generator(prompt=\"A bright blue space suit wearing rabbit, on the surface of the moon, under a bright orange sunset, with the Earth visible in the background\")\n",
    "final_answer(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manage your agent‚Äôs toolbox\n",
    "\n",
    "You can manage an agent‚Äôs toolbox by adding or replacing a tool in attribute agent.tools, since it is a standard dictionary.\n",
    "\n",
    "Let‚Äôs add the model_download_tool to an existing agent initialized with only the default toolbox.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import HfApiModel\n",
    "\n",
    "model = HfApiModel(\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "\n",
    "agent = CodeAgent(tools=[], model=model, add_base_tools=True)\n",
    "agent.tools[model_download_tool.name] = model_download_tool\n",
    "\n",
    "Now we can leverage the new tool:\n",
    "\n",
    "agent.run(\n",
    "    \"Can you give me the name of the model that has the most downloads in the 'text-to-video' task on the Hugging Face Hub but reverse the letters?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Beware of not adding too many tools to an agent: this can overwhelm weaker LLM engines.\n",
    "Use a collection of tools\n",
    "\n",
    "You can leverage tool collections by using the ToolCollection object, with the slug of the collection you want to use. Then pass them as a list to initialize your agent, and start using them!\n",
    "\n",
    "\n",
    "To speed up the start, tools are loaded only if called by the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eidos_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
