{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
      "Collecting colorama\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.1.1+cu121)\n",
      "Collecting torch\n",
      "  Downloading torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.16.1+cu121)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.20.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.66.1)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.5)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.35.2)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (2.15.0)\n",
      "Collecting keras\n",
      "  Downloading keras-3.8.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.59.5)\n",
      "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
      "Requirement already satisfied: pypdf2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2023.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.1.0 (from torch)\n",
      "  Downloading triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.9.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.3)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.8)\n",
      "Requirement already satisfied: pyparsing in /usr/lib/python3/dist-packages (from torch-geometric) (2.4.7)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.0)\n",
      "Collecting requests (from torch-geometric)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Collecting huggingface-hub>=0.23.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2023.12.25)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (2.1.0)\n",
      "Collecting rich (from keras)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.10.0)\n",
      "Collecting optree (from keras)\n",
      "  Downloading optree-0.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.2.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.10.14)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /usr/lib/python3/dist-packages (from python-docx) (4.8.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/lib/python3/dist-packages (from anyio<5,>=3.5.0->openai) (3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx<1,>=0.23.0->openai) (2020.6.20)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.0.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2023.4)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.17.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl (906.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.5/906.5 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.20.1-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hDownloading keras-3.8.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.7/450.7 kB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.0/462.0 kB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (391 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m391.8/391.8 kB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, triton, tqdm, sympy, safetensors, requests, optree, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mdurl, colorama, nvidia-cusparse-cu12, nvidia-cudnn-cu12, markdown-it-py, huggingface-hub, tokenizers, rich, nvidia-cusolver-cu12, transformers, torch, keras, datasets, torchvision\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.1.0\n",
      "    Uninstalling triton-2.1.0:\n",
      "      Successfully uninstalled triton-2.1.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.1\n",
      "    Uninstalling tqdm-4.66.1:\n",
      "      Successfully uninstalled tqdm-4.66.1\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.4.0\n",
      "    Uninstalling safetensors-0.4.0:\n",
      "      Successfully uninstalled safetensors-0.4.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.3\n",
      "    Uninstalling colorama-0.4.3:\n",
      "      Successfully uninstalled colorama-0.4.3\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.20.3\n",
      "    Uninstalling huggingface-hub-0.20.3:\n",
      "      Successfully uninstalled huggingface-hub-0.20.3\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.1\n",
      "    Uninstalling tokenizers-0.15.1:\n",
      "      Successfully uninstalled tokenizers-0.15.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.35.2\n",
      "    Uninstalling transformers-4.35.2:\n",
      "      Successfully uninstalled transformers-4.35.2\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.1+cu121\n",
      "    Uninstalling torch-2.1.1+cu121:\n",
      "      Successfully uninstalled torch-2.1.1+cu121\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.15.0\n",
      "    Uninstalling keras-2.15.0:\n",
      "      Successfully uninstalled keras-2.15.0\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.14.5\n",
      "    Uninstalling datasets-2.14.5:\n",
      "      Successfully uninstalled datasets-2.14.5\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.16.1+cu121\n",
      "    Uninstalling torchvision-0.16.1+cu121:\n",
      "      Successfully uninstalled torchvision-0.16.1+cu121\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradient 2.0.6 requires attrs<=19, but you have attrs 23.1.0 which is incompatible.\n",
      "gradient 2.0.6 requires colorama==0.4.3, but you have colorama 0.4.6 which is incompatible.\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.8.0 which is incompatible.\n",
      "torchaudio 2.1.1+cu121 requires torch==2.1.1, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed colorama-0.4.6 datasets-3.2.0 huggingface-hub-0.27.1 keras-3.8.0 markdown-it-py-3.0.0 mdurl-0.1.2 namex-0.0.8 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 optree-0.13.1 requests-2.32.3 rich-13.9.4 safetensors-0.5.2 sympy-1.13.1 tokenizers-0.21.0 torch-2.5.1 torchvision-0.20.1 tqdm-4.67.1 transformers-4.47.1 triton-3.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aabbfc2ef9e0472682035197cb1f7a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prompt: pip command to install all required packages from this list, ready to run in notebook import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch_geometric.nn import GCNConv\n",
    "# from torch_geometric.data import Data\n",
    "# from torch_geometric.utils import dense_to_sparse\n",
    "# from torchvision import datasets, transforms\n",
    "# from tqdm import tqdm  # NEW: for real-time progress bars\n",
    "# import os\n",
    "\n",
    "!pip install colorama torch torch-geometric torchvision tqdm datasets transformers keras openai python-docx pypdf2 --upgrade\n",
    "import os\n",
    "os.environ['HF_TOKEN'] = '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'  # Replace with your actual token\n",
    "import huggingface_hub\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'qwen2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 65\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Run the chat function\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m \u001b[43mchat_with_qwen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 29\u001b[0m, in \u001b[0;36mchat_with_qwen\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_path):\n\u001b[1;32m     28\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(model_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 29\u001b[0m     \u001b[43mdownload_and_save_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Load the model and tokenizer\u001b[39;00m\n\u001b[1;32m     32\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m load_model_from_local(model_path)\n",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m, in \u001b[0;36mdownload_and_save_model\u001b[0;34m(model_name, save_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_and_save_model\u001b[39m(model_name, save_path):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Download the model and tokenizer from Hugging Face and save locally\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m      9\u001b[0m     model\u001b[38;5;241m.\u001b[39msave_pretrained(save_path)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py:526\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    524\u001b[0m     _ \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 526\u001b[0m config, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py:1064\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[0;32m-> 1064\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m \u001b[43mCONFIG_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py:761\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_content[key]\n\u001b[1;32m    760\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[0;32m--> 761\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    762\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n\u001b[1;32m    763\u001b[0m module_name \u001b[38;5;241m=\u001b[39m model_type_to_module_name(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'qwen2'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def download_and_save_model(model_name, save_path):\n",
    "    # Download the model and tokenizer from Hugging Face and save locally\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "\n",
    "def load_model_from_local(model_path):\n",
    "    # Load the model and tokenizer from the local path\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "def chat_with_qwen():\n",
    "    model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "    model_path = f\"./saved_models/{model_name.replace('/', '_')}\"\n",
    "\n",
    "    # Check if the model is already downloaded, if not, download it\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path, exist_ok=True)\n",
    "        download_and_save_model(model_name, model_path)\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    model, tokenizer = load_model_from_local(model_path)\n",
    "\n",
    "    # Define a simple chat prompt\n",
    "    prompt = \"Hello! Can you tell me about the concept of a supernode grid?\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate a response\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=512\n",
    "        )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    # Decode the generated tokens to text\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    # Print the conversation\n",
    "    print(\"User:\", prompt)\n",
    "    print(\"Qwen:\", response)\n",
    "\n",
    "# Run the chat function\n",
    "chat_with_qwen()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'https://ndk0e9vo1w.clg07azjl.paperspacegradient.com/'. Verify the server is running and reachable. (Kernel not initialized in Session)."
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "A G I blueprint.py\n",
    "\n",
    "A hyper-scalable, self-evolving grid-based architecture for AGI that combines graph neural networks,\n",
    "spatial-temporal modeling, and dynamic task adaptation.\n",
    "\n",
    "CURRENT IMPLEMENTATION STATUS:\n",
    "1. Core Architecture (MOSTLY COMPLETE):\n",
    "   - Spatial-temporal grid of supernodes (3x3 graph structures) with full neighbor connectivity\n",
    "   - Concurrent processing via ThreadPoolExecutor with dynamic worker allocation\n",
    "   - Dynamic grid expansion with preserved state for new tasks\n",
    "   - Comprehensive resource monitoring and logging system (CPU/RAM/GPU) with JSON/text output\n",
    "     and optional streaming to Eidos for self-monitoring\n",
    "\n",
    "2. Multi-Task Learning (PARTIALLY COMPLETE - Mainly Need to integrate and utilise the task classifier better):\n",
    "   - Successfully handles two primary tasks with shared architecture:\n",
    "     a) Text Generation: Using Qwen2.5-0.5B-Instruct model (0.5B parameters)\n",
    "        - Chunk-based processing (size=1) for memory efficiency is fully functional\n",
    "        - Coherent text generation with context preservation (tested on small data)\n",
    "        - Perplexity scores tracked during training; concurrency fully active\n",
    "     b) Image Classification: MNIST digit recognition\n",
    "        - 98.4% test accuracy achieved in 2 training epochs!\n",
    "        - No catastrophic forgetting when switching tasks\n",
    "        - Efficient feature extraction via GCN layers\n",
    "\n",
    "3. Task Detection & Adaptation (IN PROGRESS):\n",
    "   - Basic MetaTaskGrid implemented with linear threshold detection\n",
    "   - Current limitations:\n",
    "     * Simple scalar threshold (0.2) for task novelty (with possibility to attach\n",
    "       a secondary 'task classifier' supernode grid for more sophisticated logic)\n",
    "     * Binary expansion decisions only\n",
    "     * No sophisticated task boundary detection\n",
    "   - Planned enhancements:\n",
    "     * Learned task embeddings\n",
    "     * Hierarchical task classification\n",
    "     * Dynamic architecture optimization\n",
    "     * Composed of a miniaturised and specialised version of the main supernode grid, Eidos.\n",
    "\n",
    "4. Key Features Implemented:\n",
    "   - Parallel Processing: ThreadPoolExecutor with CPU-aware scaling\n",
    "   - Modular Design: Arbitrary neural modules via supernode.arbitrary_module\n",
    "   - Checkpointing: Complete state save/restore with versioning\n",
    "   - Resource Monitoring: CPU/RAM/GPU tracking with temporal analysis, JSON and text logging,\n",
    "     plus optional Eidos streaming\n",
    "\n",
    "5. Areas for Enhancement:\n",
    "   - Task Classifier: Replace linear threshold with learned boundaries\n",
    "   - Memory Management: Implement finer-grained state preservation, chunk-based expansions\n",
    "   - Meta-Learning: Add architecture search capabilities\n",
    "   - Cross-Task Transfer: Enable more robust feature sharing across tasks\n",
    "\n",
    "ARCHITECTURAL HIGHLIGHTS:\n",
    "- Infinitely Scalable: Parallel processing of arbitrary grid subsets\n",
    "- Universal Deployment: Hardware-agnostic from edge to datacenter\n",
    "- Modular Extensions: Support for arbitrary neural heads (classification, generation)\n",
    "- Self-Evolution: Task-driven growth with state preservation\n",
    "\n",
    "PERFORMANCE CHARACTERISTICS:\n",
    "- Memory Efficiency: Text processed in chunks of size=1\n",
    "- Concurrency: Dynamic CPU core allocation (n_cores - 2)\n",
    "- Grid Dimensions: 2×2×1 default, expandable in x/y\n",
    "- Time Steps: 3-step temporal evolution with neighbor aggregation\n",
    "\n",
    "CURRENT LIMITATIONS:\n",
    "1. Task Detection:\n",
    "   - Simple threshold-based detection\n",
    "   - No sophisticated task similarity metrics\n",
    "2. Resource Usage:\n",
    "   - Full adjacency matrices may be memory-intensive\n",
    "   - Could benefit from sparse representations\n",
    "3. Training:\n",
    "   - Currently requires task-specific training phases\n",
    "   - Limited cross-task knowledge transfer\n",
    "\n",
    "This implementation demonstrates core AGI principles:\n",
    "1. Multi-task learning without interference\n",
    "2. Dynamic architecture adaptation\n",
    "3. Resource-aware scaling\n",
    "4. Modular extensibility\n",
    "\n",
    "Near-term Development Focus:\n",
    "1. Enhanced meta-learning capabilities\n",
    "2. Sophisticated task detection\n",
    "3. Cross-domain knowledge transfer\n",
    "4. Memory-efficient sparse operations\n",
    "\n",
    "Every component is thoroughly documented inline for research/production use.\n",
    "\"\"\"\n",
    "\n",
    "###############################################################################\n",
    "# (A) STANDARD LIBRARIES AND EXTERNAL IMPORTS\n",
    "###############################################################################\n",
    "\n",
    "import os  # (L1) Manages file paths, environment variables, general OS interactions\n",
    "import math  # (L2) For mathematical functions like exp (used in perplexity calculations)\n",
    "import random  # (L3) For random shuffling of lines_of_text\n",
    "import psutil  # (L4) For CPU and memory usage statistics\n",
    "from datetime import datetime, timedelta  # (L5) Timestamps for resource usage logs\n",
    "import re  # (L6) Regular expressions for parsing log messages\n",
    "import glob  # (L7) For file path manipulation\n",
    "import json  # (L8) For JSON serialization and deserialization\n",
    "import uuid  # (L9) For generating unique identifiers\n",
    "import shutil  # (L10) For file and directory operations\n",
    "from typing import List, Dict, Any  # (L11) For type hints in function signatures\n",
    "from collections import deque, defaultdict, Counter, OrderedDict # (L12) For efficient queue operations\n",
    "\n",
    "import torch  # (L6) Main PyTorch library for tensor operations\n",
    "import torch.nn as nn  # (L7) Neural network layers\n",
    "import torch.nn.functional as F  # (L8) Common functional operations (e.g., F.relu)\n",
    "from torch.utils.data import DataLoader  # (L9) Data pipeline for loading datasets\n",
    "\n",
    "from torchvision import datasets, transforms  # (L10) Provides MNIST, data augmentations\n",
    "from tqdm import tqdm  # (L11) Progress bar for training loops\n",
    "\n",
    "from torch_geometric.nn import GCNConv  # (L12) GCN layers for graph-based computations\n",
    "from torch_geometric.data import Data  # (L13) Data object for graph processing\n",
    "from torch_geometric.utils import dense_to_sparse  # (L14) Convert adjacency matrix to edge list\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed  # (L15) Concurrency\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig # (L16) QWEN-based LLM, Ensure AutoConfig is imported\n",
    "\n",
    "###############################################################################\n",
    "# (B) OPTIONAL IMPORT FROM dataset_downloader_text.py FOR TEXT LINES\n",
    "###############################################################################\n",
    "try:\n",
    "    # (L17) If a local \"dataset_downloader_text.py\" is present, we can import a function\n",
    "    #       that returns text lines. This helps unify the program with any external text data.\n",
    "    from dataset_downloader_text import DatasetHandler, load_text_data \n",
    "    DATASET_DOWNLOADER_AVAILABLE = True\n",
    "except ImportError:\n",
    "    # (L18) If not available, we fallback to local lines_of_text in main.\n",
    "    DATASET_DOWNLOADER_AVAILABLE = False\n",
    "\n",
    "###############################################################################\n",
    "# (C) TORCH THREADS\n",
    "###############################################################################\n",
    "# (L19) Optionally limit PyTorch's internal parallelism for ops like matrix mult\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "###############################################################################\n",
    "# 0. RESOURCE LOGGING: CPU, RAM, (Optional) GPU\n",
    "###############################################################################\n",
    "def log_resource_usage(tag=\"\"):\n",
    "    \"\"\"\n",
    "    (L20) Gathers and prints system resource usage (CPU, RAM), optionally logs\n",
    "          GPU usage if CUDA is available. Also writes logs to JSON + text files\n",
    "          for record-keeping, and can stream the usage data to Eidos if desired.\n",
    "    \"\"\"\n",
    "    # (L21) Gather system memory usage using psutil\n",
    "    vm = psutil.virtual_memory()\n",
    "    cpu_pct = psutil.cpu_percent(interval=None)\n",
    "    mem_pct = vm.percent\n",
    "    mem_used_mb = vm.used / (1024 * 1024)\n",
    "    mem_total_mb = vm.total / (1024 * 1024)\n",
    "\n",
    "    # (L22) Begin forming a log message string\n",
    "    log_message = (\n",
    "        f\"[ResourceUsage{(':' + tag) if tag else ''}] \"\n",
    "        f\"CPU={cpu_pct:.1f}% | RAM={mem_used_mb:.0f}/{mem_total_mb:.0f}MB \"\n",
    "        f\"({mem_pct:.1f}%)\"\n",
    "    )\n",
    "\n",
    "    # (L23) Optionally gather GPU usage if CUDA is available\n",
    "    gpu_mem_allocated = None\n",
    "    gpu_mem_reserved = None\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_mem_allocated = torch.cuda.memory_allocated() / (1024 * 1024)\n",
    "        gpu_mem_reserved = torch.cuda.memory_reserved() / (1024 * 1024)\n",
    "        log_message += (\n",
    "            f\" | GPU=Allocated:{gpu_mem_allocated:.0f}MB,\"\n",
    "            f\" Reserved:{gpu_mem_reserved:.0f}MB\"\n",
    "        )\n",
    "\n",
    "    # (L24) Print usage message to stdout\n",
    "    print(log_message)\n",
    "\n",
    "    # (L25) Create structured log data for JSON\n",
    "    structured_log = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"tag\": tag,\n",
    "        \"cpu_usage_percent\": cpu_pct,\n",
    "        \"ram_usage_mb\": mem_used_mb,\n",
    "        \"ram_total_mb\": mem_total_mb,\n",
    "        \"ram_usage_percent\": mem_pct,\n",
    "        \"gpu\": {\n",
    "            \"allocated_mb\": gpu_mem_allocated,\n",
    "            \"reserved_mb\": gpu_mem_reserved\n",
    "        } if torch.cuda.is_available() else None\n",
    "    }\n",
    "\n",
    "    # (L26) Append structured data to resource_usage.json\n",
    "    try:\n",
    "        os.makedirs(\"./logs\", exist_ok=True)\n",
    "        with open(\"./logs/resource_usage.json\", \"a\") as json_file:\n",
    "            import json\n",
    "            json.dump(structured_log, json_file)\n",
    "            json_file.write(\"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not write to resource_usage.json: {e}\")\n",
    "\n",
    "    # (L27) Also append to resource_usage.log in plain text\n",
    "    try:\n",
    "        with open(\"./logs/resource_usage.log\", \"a\") as log_file:\n",
    "            log_file.write(log_message + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not write to resource_usage.log: {e}\")\n",
    "\n",
    "    # (L28) Optionally send the log message to Eidos, so it can monitor its internal state\n",
    "    send_to_Eidos(log_message)\n",
    "\n",
    "\n",
    "def send_to_Eidos(log_message):\n",
    "    \"\"\"Send system monitoring data to Eidos for self-monitoring and state tracking.\n",
    "    \n",
    "    This function integrates with Eidos's internal state monitoring system, which requires:\n",
    "\n",
    "    1. StateMemoryBuffer class:\n",
    "        - add_observation(timestamp: datetime, metrics: dict) -> None\n",
    "            Adds new metrics to circular buffer with timestamp\n",
    "        - get_window(start_time: datetime, end_time: datetime) -> List[dict] \n",
    "            Returns metrics between start/end times\n",
    "        - prune_old_data(max_age: timedelta) -> None\n",
    "            Removes data older than max_age\n",
    "        - get_summary_statistics() -> dict\n",
    "            Returns statistical summaries of stored metrics\n",
    "\n",
    "    2. MetricsAnalyzer class:\n",
    "        - parse_log_message(message: str) -> dict\n",
    "            Extracts structured metrics from log message\n",
    "        - calculate_metrics_importance(metrics: dict) -> dict\n",
    "            Scores importance of each metric using attention\n",
    "        - detect_anomalies(window: List[dict]) -> List[dict]\n",
    "            Identifies anomalous patterns in metrics\n",
    "        - generate_summary(window: List[dict]) -> dict\n",
    "            Creates high-level summary of system state\n",
    "\n",
    "    3. StateManager class:\n",
    "        - update_state(metrics: dict) -> None\n",
    "            Updates internal state representation\n",
    "        - get_current_state() -> dict\n",
    "            Returns current system state assessment\n",
    "        - evaluate_state_change(old_state: dict, new_state: dict) -> dict\n",
    "            Analyzes significance of state transitions\n",
    "        - predict_next_state(current_state: dict) -> dict\n",
    "            Projects likely next state\n",
    "\n",
    "    4. ActionEngine class:\n",
    "        - evaluate_situation(state: dict, anomalies: List[dict]) -> dict\n",
    "            Determines if action is needed\n",
    "        - generate_action_plan(situation: dict) -> dict\n",
    "            Creates specific action steps\n",
    "        - execute_action(action: dict) -> bool\n",
    "            Performs the action\n",
    "        - monitor_action_outcome(action_id: str) -> dict\n",
    "            Tracks results of actions taken\n",
    "\n",
    "    Args:\n",
    "        log_message: String containing resource usage metrics\n",
    "        \n",
    "    Returns:\n",
    "        None - State updates and actions are handled asynchronously\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize core components if needed\n",
    "        if not hasattr(Eidos, 'state_memory'):\n",
    "            Eidos.state_memory = StateMemoryBuffer(max_size=10000)\n",
    "        if not hasattr(Eidos, 'metrics_analyzer'):\n",
    "            Eidos.metrics_analyzer = MetricsAnalyzer()\n",
    "        if not hasattr(Eidos, 'state_manager'):\n",
    "            Eidos.state_manager = StateManager()\n",
    "        if not hasattr(Eidos, 'action_engine'):\n",
    "            Eidos.action_engine = ActionEngine()\n",
    "\n",
    "        # Extract and store metrics\n",
    "        current_time = datetime.now()\n",
    "        metrics = Eidos.metrics_analyzer.parse_log_message(log_message)\n",
    "        Eidos.state_memory.add_observation(current_time, metrics)\n",
    "\n",
    "        # Analyze recent window\n",
    "        window = Eidos.state_memory.get_window(\n",
    "            start_time=current_time - timedelta(minutes=5),\n",
    "            end_time=current_time\n",
    "        )\n",
    "        \n",
    "        # Process current state\n",
    "        importance_scores = Eidos.metrics_analyzer.calculate_metrics_importance(metrics)\n",
    "        anomalies = Eidos.metrics_analyzer.detect_anomalies(window)\n",
    "        state_summary = Eidos.metrics_analyzer.generate_summary(window)\n",
    "        \n",
    "        # Update state tracking\n",
    "        old_state = Eidos.state_manager.get_current_state()\n",
    "        Eidos.state_manager.update_state(metrics)\n",
    "        new_state = Eidos.state_manager.get_current_state()\n",
    "        state_change = Eidos.state_manager.evaluate_state_change(old_state, new_state)\n",
    "        \n",
    "        # Determine and take action if needed\n",
    "        situation = Eidos.action_engine.evaluate_situation(new_state, anomalies)\n",
    "        if situation['action_required']:\n",
    "            action_plan = Eidos.action_engine.generate_action_plan(situation)\n",
    "            action_success = Eidos.action_engine.execute_action(action_plan)\n",
    "            if action_success:\n",
    "                Eidos.action_engine.monitor_action_outcome(action_plan['id'])\n",
    "\n",
    "        # Cleanup old data periodically\n",
    "        Eidos.state_memory.prune_old_data(timedelta(hours=24))\n",
    "\n",
    "    except AttributeError as e:\n",
    "        # Gracefully handle case where Eidos monitoring is not configured\n",
    "        pass\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 0.1 STATE MONITORING AND MANAGEMENT SYSTEM\n",
    "###############################################################################\n",
    "\n",
    "class StateMemoryBuffer:\n",
    "    \"\"\"\n",
    "    Circular buffer for storing and managing temporal system state observations.\n",
    "    Provides efficient storage and retrieval of time-series metrics with automatic\n",
    "    pruning of old data.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_size: int = 10000):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = []  # List[Dict[str, Any]]\n",
    "        self.current_index = 0\n",
    "        \n",
    "    def add_observation(self, timestamp: datetime, metrics: dict):\n",
    "        \"\"\"Add new metrics observation with timestamp.\"\"\"\n",
    "        observation = {\n",
    "            \"timestamp\": timestamp,\n",
    "            \"metrics\": metrics,\n",
    "            \"importance_score\": 0.0  # Updated by MetricsAnalyzer\n",
    "        }\n",
    "        \n",
    "        if len(self.buffer) < self.max_size:\n",
    "            self.buffer.append(observation)\n",
    "        else:\n",
    "            self.buffer[self.current_index] = observation\n",
    "            self.current_index = (self.current_index + 1) % self.max_size\n",
    "            \n",
    "    def get_window(self, start_time: datetime, end_time: datetime) -> list:\n",
    "        \"\"\"Retrieve metrics within specified time window.\"\"\"\n",
    "        return [\n",
    "            obs for obs in self.buffer\n",
    "            if start_time <= obs[\"timestamp\"] <= end_time\n",
    "        ]\n",
    "        \n",
    "    def prune_old_data(self, max_age: timedelta):\n",
    "        \"\"\"Remove data older than max_age.\"\"\"\n",
    "        current_time = datetime.now()\n",
    "        self.buffer = [\n",
    "            obs for obs in self.buffer\n",
    "            if (current_time - obs[\"timestamp\"]) <= max_age\n",
    "        ]\n",
    "        \n",
    "    def get_summary_statistics(self) -> dict:\n",
    "        \"\"\"Calculate statistical summaries of stored metrics.\"\"\"\n",
    "        if not self.buffer:\n",
    "            return {}\n",
    "            \n",
    "        all_metrics = {}\n",
    "        for obs in self.buffer:\n",
    "            for key, value in obs[\"metrics\"].items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    if key not in all_metrics:\n",
    "                        all_metrics[key] = []\n",
    "                    all_metrics[key].append(value)\n",
    "                    \n",
    "        summaries = {}\n",
    "        for key, values in all_metrics.items():\n",
    "            values_tensor = torch.tensor(values)\n",
    "            summaries[key] = {\n",
    "                \"mean\": values_tensor.mean().item(),\n",
    "                \"std\": values_tensor.std().item(),\n",
    "                \"min\": values_tensor.min().item(),\n",
    "                \"max\": values_tensor.max().item()\n",
    "            }\n",
    "            \n",
    "        return summaries\n",
    "\n",
    "\n",
    "class MetricsAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes system metrics using attention mechanisms and statistical methods\n",
    "    to identify patterns, anomalies, and generate summaries.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=64, num_heads=4, batch_first=True\n",
    "        )\n",
    "        self.metric_embeddings = nn.Linear(1, 64)\n",
    "        \n",
    "    def parse_log_message(self, message: str) -> dict:\n",
    "        \"\"\"Extract structured metrics from log message.\"\"\"\n",
    "        metrics = {}\n",
    "        try:\n",
    "            # Parse CPU usage\n",
    "            if \"CPU=\" in message:\n",
    "                cpu_match = re.search(r\"CPU=(\\d+\\.?\\d*)%\", message)\n",
    "                if cpu_match:\n",
    "                    metrics[\"cpu_usage\"] = float(cpu_match.group(1))\n",
    "                    \n",
    "            # Parse RAM usage\n",
    "            if \"RAM=\" in message:\n",
    "                ram_match = re.search(\n",
    "                    r\"RAM=(\\d+)/(\\d+)MB \\((\\d+\\.?\\d*)%\\)\", message\n",
    "                )\n",
    "                if ram_match:\n",
    "                    metrics[\"ram_used\"] = float(ram_match.group(1))\n",
    "                    metrics[\"ram_total\"] = float(ram_match.group(2))\n",
    "                    metrics[\"ram_percent\"] = float(ram_match.group(3))\n",
    "                    \n",
    "            # Parse GPU usage if present\n",
    "            if \"GPU=\" in message:\n",
    "                gpu_match = re.search(\n",
    "                    r\"GPU=Allocated:(\\d+)MB, Reserved:(\\d+)MB\", message\n",
    "                )\n",
    "                if gpu_match:\n",
    "                    metrics[\"gpu_allocated\"] = float(gpu_match.group(1))\n",
    "                    metrics[\"gpu_reserved\"] = float(gpu_match.group(2))\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing metrics: {e}\")\n",
    "            \n",
    "        return metrics\n",
    "        \n",
    "    def calculate_metrics_importance(self, metrics: dict) -> dict:\n",
    "        \"\"\"Score importance of metrics using attention mechanism.\"\"\"\n",
    "        importance_scores = {}\n",
    "        try:\n",
    "            # Convert metrics to tensors for attention\n",
    "            metric_values = []\n",
    "            metric_keys = []\n",
    "            \n",
    "            for key, value in metrics.items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    metric_values.append([float(value)])\n",
    "                    metric_keys.append(key)\n",
    "                    \n",
    "            if not metric_values:\n",
    "                return importance_scores\n",
    "                \n",
    "            # Create embeddings\n",
    "            values_tensor = torch.tensor(metric_values, dtype=torch.float32)\n",
    "            embedded = self.metric_embeddings(values_tensor)\n",
    "            \n",
    "            # Self-attention to determine importance\n",
    "            attn_output, attn_weights = self.attention(\n",
    "                embedded, embedded, embedded\n",
    "            )\n",
    "            \n",
    "            # Average attention weights for each metric\n",
    "            importance = attn_weights.mean(dim=1).squeeze()\n",
    "            \n",
    "            # Create importance score dictionary\n",
    "            for idx, key in enumerate(metric_keys):\n",
    "                importance_scores[key] = importance[idx].item()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating importance: {e}\")\n",
    "            \n",
    "        return importance_scores\n",
    "        \n",
    "    def detect_anomalies(self, window: list) -> list:\n",
    "        \"\"\"Identify anomalous patterns in metrics.\"\"\"\n",
    "        anomalies = []\n",
    "        if not window:\n",
    "            return anomalies\n",
    "            \n",
    "        try:\n",
    "            # Group metrics by type\n",
    "            metric_series = defaultdict(list)\n",
    "            timestamps = []\n",
    "            \n",
    "            for obs in window:\n",
    "                timestamps.append(obs[\"timestamp\"])\n",
    "                for key, value in obs[\"metrics\"].items():\n",
    "                    if isinstance(value, (int, float)):\n",
    "                        metric_series[key].append(value)\n",
    "                        \n",
    "            # Calculate z-scores for each metric\n",
    "            for metric_name, values in metric_series.items():\n",
    "                values_tensor = torch.tensor(values)\n",
    "                mean = values_tensor.mean()\n",
    "                std = values_tensor.std()\n",
    "                \n",
    "                if std == 0:\n",
    "                    continue\n",
    "                    \n",
    "                z_scores = (values_tensor - mean) / std\n",
    "                \n",
    "                # Detect points beyond 3 standard deviations\n",
    "                anomaly_indices = torch.where(z_scores.abs() > 3)[0]\n",
    "                \n",
    "                for idx in anomaly_indices:\n",
    "                    anomalies.append({\n",
    "                        \"metric\": metric_name,\n",
    "                        \"timestamp\": timestamps[idx],\n",
    "                        \"value\": values[idx],\n",
    "                        \"z_score\": z_scores[idx].item()\n",
    "                    })\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error detecting anomalies: {e}\")\n",
    "            \n",
    "        return anomalies\n",
    "\n",
    "\n",
    "class StateManager:\n",
    "    \"\"\"\n",
    "    Manages system state transitions and predictions using a combination\n",
    "    of statistical and neural approaches.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.current_state = {}\n",
    "        self.state_history = []\n",
    "        self.state_predictor = nn.GRU(\n",
    "            input_size=64,\n",
    "            hidden_size=128,\n",
    "            num_layers=2,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.state_embedding = nn.Linear(1, 64)\n",
    "        \n",
    "    def update_state(self, metrics: dict):\n",
    "        \"\"\"Update internal state representation with new metrics.\"\"\"\n",
    "        self.state_history.append(self.current_state)\n",
    "        self.current_state = {\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"metrics\": metrics,\n",
    "            \"derived_features\": self._calculate_derived_features(metrics)\n",
    "        }\n",
    "        \n",
    "    def _calculate_derived_features(self, metrics: dict) -> dict:\n",
    "        \"\"\"Calculate additional features from raw metrics.\"\"\"\n",
    "        derived = {}\n",
    "        try:\n",
    "            if \"cpu_usage\" in metrics and \"ram_percent\" in metrics:\n",
    "                derived[\"resource_pressure\"] = (\n",
    "                    metrics[\"cpu_usage\"] + metrics[\"ram_percent\"]\n",
    "                ) / 2\n",
    "                \n",
    "            if \"gpu_allocated\" in metrics and \"gpu_reserved\" in metrics:\n",
    "                derived[\"gpu_efficiency\"] = (\n",
    "                    metrics[\"gpu_allocated\"] / metrics[\"gpu_reserved\"]\n",
    "                    if metrics[\"gpu_reserved\"] > 0 else 0\n",
    "                )\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating derived features: {e}\")\n",
    "            \n",
    "        return derived\n",
    "        \n",
    "    def get_current_state(self) -> dict:\n",
    "        \"\"\"Return current system state assessment.\"\"\"\n",
    "        return self.current_state\n",
    "        \n",
    "    def evaluate_state_change(\n",
    "        self, old_state: dict, new_state: dict\n",
    "    ) -> dict:\n",
    "        \"\"\"Analyze significance of state transitions.\"\"\"\n",
    "        changes = {}\n",
    "        try:\n",
    "            if not old_state or not new_state:\n",
    "                return changes\n",
    "                \n",
    "            # Compare metrics\n",
    "            for key in new_state[\"metrics\"]:\n",
    "                if key in old_state[\"metrics\"]:\n",
    "                    old_val = old_state[\"metrics\"][key]\n",
    "                    new_val = new_state[\"metrics\"][key]\n",
    "                    if isinstance(old_val, (int, float)):\n",
    "                        pct_change = (\n",
    "                            (new_val - old_val) / old_val * 100\n",
    "                            if old_val != 0 else float('inf')\n",
    "                        )\n",
    "                        changes[key] = {\n",
    "                            \"old_value\": old_val,\n",
    "                            \"new_value\": new_val,\n",
    "                            \"percent_change\": pct_change\n",
    "                        }\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating state change: {e}\")\n",
    "            \n",
    "        return changes\n",
    "\n",
    "\n",
    "class ActionEngine:\n",
    "    \"\"\"\n",
    "    Determines and executes actions based on system state analysis.\n",
    "    Implements a policy network for action selection and outcome monitoring.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32)  # Action space dimension\n",
    "        )\n",
    "        self.action_history = []\n",
    "        \n",
    "    def evaluate_situation(\n",
    "        self, state: dict, anomalies: list\n",
    "    ) -> dict:\n",
    "        \"\"\"Determine if action is needed based on current situation.\"\"\"\n",
    "        evaluation = {\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"requires_action\": False,\n",
    "            \"priority\": 0.0,\n",
    "            \"triggers\": []\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Check resource thresholds\n",
    "            metrics = state.get(\"metrics\", {})\n",
    "            \n",
    "            if metrics.get(\"cpu_usage\", 0) > 90:\n",
    "                evaluation[\"requires_action\"] = True\n",
    "                evaluation[\"priority\"] = max(\n",
    "                    evaluation[\"priority\"], 0.8\n",
    "                )\n",
    "                evaluation[\"triggers\"].append(\"high_cpu_usage\")\n",
    "                \n",
    "            if metrics.get(\"ram_percent\", 0) > 85:\n",
    "                evaluation[\"requires_action\"] = True\n",
    "                evaluation[\"priority\"] = max(\n",
    "                    evaluation[\"priority\"], 0.7\n",
    "                )\n",
    "                evaluation[\"triggers\"].append(\"high_ram_usage\")\n",
    "                \n",
    "            # Consider anomalies\n",
    "            if anomalies:\n",
    "                evaluation[\"requires_action\"] = True\n",
    "                evaluation[\"priority\"] = max(\n",
    "                    evaluation[\"priority\"], 0.6\n",
    "                )\n",
    "                evaluation[\"triggers\"].extend(\n",
    "                    [f\"anomaly_{a['metric']}\" for a in anomalies]\n",
    "                )\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating situation: {e}\")\n",
    "            \n",
    "        return evaluation\n",
    "        \n",
    "    def generate_action_plan(self, situation: dict) -> dict:\n",
    "        \"\"\"Create specific action steps based on situation assessment.\"\"\"\n",
    "        action_plan = {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"actions\": [],\n",
    "            \"priority\": situation.get(\"priority\", 0.0)\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            triggers = situation.get(\"triggers\", [])\n",
    "            \n",
    "            if \"high_cpu_usage\" in triggers:\n",
    "                action_plan[\"actions\"].append({\n",
    "                    \"type\": \"reduce_concurrency\",\n",
    "                    \"params\": {\"target_workers\": 1}\n",
    "                })\n",
    "                \n",
    "            if \"high_ram_usage\" in triggers:\n",
    "                action_plan[\"actions\"].append({\n",
    "                    \"type\": \"clear_cache\",\n",
    "                    \"params\": {}\n",
    "                })\n",
    "                \n",
    "            for trigger in triggers:\n",
    "                if trigger.startswith(\"anomaly_\"):\n",
    "                    action_plan[\"actions\"].append({\n",
    "                        \"type\": \"log_anomaly\",\n",
    "                        \"params\": {\"metric\": trigger[8:]}\n",
    "                    })\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating action plan: {e}\")\n",
    "            \n",
    "        return action_plan\n",
    "        \n",
    "    def execute_action(self, action: dict) -> bool:\n",
    "        \"\"\"Execute a planned action and record its execution.\"\"\"\n",
    "        success = False\n",
    "        try:\n",
    "            action_type = action.get(\"type\")\n",
    "            params = action.get(\"params\", {})\n",
    "            \n",
    "            if action_type == \"reduce_concurrency\":\n",
    "                # Implementation for reducing worker count\n",
    "                target_workers = params.get(\"target_workers\", 1)\n",
    "                # Actual implementation would modify ThreadPoolExecutor\n",
    "                success = True\n",
    "                \n",
    "            elif action_type == \"clear_cache\":\n",
    "                # Implementation for cache clearing\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                success = True\n",
    "                \n",
    "            elif action_type == \"log_anomaly\":\n",
    "                # Implementation for anomaly logging\n",
    "                metric = params.get(\"metric\")\n",
    "                with open(\"anomalies.log\", \"a\") as f:\n",
    "                    f.write(f\"{datetime.now()}: Anomaly in {metric}\\n\")\n",
    "                success = True\n",
    "                \n",
    "            # Record action execution\n",
    "            self.action_history.append({\n",
    "                \"timestamp\": datetime.now(),\n",
    "                \"action\": action,\n",
    "                \"success\": success\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error executing action: {e}\")\n",
    "            success = False\n",
    "            \n",
    "        return success\n",
    "        \n",
    "    def monitor_action_outcome(self, action_id: str) -> dict:\n",
    "        \"\"\"Track and analyze results of executed actions.\"\"\"\n",
    "        outcome = {\n",
    "            \"action_id\": action_id,\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"success\": False,\n",
    "            \"effects\": {}\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Find action in history\n",
    "            action_record = next(\n",
    "                (a for a in self.action_history if a[\"action\"].get(\"id\") == action_id),\n",
    "                None\n",
    "            )\n",
    "            \n",
    "            if action_record:\n",
    "                outcome[\"success\"] = action_record[\"success\"]\n",
    "                # Additional outcome analysis could be added here\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error monitoring action outcome: {e}\")\n",
    "            \n",
    "        return outcome\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 1.1 MEMORY-EFFICIENT MODULE COMPONENTS\n",
    "###############################################################################\n",
    "\n",
    "class ModuleWrapper:\n",
    "    \"\"\"\n",
    "    Wraps arbitrary neural modules with chunked processing and disk caching.\n",
    "    Handles automatic chunking of large inputs and manages temporary storage.\n",
    "    \"\"\"\n",
    "    def __init__(self, module: nn.Module, chunk_size: int = 1024, \n",
    "                 cache_dir: str = \"./disk_offload_dir/modules\"):\n",
    "        self.module = module\n",
    "        self.chunk_size = chunk_size\n",
    "        self.cache_dir = cache_dir\n",
    "        self.cache_file = os.path.join(cache_dir, f\"module_{id(self)}.pt\")\n",
    "        os.makedirs(os.path.join(\"./disk_offload_dir\", \"modules\"), exist_ok=True)\n",
    "        \n",
    "        # Track memory usage\n",
    "        self.peak_memory = 0\n",
    "        self.total_processed = 0\n",
    "        \n",
    "    def process_chunk(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Process input tensor in chunks with disk offloading if needed.\"\"\"\n",
    "        try:\n",
    "            # Monitor memory\n",
    "            current_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
    "            self.peak_memory = max(self.peak_memory, current_memory)\n",
    "            \n",
    "            # Process in chunks if input is large\n",
    "            if x.shape[0] > self.chunk_size:\n",
    "                chunks = torch.split(x, self.chunk_size)\n",
    "                processed = []\n",
    "                \n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    # Process chunk\n",
    "                    result = self.module(chunk)\n",
    "                    \n",
    "                    # Save to disk if memory pressure is high\n",
    "                    if current_memory > 0.8 * torch.cuda.max_memory_allocated():\n",
    "                        torch.save(result, f\"{self.cache_file}.{i}\")\n",
    "                        processed.append(f\"{self.cache_file}.{i}\")\n",
    "                    else:\n",
    "                        processed.append(result)\n",
    "                \n",
    "                # Combine results, loading from disk if needed\n",
    "                final = []\n",
    "                for item in processed:\n",
    "                    if isinstance(item, str):\n",
    "                        chunk_result = torch.load(item)\n",
    "                        os.remove(item)\n",
    "                        final.append(chunk_result)\n",
    "                    else:\n",
    "                        final.append(item)\n",
    "                        \n",
    "                return torch.cat(final, dim=0)\n",
    "            \n",
    "            # Direct processing for small inputs\n",
    "            return self.module(x)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in ModuleWrapper: {str(e)}\")\n",
    "            # Fallback: process without chunking\n",
    "            return self.module(x)\n",
    "            \n",
    "    def cleanup(self):\n",
    "        \"\"\"Remove temporary cache files.\"\"\"\n",
    "        if os.path.exists(self.cache_dir):\n",
    "            shutil.rmtree(self.cache_dir)\n",
    "\n",
    "    @property\n",
    "    def offload_threshold(self):\n",
    "        \"\"\"\n",
    "        Dynamically adjust offloading threshold based on system resources.\n",
    "\n",
    "        We prioritize storing data on disk, then CPU, then GPU:\n",
    "        - If memory usage on GPU is above a certain fraction, we offload to disk/CPU.\n",
    "        - If CPU usage or memory usage is also high, we adapt the threshold accordingly.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            vm = psutil.virtual_memory()  # total system memory info\n",
    "            # For illustration: if system memory usage is above 80%, we lower threshold.\n",
    "            usage_fraction = vm.percent / 100.0\n",
    "            base_threshold = 0.65  # default fraction of GPU memory usage we allow\n",
    "            # If system memory is heavily used, reduce threshold\n",
    "            adjusted = base_threshold - (usage_fraction * 0.25)\n",
    "            if adjusted < 0.25:\n",
    "                adjusted = 0.25\n",
    "            return adjusted\n",
    "        except Exception as e:\n",
    "            print(f\"[ModuleWrapper] Warning: Could not compute offload_threshold: {e}\")\n",
    "            return 0.5\n",
    "\n",
    "\n",
    "class CheckpointManager:\n",
    "    \"\"\"\n",
    "    Manages model checkpoints with versioning and automatic pruning.\n",
    "    Supports both state dict and optimizer state saving/loading.\n",
    "    \"\"\"\n",
    "    def __init__(self, checkpoint_dir: str = \"./checkpoints\", max_checkpoints: int = 5):\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.max_checkpoints = max_checkpoints\n",
    "        os.makedirs(os.path.join(\"./checkpoints\"), exist_ok=True)\n",
    "        \n",
    "        # Track checkpoint history\n",
    "        self.checkpoint_history = []\n",
    "        self._load_history()\n",
    "        \n",
    "    def _load_history(self):\n",
    "        \"\"\"Load existing checkpoint history.\"\"\"\n",
    "        history_file = os.path.join(self.checkpoint_dir, \"checkpoint_history.json\")\n",
    "        if os.path.exists(history_file):\n",
    "            with open(history_file, 'r') as f:\n",
    "                self.checkpoint_history = json.load(f)\n",
    "                \n",
    "    def _save_history(self):\n",
    "        \"\"\"Save checkpoint history to disk.\"\"\"\n",
    "        history_file = os.path.join(self.checkpoint_dir, \"checkpoint_history.json\")\n",
    "        with open(history_file, 'w') as f:\n",
    "            json.dump(self.checkpoint_history, f)\n",
    "            \n",
    "    def save_checkpoint(self, state_dict: dict, metadata: dict = None):\n",
    "        \"\"\"Save a new checkpoint with metadata.\"\"\"\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        checkpoint_path = os.path.join(\n",
    "            self.checkpoint_dir, \n",
    "            f\"checkpoint_{timestamp}.pt\"\n",
    "        )\n",
    "        \n",
    "        # Save checkpoint with metadata\n",
    "        save_dict = {\n",
    "            'state_dict': state_dict,\n",
    "            'metadata': metadata or {},\n",
    "            'timestamp': timestamp\n",
    "        }\n",
    "        torch.save(save_dict, checkpoint_path)\n",
    "        \n",
    "        # Update history\n",
    "        self.checkpoint_history.append({\n",
    "            'path': checkpoint_path,\n",
    "            'timestamp': timestamp,\n",
    "            'metadata': metadata\n",
    "        })\n",
    "        \n",
    "        # Prune old checkpoints if needed\n",
    "        self._prune_old_checkpoints()\n",
    "        self._save_history()\n",
    "        \n",
    "    def restore_latest_checkpoint(self, model: nn.Module) -> bool:\n",
    "        \"\"\"Restore the most recent checkpoint.\"\"\"\n",
    "        if not self.checkpoint_history:\n",
    "            return False\n",
    "            \n",
    "        latest = max(self.checkpoint_history, \n",
    "                    key=lambda x: x['timestamp'])\n",
    "        \n",
    "        try:\n",
    "            checkpoint = torch.load(latest['path'])\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error restoring checkpoint: {str(e)}\")\n",
    "            return False\n",
    "            \n",
    "    def _prune_old_checkpoints(self):\n",
    "        \"\"\"Remove old checkpoints, keeping only max_checkpoints.\"\"\"\n",
    "        if len(self.checkpoint_history) > self.max_checkpoints:\n",
    "            # Sort by timestamp\n",
    "            sorted_checkpoints = sorted(\n",
    "                self.checkpoint_history,\n",
    "                key=lambda x: x['timestamp']\n",
    "            )\n",
    "            \n",
    "            # Remove oldest\n",
    "            for checkpoint in sorted_checkpoints[:-self.max_checkpoints]:\n",
    "                try:\n",
    "                    os.remove(checkpoint['path'])\n",
    "                    self.checkpoint_history.remove(checkpoint)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error pruning checkpoint: {str(e)}\")\n",
    "                    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Remove all checkpoints and history.\"\"\"\n",
    "        for checkpoint in self.checkpoint_history:\n",
    "            try:\n",
    "                os.remove(checkpoint['path'])\n",
    "            except Exception:\n",
    "                pass\n",
    "        if os.path.exists(self.checkpoint_dir):\n",
    "            shutil.rmtree(self.checkpoint_dir)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# REPLACE THE OLD OffloadableLinear CLASS WITH AdaptiveOffloadableLinear\n",
    "###############################################################################\n",
    "class AdaptiveOffloadableLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, cache_dir, project_input=False, offload_threshold=0.8, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.project_input = project_input\n",
    "        self.offload_threshold = offload_threshold\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features)) if bias else None\n",
    "\n",
    "        # Optional adapter for input size mismatch\n",
    "        self.adapter = nn.Linear(in_features, out_features) if in_features != out_features else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Adjust input size if necessary\n",
    "        if self.adapter is not None:\n",
    "            x = self.adapter(x)\n",
    "\n",
    "        # Perform linear transformation\n",
    "        return F.linear(x, self.weight, self.bias)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 1. SUPERNODE DEFINITION\n",
    "###############################################################################\n",
    "class Supernode(nn.Module):\n",
    "    \"\"\"\n",
    "    A 3×3 (9-node) mini-graph with GCN layers, plus neighbor and temporal features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, arbitrary_module: nn.Module = None,\n",
    "                 chunk_size: int = 1024, disk_cache_dir: str = \"./disk_offload_dir/supernode\"):\n",
    "        super().__init__()\n",
    "\n",
    "        # Adapter to handle input size mismatches\n",
    "        self.adapter = nn.Linear(in_channels, out_channels) if in_channels != out_channels else None\n",
    "\n",
    "        # Existing GCN layers\n",
    "        self.conv1 = GCNConv(out_channels, out_channels)\n",
    "        self.conv2 = GCNConv(out_channels, out_channels)\n",
    "\n",
    "        # Adapters for neighbor and temporal features\n",
    "        self.neighbor_input_adapter = AdaptiveOffloadableLinear(\n",
    "            in_features=in_channels,\n",
    "            out_features=out_channels,\n",
    "            cache_dir=os.path.join(disk_cache_dir, \"neighbor_input_adapter\"),\n",
    "            project_input=False,\n",
    "            offload_threshold=0.8,\n",
    "            bias=False\n",
    "        )\n",
    "        self.temporal_input_adapter = AdaptiveOffloadableLinear(\n",
    "            in_features=in_channels,\n",
    "            out_features=out_channels,\n",
    "            cache_dir=os.path.join(disk_cache_dir, \"temporal_input_adapter\"),\n",
    "            project_input=False,\n",
    "            offload_threshold=0.8,\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "        # Arbitrary module handling with auto-chunking\n",
    "        self.arbitrary_module = None\n",
    "        if arbitrary_module is not None:\n",
    "            self.arbitrary_module = ModuleWrapper(\n",
    "                arbitrary_module,\n",
    "                chunk_size=chunk_size,\n",
    "                cache_dir=disk_cache_dir\n",
    "            )\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.chunk_size = chunk_size\n",
    "        self.disk_cache_dir = disk_cache_dir\n",
    "        os.makedirs(os.path.join(\"./disk_offload_dir\", \"supernode\"), exist_ok=True)\n",
    "\n",
    "        self.training_state = {'epoch': 0, 'iterations': 0}\n",
    "        self.checkpoint_manager = CheckpointManager(disk_cache_dir)\n",
    "\n",
    "        # Add dimension adapter for MNIST (784->64)\n",
    "        self.input_adapter = None\n",
    "        if in_channels == 784 and out_channels == 64:\n",
    "            self.input_adapter = nn.Linear(784, 64)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data: Data,\n",
    "        neighbor_features: torch.Tensor = None,\n",
    "        prev_time_features: torch.Tensor = None,\n",
    "        executor: ThreadPoolExecutor = None\n",
    "    ) -> torch.Tensor:\n",
    "        # Determine device priority\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        data.x = data.x.to(device)\n",
    "        if neighbor_features is not None:\n",
    "            neighbor_features = neighbor_features.to(device)\n",
    "        if prev_time_features is not None:\n",
    "            prev_time_features = prev_time_features.to(device)\n",
    "\n",
    "        # Adapt data.x to match out_channels if needed\n",
    "        if self.adapter is not None:\n",
    "            data.x = self.adapter(data.x)\n",
    "\n",
    "        # Adapt neighbor/temporal features if they exist and sizes mismatch\n",
    "        if neighbor_features is not None and neighbor_features.size(1) != self.out_channels:\n",
    "            neighbor_features = self.neighbor_input_adapter(neighbor_features).to(device)\n",
    "        if prev_time_features is not None and prev_time_features.size(1) != self.out_channels:\n",
    "            prev_time_features = self.temporal_input_adapter(prev_time_features).to(device)\n",
    "\n",
    "        # Process in chunks with concurrency\n",
    "        chunks = self.chunk_tensor(data.x)\n",
    "        processed_chunks = []\n",
    "\n",
    "        if executor:\n",
    "            futures = []\n",
    "            for chunk in chunks:\n",
    "                futures.append(executor.submit(self._process_chunk, chunk, data.edge_index, neighbor_features, prev_time_features))\n",
    "            for future in futures:\n",
    "                processed_chunks.append(future.result())\n",
    "        else:\n",
    "            for chunk in chunks:\n",
    "                processed_chunks.append(self._process_chunk(chunk, data.edge_index, neighbor_features, prev_time_features))\n",
    "\n",
    "        return torch.cat(processed_chunks, dim=0)\n",
    "\n",
    "    def _process_chunk(self, x_chunk, edge_index, neighbor_features, prev_time_features):\n",
    "        \"\"\"\n",
    "        Minimal chunk processing code with adaptive device/disk usage.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Offload everything to CPU first\n",
    "            device = torch.device(\"cpu\")\n",
    "            x_chunk = x_chunk.to(device)\n",
    "\n",
    "            # If GPU is available and usage is below threshold, move to GPU\n",
    "            if torch.cuda.is_available():\n",
    "                total_gpu_mem = torch.cuda.get_device_properties(0).total_memory\n",
    "                used_gpu_mem = torch.cuda.memory_reserved(0)\n",
    "                usage_ratio = used_gpu_mem / total_gpu_mem if total_gpu_mem else 1.0\n",
    "\n",
    "                if usage_ratio < 0.5:  # Simplistic example condition\n",
    "                    device = torch.device(\"cuda\")\n",
    "                    x_chunk = x_chunk.to(device)\n",
    "\n",
    "            # Perform GCN operations in chunks if needed (the chunk is already small).\n",
    "            x = self.conv1(x_chunk, edge_index.to(device))\n",
    "            x = F.relu(x)\n",
    "            x = self.conv2(x, edge_index.to(device))\n",
    "\n",
    "            return x.to(\"cpu\")  # Return final result to CPU, or disk if needed\n",
    "        except Exception as e:\n",
    "            print(f\"[Supernode] Error in _process_chunk: {e}\")\n",
    "            return x_chunk  # fallback to input\n",
    "\n",
    "    def _safe_forward(self, data, neighbor_features, prev_time_features):\n",
    "        \"\"\"Fallback forward pass with minimal functionality.\"\"\"\n",
    "        x = F.relu(self.conv1(data.x, data.edge_index))\n",
    "        return self.conv2(x, data.edge_index)\n",
    "\n",
    "    @staticmethod\n",
    "    def chunk_tensor(tensor, chunk_size=None):\n",
    "        \"\"\"Split tensor into chunks for streaming processing.\"\"\"\n",
    "        if chunk_size is None:\n",
    "            chunk_size = tensor.shape[0]\n",
    "        return torch.split(tensor, chunk_size)\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up disk cache and temporary files.\"\"\"\n",
    "        self.checkpoint_manager.cleanup()\n",
    "        if os.path.exists(self.disk_cache_dir):\n",
    "            shutil.rmtree(self.disk_cache_dir)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 2. CONSTRUCT A SINGLE 3×3 SUPERNODE GRAPH\n",
    "###############################################################################\n",
    "def create_dense_supernode_graph(size: int = 3, feature_dim: int = 16) -> Data:\n",
    "    \"\"\"\n",
    "    (L44) Creates a single, fully-connected 3×3 graph (9 nodes). The adjacency is \n",
    "          complete (except self-loops), and features are random initialization.\n",
    "    \"\"\"\n",
    "    # (L45) 3×3 => 9 nodes\n",
    "    num_nodes = size * size\n",
    "    x = torch.randn((num_nodes, feature_dim))  # random feature initialization\n",
    "    adj = torch.ones((num_nodes, num_nodes)) - torch.eye(num_nodes)  # fully connected minus self-loops\n",
    "    edge_index, _ = dense_to_sparse(adj)  # convert adjacency matrix to edge list\n",
    "    return Data(x=x, edge_index=edge_index)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 3. EIDOS: A GRID OF SUPERNODES OVER X×Y×Z, EVOLVED OVER T TIME STEPS\n",
    "###############################################################################\n",
    "class Eidos:\n",
    "    \"\"\"\n",
    "    (L46) Eidos organizes multiple supernodes in a 3D grid (x_dim, y_dim, z_dim),\n",
    "          each advanced one step at a time for t_steps. This forms a spatiotemporal\n",
    "          GCN for tasks like text CLM or MNIST classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_dim: int,\n",
    "        y_dim: int,\n",
    "        z_dim: int,\n",
    "        t_steps: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        supernode_class=Supernode,\n",
    "        chunk_size: int = 1024,  # Default chunk size for processing\n",
    "        checkpoint_dir: str = \"./checkpoints\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        (L47) Eidos constructor:\n",
    "             - x_dim, y_dim, z_dim: the grid shape in 3D\n",
    "             - t_steps: how many time steps we evolve\n",
    "             - in_channels, out_channels: feature sizes for supernode\n",
    "             - supernode_class: by default, uses Supernode\n",
    "             - chunk_size: size of chunks for memory-efficient processing\n",
    "             - checkpoint_dir: directory for model checkpoints\n",
    "        \"\"\"\n",
    "        self.x_dim = x_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.t_steps = t_steps\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "        # Initialize checkpoint manager\n",
    "        self.checkpoint_manager = CheckpointManager(checkpoint_dir)\n",
    "\n",
    "        # (L48) A single shared supernode_model for all cells:\n",
    "        self.supernode_model = supernode_class(in_channels, out_channels)\n",
    "\n",
    "        # (L49) Create a template 3×3 supernode graph\n",
    "        self.template_data = create_dense_supernode_graph(size=3, feature_dim=in_channels)\n",
    "\n",
    "        # (L50) Maintain two grids for each cell over all time steps: current_grid, next_grid\n",
    "        self.current_grid = {}\n",
    "        self.next_grid = {}\n",
    "        for t in range(self.t_steps):\n",
    "            for z in range(self.z_dim):\n",
    "                for y in range(self.y_dim):\n",
    "                    for x in range(self.x_dim):\n",
    "                        self.current_grid[(x, y, z, t)] = self.template_data.clone()\n",
    "                        self.next_grid[(x, y, z, t)] = self.template_data.clone()\n",
    "\n",
    "        # (L51) Additional heads can be attached for multi-task or multi-head usage\n",
    "        self.additional_heads = {}\n",
    "\n",
    "        # Setup disk caching for large operations\n",
    "        self.disk_cache_dir = \"./disk_offload_dir/eidos_cache\"\n",
    "        # create it if not present\n",
    "        os.makedirs(\"./disk_offload_dir\", exist_ok=True)\n",
    "        os.makedirs(self.disk_cache_dir, exist_ok=True)\n",
    "\n",
    "    def attach_head(self, name: str, head_module: nn.Module):\n",
    "        \"\"\"\n",
    "        (L52) Attach an additional head (e.g., a classifier) to the Eidos model for\n",
    "              specialized tasks. The head might accept the final embeddings from\n",
    "              run_full_sequence and produce a classification output.\n",
    "        \"\"\"\n",
    "        self.additional_heads[name] = head_module\n",
    "\n",
    "    def get_neighbor_features(self, x, y, z, t) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        (L53) Average the node features from valid neighboring grid cells in ±x, ±y, ±z.\n",
    "             If none exist, return None.\n",
    "        \"\"\"\n",
    "        neighbor_coords = [\n",
    "            (x - 1, y, z),\n",
    "            (x + 1, y, z),\n",
    "            (x, y - 1, z),\n",
    "            (x, y + 1, z),\n",
    "            (x, y, z - 1),\n",
    "            (x, y, z + 1)\n",
    "        ]\n",
    "        neighbors = []\n",
    "        for nx, ny, nz in neighbor_coords:\n",
    "            if 0 <= nx < self.x_dim and 0 <= ny < self.y_dim and 0 <= nz < self.z_dim:\n",
    "                neighbors.append(self.current_grid[(nx, ny, nz, t)].x)\n",
    "        if len(neighbors) == 0:\n",
    "            return None\n",
    "        return torch.stack(neighbors).mean(dim=0)\n",
    "\n",
    "    def get_temporal_features(self, x, y, z, t) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        (L54) Return node features from t-1 if valid, else None.\n",
    "        \"\"\"\n",
    "        if t <= 0:\n",
    "            return torch.randn_like(self.current_grid[(x, y, z, t)].x)\n",
    "        return self.current_grid[(x, y, z, t - 1)].x\n",
    "\n",
    "    def _process_one_cell(self, z, y, x, t):\n",
    "        \"\"\"\n",
    "        Worker function for concurrency; processes a single cell (x,y,z,t).\n",
    "        Now integrated with disk-first, CPU-second, GPU-third logic.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            current_data = self.current_grid[(x, y, z, t)]\n",
    "            neighbor_data = self.get_neighbor_features(x, y, z, t)\n",
    "            temporal_data = self.get_temporal_features(x, y, z, t)\n",
    "\n",
    "            # Move data.x to CPU by default\n",
    "            current_data.x = current_data.x.to(\"cpu\")\n",
    "\n",
    "            # If needed, we can load from disk or move to GPU later\n",
    "            # ... or chunk further if x is large ...\n",
    "            updated_features = self.supernode_model._process_chunk(\n",
    "                current_data.x,\n",
    "                current_data.edge_index,\n",
    "                neighbor_data,\n",
    "                temporal_data\n",
    "            )\n",
    "\n",
    "            self.next_grid[(x, y, z, t)].x = updated_features\n",
    "        except Exception as e:\n",
    "            print(f\"[Eidos] Error in _process_one_cell at (x={x},y={y},z={z},t={t}): {e}\")\n",
    "\n",
    "    def process_time_step(self, t: int):\n",
    "        \"\"\"\n",
    "        (L56) Processes all cells at time t concurrently using ThreadPoolExecutor.\n",
    "        \"\"\"\n",
    "        tasks = []\n",
    "        # (L57) Use CPU_count-2 threads if possible, or at least 1\n",
    "        max_workers = max(1, psutil.cpu_count(logical=True) - 2)\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            for z in range(self.z_dim):\n",
    "                for y in range(self.y_dim):\n",
    "                    for x in range(self.x_dim):\n",
    "                        tasks.append(executor.submit(self._process_one_cell, z, y, x, t))\n",
    "\n",
    "            # (L58) Wait for tasks to finish\n",
    "            for future in as_completed(tasks):\n",
    "                try:\n",
    "                    _ = future.result()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in concurrency worker: {e}\")\n",
    "\n",
    "    def run_full_sequence(self):\n",
    "        \"\"\"\n",
    "        Evolves the grid for t_steps, ensuring chunkwise, disk/CPU/GPU usage.\n",
    "        \"\"\"\n",
    "        for t in range(self.t_steps):\n",
    "            log_resource_usage(tag=f\"TimeStep{t}\")\n",
    "            self.process_time_step(t)\n",
    "\n",
    "            # Move all node features to disk/CPU after each time step if needed\n",
    "            for z in range(self.z_dim):\n",
    "                for y in range(self.y_dim):\n",
    "                    for x in range(self.x_dim):\n",
    "                        # Offload to CPU by default; in real scenario we can store to disk\n",
    "                        self.current_grid[(x, y, z, t)].x = \\\n",
    "                            self.current_grid[(x, y, z, t)].x.to(\"cpu\")\n",
    "\n",
    "            # Copy all next_grid => current_grid\n",
    "            for z in range(self.z_dim):\n",
    "                for y in range(self.y_dim):\n",
    "                    for x in range(self.x_dim):\n",
    "                        self.current_grid[(x, y, z, t)].x = \\\n",
    "                            self.next_grid[(x, y, z, t)].x.clone().to(\"cpu\")\n",
    "\n",
    "            # Save checkpoint after each time step\n",
    "            self.checkpoint_manager.save_checkpoint(\n",
    "                {\n",
    "                    'model_state': self.supernode_model.state_dict(),\n",
    "                    'current_grid': {\n",
    "                        k: v.x.cpu() for k, v in self.current_grid.items()\n",
    "                    },\n",
    "                    'next_grid': {\n",
    "                        k: v.x.cpu() for k, v in self.next_grid.items()\n",
    "                    },\n",
    "                    'time_step': t\n",
    "                },\n",
    "                metadata={'time_step': t}\n",
    "            )\n",
    "\n",
    "    def reinitialize_grid(self):\n",
    "        \"\"\"\n",
    "        (L60) Reset both current_grid and next_grid to the template 3×3 supernode data.\n",
    "        \"\"\"\n",
    "        for t in range(self.t_steps):\n",
    "            for z in range(self.z_dim):\n",
    "                for y in range(self.y_dim):\n",
    "                    for x in range(self.x_dim):\n",
    "                        self.current_grid[(x, y, z, t)] = self.template_data.clone()\n",
    "                        self.next_grid[(x, y, z, t)] = self.template_data.clone()\n",
    "\n",
    "    def get_final_embeddings(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Collect embeddings from the last time step (t_steps-1) across all (x,y,z),\n",
    "        ensuring consistent tensor sizes for concatenation.\n",
    "        \"\"\"\n",
    "        final_ts = self.t_steps - 1\n",
    "        outputs = []\n",
    "        for z in range(self.z_dim):\n",
    "            for y in range(self.y_dim):\n",
    "                for x in range(self.x_dim):\n",
    "                    node_embedding = self.current_grid[(x, y, z, final_ts)].x\n",
    "                    # Ensure the embedding size matches the expected output size\n",
    "                    if node_embedding.size(1) != self.out_channels:\n",
    "                        node_embedding = F.linear(node_embedding, torch.eye(self.out_channels, node_embedding.size(1)))\n",
    "                    outputs.append(node_embedding)\n",
    "        return torch.cat(outputs, dim=0)\n",
    "\n",
    "    def expand_grid(self, expand_x=0, expand_y=0):\n",
    "        \"\"\"\n",
    "        (L62) Dynamically expand the grid in the X or Y dimension, preserving existing data.\n",
    "        \"\"\"\n",
    "        new_x_dim = self.x_dim + expand_x\n",
    "        new_y_dim = self.y_dim + expand_y\n",
    "        if expand_x <= 0 and expand_y <= 0:\n",
    "            print(\"No expansion requested. Doing nothing.\")\n",
    "            return\n",
    "        new_cur = {}\n",
    "        new_next = {}\n",
    "        for t in range(self.t_steps):\n",
    "            for z in range(self.z_dim):\n",
    "                for ny in range(new_y_dim):\n",
    "                    for nx in range(new_x_dim):\n",
    "                        if nx < self.x_dim and ny < self.y_dim:\n",
    "                            new_cur[(nx, ny, z, t)] = self.current_grid[(nx, ny, z, t)]\n",
    "                            new_next[(nx, ny, z, t)] = self.next_grid[(nx, ny, z, t)]\n",
    "                        else:\n",
    "                            new_cur[(nx, ny, z, t)] = self.template_data.clone()\n",
    "                            new_next[(nx, ny, z, t)] = self.template_data.clone()\n",
    "\n",
    "        self.x_dim = new_x_dim\n",
    "        self.y_dim = new_y_dim\n",
    "        self.current_grid = new_cur\n",
    "        self.next_grid = new_next\n",
    "        print(f\"Grid expanded to x_dim={self.x_dim}, y_dim={self.y_dim}.\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 3. TASK DETECTION & ADAPTATION (REVISED FOR MORE ROBUST META-LEARNING)\n",
    "###############################################################################\n",
    "class MetaTaskGrid(nn.Module):\n",
    "    \"\"\"\n",
    "    A smarter meta-grid that detects tasks using a learnable neural network-based\n",
    "    embedding and unsupervised clustering, rather than a simple linear threshold.\n",
    "    Once a new task is identified, it can trigger a more flexible expansion\n",
    "    strategy in the main Eidos grid (e.g., expanding x_dim, y_dim, or z_dim,\n",
    "    depending on available resources).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, embedding_dim: int = 32):\n",
    "        super().__init__()\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # (A) Replace the old linear + threshold detection with a small net +\n",
    "        #     unsupervised clustering approach\n",
    "        # ----------------------------------------------------------------------\n",
    "        \n",
    "        # 1) A small embedding network that learns a compact representation\n",
    "        #    for each incoming sample (task).\n",
    "        #    If your tasks are text lines, images, or any data, pass them here.\n",
    "        self.task_detection_net = nn.Sequential(\n",
    "            nn.Linear(in_channels, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, embedding_dim)\n",
    "        )\n",
    "\n",
    "        # 2) Maintain a dynamic list of cluster centers, each representing a known task\n",
    "        self.cluster_centers = []  # list of shape [embedding_dim,]\n",
    "        \n",
    "        # 3) Distance threshold for deciding if a new cluster (i.e., new task) is found\n",
    "        self.distance_threshold = 15.0  # tune as needed\n",
    "\n",
    "        # 4) Optional aggregator (like a simple buffer) for unsupervised refinement\n",
    "        #    (We keep it minimal to maintain the \"small changes\" requirement).\n",
    "        self.recent_embeddings = []\n",
    "        self.max_recent = 256  # keep up to 256 embeddings in memory\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encodes an input into a task embedding using our small neural net.\n",
    "        This forward pass can also be used for training the net in a supervised\n",
    "        or self-supervised manner, if desired.\n",
    "        \"\"\"\n",
    "        return self.task_detection_net(x)\n",
    "\n",
    "    def detect_new_task(self, x: torch.Tensor) -> bool:\n",
    "        \"\"\"\n",
    "        1) Generate an embedding for x.\n",
    "        2) Compute distance from existing cluster centers.\n",
    "        3) If it exceeds the threshold from all known clusters => new cluster => new task.\n",
    "        4) Otherwise, treat as existing cluster => no new supernode expansions.\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # (A) Get embedding\n",
    "            embedding = self.forward(x)  # shape e.g. [*, embedding_dim]\n",
    "            # We average over first dimension if multiple samples\n",
    "            embedding_mean = embedding.mean(dim=0)\n",
    "\n",
    "            # (B) Compute distance to any known cluster center\n",
    "            if not self.cluster_centers:\n",
    "                # If no known tasks, first encountered => new task\n",
    "                self.cluster_centers.append(embedding_mean.clone())\n",
    "                return True\n",
    "            else:\n",
    "                dists = []\n",
    "                for center in self.cluster_centers:\n",
    "                    dist = torch.norm(embedding_mean - center, p=2).item()\n",
    "                    dists.append(dist)\n",
    "\n",
    "                min_dist = min(dists)\n",
    "                if min_dist > self.distance_threshold:\n",
    "                    # new cluster => add center\n",
    "                    self.cluster_centers.append(embedding_mean.clone())\n",
    "                    return True\n",
    "                else:\n",
    "                    # existing cluster => optional update center\n",
    "                    closest_idx = dists.index(min_dist)\n",
    "                    # Simple approach: nudge the cluster center\n",
    "                    self.cluster_centers[closest_idx] = 0.95*self.cluster_centers[closest_idx] + \\\n",
    "                                                        0.05*embedding_mean\n",
    "                    return False\n",
    "\n",
    "    def refine_clusters(self):\n",
    "        \"\"\"\n",
    "        OPTIONAL: A method to refine cluster centers with stored embeddings\n",
    "        (e.g., via k-means). We keep it minimal for demonstration.\n",
    "        \"\"\"\n",
    "        if len(self.recent_embeddings) < 2 or not self.cluster_centers:\n",
    "            return  # not enough data to refine\n",
    "\n",
    "        # Example approach: single iteration of naive re-clustering\n",
    "        # Could add more sophisticated logic if needed\n",
    "        embeddings_tensor = torch.stack(self.recent_embeddings, dim=0)\n",
    "        new_centers = [torch.zeros_like(self.cluster_centers[0]) for _ in self.cluster_centers]\n",
    "        counts = [0]*len(self.cluster_centers)\n",
    "\n",
    "        for emb in embeddings_tensor:\n",
    "            dists = [torch.norm(emb - c, p=2).item() for c in self.cluster_centers]\n",
    "            idx = dists.index(min(dists))\n",
    "            new_centers[idx] += emb\n",
    "            counts[idx] += 1\n",
    "\n",
    "        for i, ccount in enumerate(counts):\n",
    "            if ccount > 0:\n",
    "                new_centers[i] = new_centers[i]/ccount\n",
    "\n",
    "        # Update cluster centers\n",
    "        for i in range(len(self.cluster_centers)):\n",
    "            if counts[i] > 0:\n",
    "                self.cluster_centers[i] = 0.5*self.cluster_centers[i] + 0.5*new_centers[i]\n",
    "\n",
    "        # Clear recent memory for next round (optional)\n",
    "        self.recent_embeddings = []\n",
    "\n",
    "    def store_embedding_for_refinement(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Keep track of embeddings for optional unsupervised cluster refinement.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            emb = self.forward(x)\n",
    "            if emb.ndim == 2:\n",
    "                emb = emb.mean(dim=0)\n",
    "            self.recent_embeddings.append(emb.clone())\n",
    "\n",
    "        if len(self.recent_embeddings) > self.max_recent:\n",
    "            self.recent_embeddings.pop(0)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 3.1: A More Flexible Expansion Strategy Within Eidos\n",
    "###############################################################################\n",
    "def expand_for_new_task(eidos_model, axis=\"x\"):\n",
    "    \"\"\"\n",
    "    Expands Eidos along a specified axis. \n",
    "    axis can be 'x', 'y', or 'z' for new layers.\n",
    "    Minimal changes: re-use eidos_model.expand_grid or\n",
    "    define a new z-based expansion if needed.\n",
    "    \"\"\"\n",
    "    # For demonstration, we do a simple approach:\n",
    "    # - expand along x => eidos_model.expand_grid(expand_x=1)\n",
    "    # - expand along y => eidos_model.expand_grid(expand_y=1)\n",
    "    # - expand along z => we define a new function expand_z if needed\n",
    "    # This is just an example. Full z expansion can be added if desired.\n",
    "    if axis == \"x\":\n",
    "        eidos_model.expand_grid(expand_x=1)\n",
    "    elif axis == \"y\":\n",
    "        eidos_model.expand_grid(expand_y=1)\n",
    "    elif axis == \"z\":\n",
    "        print(\"Expanding along z-dimension is not yet implemented. Consider adding it.\")\n",
    "    else:\n",
    "        print(f\"Unknown axis {axis}; no expansion performed.\")\n",
    "\n",
    "###############################################################################\n",
    "# 3.2: Minimal changes in main or wherever new tasks are detected\n",
    "###############################################################################\n",
    "# Below is a snippet showing how to incorporate the new meta-learning logic\n",
    "# into your existing system with minimal changes.\n",
    "\n",
    "    def detect_new_task(self, x: torch.Tensor) -> bool:\n",
    "        \"\"\"\n",
    "        (L66) If the embedding's mean absolute value > threshold => new task\n",
    "        \"\"\"\n",
    "        embedding = self.forward(x)\n",
    "        measure = embedding.abs().mean().item()\n",
    "        return (measure > self.threshold)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 5. ADVANCED CLM HEAD: QWEN-BASED TEXT GENERATION\n",
    "###############################################################################\n",
    "class AdvancedCLMHead(nn.Module):\n",
    "    \"\"\"\n",
    "    (L67) Wraps a Qwen-based LM for text generation. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"Qwen/Qwen2.5-0.5B-Instruct\"):\n",
    "        \"\"\"\n",
    "        (L68) Loads the Qwen model. Using it as a pretrained foundation froom which to train the Eidos model.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Hugging Face model repository name.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        print(f\"Loading advanced CLM model: {model_name}\")\n",
    "\n",
    "        try:\n",
    "            self.lm_model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=\"auto\",\n",
    "                device_map=\"auto\",\n",
    "                load_in_8bit=True if torch.cuda.is_available() else False,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model from Hugging Face: {e}\")\n",
    "            print(\"Attempting to load model from local directory './saved_models'\")\n",
    "            local_model_name = \"./saved_models/Qwen2.5-0.5B-Instruct\"\n",
    "            try:\n",
    "                self.lm_model = AutoModelForCausalLM.from_pretrained(\n",
    "                    local_model_name,\n",
    "                    torch_dtype=\"auto\",\n",
    "                    device_map=\"auto\",\n",
    "                    load_in_8bit=True if torch.cuda.is_available() else False,\n",
    "                    trust_remote_code=True\n",
    "                )\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(local_model_name)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading model from local directory: {e}\")\n",
    "                self.tokenizer = None  # Ensure tokenizer is set to None if loading fails\n",
    "\n",
    "        if self.tokenizer is not None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        # --------------------- ADDED LINES BELOW FOR MEMORY SAVING ---------------------\n",
    "        # Disable cache (speeds up training / reduces memory).\n",
    "        if hasattr(self.lm_model.config, \"use_cache\"):\n",
    "            self.lm_model.config.use_cache = False\n",
    "\n",
    "        # Enable gradient checkpointing to reduce memory usage.\n",
    "        if hasattr(self.lm_model, \"gradient_checkpointing_enable\"):\n",
    "            self.lm_model.gradient_checkpointing_enable()\n",
    "        # --------------------- END ADDED LINES -----------------------------------------\n",
    "    \n",
    "    def forward(self, input_ids, labels=None):\n",
    "        \"\"\"\n",
    "        (L69) Forward pass => returns HF output with .loss and .logits\n",
    "        \"\"\"\n",
    "        outputs = self.lm_model(input_ids=input_ids, labels=labels)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 6. TRAIN ON ADVANCED CLM\n",
    "###############################################################################\n",
    "def train_on_advanced_clm(eidos_model, lines_of_text, epochs=1, checkpoint_path=None):\n",
    "    \"\"\"\n",
    "    (L70) Train Qwen-based LM on text lines using chunk-size=1 for memory efficiency.\n",
    "    \"\"\"\n",
    "    # Initialize the CLM head\n",
    "    try:\n",
    "        clm_head = AdvancedCLMHead()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from Hugging Face: {e}\")\n",
    "        print(\"Attempting to load model from local directory './saved_models'\")\n",
    "        try:\n",
    "            clm_head = AdvancedCLMHead(model_name=\"./saved_models/Qwen2.5-0.5B-Instruct\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model from local directory: {e}\")\n",
    "            return\n",
    "\n",
    "    chunk_size = 1\n",
    "\n",
    "    # Load checkpoint if available\n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        print(f\"Loading CLM checkpoint from {checkpoint_path}\")\n",
    "        state = torch.load(checkpoint_path)\n",
    "        clm_head.load_state_dict(state[\"transformers_head\"])\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        list(clm_head.parameters()) + list(eidos_model.supernode_model.parameters()),\n",
    "        lr=1e-4\n",
    "    )\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        total_count = 0\n",
    "        log_resource_usage(tag=f\"StartOfEpoch{epoch+1}\")\n",
    "\n",
    "        random.shuffle(lines_of_text)\n",
    "\n",
    "        for start_idx in range(0, len(lines_of_text), chunk_size):\n",
    "            sub_lines = lines_of_text[start_idx:start_idx + chunk_size]\n",
    "            log_resource_usage(tag=f\"Epoch{epoch+1}-Chunk\")\n",
    "\n",
    "            for text_line in sub_lines:\n",
    "                log_resource_usage(tag=f\"PreLine{total_count}\")\n",
    "\n",
    "                # Update to use Qwen's chat template pattern\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": text_line}\n",
    "                ]\n",
    "                text = clm_head.tokenizer.apply_chat_template(\n",
    "                    messages,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True\n",
    "                )\n",
    "                model_inputs = clm_head.tokenizer([text], return_tensors=\"pt\").to(clm_head.lm_model.device)\n",
    "                input_ids = model_inputs.input_ids\n",
    "                labels = input_ids.clone()\n",
    "\n",
    "                eidos_model.reinitialize_grid()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = clm_head(input_ids=input_ids, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                log_resource_usage(tag=f\"PostLine{total_count}\")\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                total_count += 1\n",
    "\n",
    "        avg_loss = total_loss / max(1, total_count)\n",
    "        ppl = math.exp(avg_loss) if avg_loss < 20 else float('inf')\n",
    "        print(f\"[CLM][Epoch {epoch+1}/{epochs}] AvgLoss={avg_loss:.4f}, PPL={ppl:.4f}\")\n",
    "\n",
    "        if checkpoint_path:\n",
    "            torch.save({\"transformers_head\": clm_head.state_dict()}, checkpoint_path)\n",
    "            print(f\"CLM checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "        log_resource_usage(tag=f\"EndOfEpoch{epoch+1}\")\n",
    "\n",
    "###############################################################################\n",
    "# 7. CHAT WITH MODEL\n",
    "###############################################################################\n",
    "def chat_with_model(checkpoint_path: str, model_name=\"Qwen/Qwen2.5-0.5B-Instruct\"):\n",
    "    \"\"\"\n",
    "    (L77) Minimal example: loads CLM from checkpoint, prompts it, prints generation.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(\"No CLM checkpoint found. Skipping chat.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loading CLM checkpoint from {checkpoint_path}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    clm_head = AdvancedCLMHead(model_name=model_name)\n",
    "\n",
    "    state = torch.load(checkpoint_path)\n",
    "    clm_head.load_state_dict(state[\"transformers_head\"], strict=False)\n",
    "    clm_head.eval()\n",
    "\n",
    "    prompt = \"Hello Eidos! Can you summarize the concept of a supernode grid for me?\"\n",
    "    input_tokens = tokenizer(prompt, return_tensors=\"pt\").to(clm_head.lm_model.device)\n",
    "\n",
    "    gen_config = GenerationConfig(\n",
    "        max_new_tokens=60,\n",
    "        do_sample=True,\n",
    "        temperature=0.8\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = clm_head.lm_model.generate(**input_tokens, generation_config=gen_config)\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"\\nUser:\", prompt)\n",
    "    print(\"Eidos:\", response)\n",
    "    print(\"---- End of chat example ----\\n\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 8. TRAIN ON MNIST\n",
    "###############################################################################\n",
    "def train_on_mnist(grid_model: Eidos, mnist_dataset, epochs: int = 1,\n",
    "                   learn_rate: float = 1e-3, checkpoint_path: str = None):\n",
    "    \"\"\"\n",
    "    (L78) Trains an Eidos model for MNIST classification. \n",
    "    \"\"\"\n",
    "    num_supernodes = grid_model.x_dim * grid_model.y_dim * grid_model.z_dim\n",
    "    input_dim = num_supernodes * 9 * grid_model.out_channels\n",
    "    classifier_head = nn.Linear(input_dim, 10)\n",
    "\n",
    "    # (L79) Possibly load checkpoint\n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        print(f\"Found checkpoint {checkpoint_path}. Resuming training from it.\")\n",
    "        chk = torch.load(checkpoint_path)\n",
    "        grid_model.supernode_model.load_state_dict(chk[\"model\"])\n",
    "        classifier_head.load_state_dict(chk[\"classifier\"])\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        list(grid_model.supernode_model.parameters()) + list(classifier_head.parameters()),\n",
    "        lr=learn_rate\n",
    "    )\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    data_loader = DataLoader(mnist_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_count = 0\n",
    "        loop = tqdm(enumerate(data_loader), total=len(data_loader),\n",
    "                    desc=f\"Epoch {epoch+1}/{epochs}\", leave=True)\n",
    "\n",
    "        for i, (img, label) in loop:\n",
    "            # (L80) Re-init grid for each sample\n",
    "            grid_model.reinitialize_grid()\n",
    "\n",
    "            # (L81) Flatten => expand => place in supernodes\n",
    "            flattened = img.view(1, 28 * 28).detach()\n",
    "            expanded = flattened.expand(9, -1)\n",
    "\n",
    "            for z in range(grid_model.z_dim):\n",
    "                for y in range(grid_model.y_dim):\n",
    "                    for x in range(grid_model.x_dim):\n",
    "                        grid_model.current_grid[(x, y, z, 0)].x = expanded.clone()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # (L82) Run concurrency => finalize => pass to classifier\n",
    "            grid_model.run_full_sequence()\n",
    "            final_embs = grid_model.get_final_embeddings()  # [N*9, out_channels]\n",
    "            flat_emb = final_embs.view(1, -1)               # [1, N*9*out_channels]\n",
    "\n",
    "            logits = classifier_head(flat_emb)\n",
    "            loss = loss_fn(logits, label)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_correct += (preds == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "\n",
    "            loop.set_postfix({\n",
    "                \"loss\": f\"{loss.item():.4f}\",\n",
    "                \"acc\": f\"{(total_correct / total_count):.4f}\"\n",
    "            })\n",
    "\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        avg_acc = total_correct / total_count\n",
    "        print(f\"Epoch {epoch+1} complete. Loss={avg_loss:.4f}, Acc={avg_acc:.4f}\")\n",
    "\n",
    "        # (L83) Save checkpoint\n",
    "        if checkpoint_path:\n",
    "            torch.save({\n",
    "                \"model\": grid_model.supernode_model.state_dict(),\n",
    "                \"classifier\": classifier_head.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"epoch\": epoch,\n",
    "                \"loss\": avg_loss,\n",
    "                \"accuracy\": avg_acc\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 9. TEST ON MNIST\n",
    "###############################################################################\n",
    "def test_on_mnist(grid_model: Eidos, checkpoint_path: str = None):\n",
    "    \"\"\"\n",
    "    (L84) Evaluate Eidos on the MNIST test set, optionally loading a checkpoint.\n",
    "    \"\"\"\n",
    "    num_supernodes = grid_model.x_dim * grid_model.y_dim * grid_model.z_dim\n",
    "    input_dim = num_supernodes * 9 * grid_model.out_channels\n",
    "    classifier_head = nn.Linear(input_dim, 10)\n",
    "\n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        print(f\"Test phase: loading checkpoint from {checkpoint_path}.\")\n",
    "        chk = torch.load(checkpoint_path)\n",
    "        grid_model.supernode_model.load_state_dict(chk[\"model\"])\n",
    "        classifier_head.load_state_dict(chk[\"classifier\"])\n",
    "    else:\n",
    "        print(\"No checkpoint found. Testing with current weights.\")\n",
    "\n",
    "    grid_model.supernode_model.eval()\n",
    "    classifier_head.eval()\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    mnist_test = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "    test_loader = DataLoader(mnist_test, batch_size=1, shuffle=False)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for img, label in tqdm(test_loader, desc=\"Testing\", leave=True):\n",
    "            # (L85) For each sample, re-init => flatten => expand => run\n",
    "            grid_model.reinitialize_grid()\n",
    "            flattened = img.view(1, 28 * 28)\n",
    "            expanded = flattened.expand(9, -1)\n",
    "\n",
    "            for z in range(grid_model.z_dim):\n",
    "                for y in range(grid_model.y_dim):\n",
    "                    for x in range(grid_model.x_dim):\n",
    "                        grid_model.current_grid[(x, y, z, 0)].x = expanded.clone()\n",
    "\n",
    "            grid_model.run_full_sequence()\n",
    "            final_embs = grid_model.get_final_embeddings()\n",
    "            flat_emb = final_embs.view(1, -1)\n",
    "            logits = classifier_head(flat_emb)\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == label).sum().item()\n",
    "            total += label.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    print(f\"Test Accuracy on entire MNIST test set: {acc:.4f}\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 10. MAIN PIPELINE => TEXT THEN MNIST\n",
    "###############################################################################\n",
    "def main():\n",
    "    \"\"\"\n",
    "    (L86) The main function that orchestrates:\n",
    "         1) Possibly load text from dataset_downloader_text.py if available,\n",
    "            else use local lines_of_text.\n",
    "         2) Build an Eidos (in_channels=768) => train on text => chat => checkpoint.\n",
    "         3) Build new Eidos (in_channels=784) => train on MNIST => evaluate => expand.\n",
    "    \"\"\"\n",
    "    # (L87) Attempt to load lines_of_text from dataset_downloader_text.py if available, else fallback\n",
    "    if DATASET_DOWNLOADER_AVAILABLE:\n",
    "        print(\"Loading text from dataset_downloader_text.py ...\")\n",
    "        lines_of_text = load_text_data('.\\datasets\\openai_humaneval\\humaneval.jsonl')  # user-provided function from dataset_downloader_text\n",
    "    else:\n",
    "        print(\"dataset_downloader_text.py not found, using local lines_of_text fallback...\")\n",
    "        lines_of_text = [\n",
    "            \"This is a short line to test QWEN-based Eidos training on small data.\",\n",
    "            \"Another line to ensure we handle chunk-based input, minimal exemplars.\",\n",
    "            \"In a real scenario, we would load from dataset_downloader_text.py JSONL.\",\n",
    "            \"We can keep appending more lines if desired...\"\n",
    "        ]\n",
    "\n",
    "    print(\"Starting advanced text training with Qwen-based Eidos model...\\n\")\n",
    "    # (L88) Construct Eidos for text => 2×2×1 grid, t_steps=3, in_channels=768, out=64\n",
    "    text_grid = Eidos(\n",
    "        x_dim=2,\n",
    "        y_dim=2,\n",
    "        z_dim=1,\n",
    "        t_steps=3,\n",
    "        in_channels=768,\n",
    "        out_channels=64\n",
    "    )\n",
    "    train_on_advanced_clm(text_grid, lines_of_text, epochs=1, checkpoint_path=\"clm_checkpoint.pt\")\n",
    "    print(\"Text training complete.\\n\")\n",
    "\n",
    "    # (L89) Chat with final model\n",
    "    chat_with_model(checkpoint_path=\"clm_checkpoint.pt\", model_name=\"Qwen/Qwen2.5-0.5B-Instruct\")\n",
    "\n",
    "    print(\"Loading Eidos for MNIST tasks. Rebuilding with in_channels=784 for images...\\n\")\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    mnist_train = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "\n",
    "    # (L90) Another Eidos instance => 2×2×1 grid => specialized for MNIST\n",
    "    main_grid = Eidos(\n",
    "        x_dim=2,\n",
    "        y_dim=2,\n",
    "        z_dim=1,\n",
    "        t_steps=3,\n",
    "        in_channels=28 * 28,\n",
    "        out_channels=64\n",
    "    )\n",
    "\n",
    "    # (L91) Initialize a few cells with examples\n",
    "    for i, (img, label) in enumerate(mnist_train):\n",
    "        if i >= 4:\n",
    "            break\n",
    "        flattened = img.view(1, 28 * 28)\n",
    "        expanded = flattened.expand(9, -1)\n",
    "        xx = i % 2\n",
    "        yy = (i // 2) % 2\n",
    "        main_grid.current_grid[(xx, yy, 0, 0)].x = expanded.clone()\n",
    "\n",
    "    # (L92) Build a meta-grid for new task detection\n",
    "    meta_grid = MetaTaskGrid(in_channels=28 * 28, embedding_dim=16)\n",
    "\n",
    "    # (L93) Attach an additional head => \"aux_classifier\"\n",
    "    alt_classifier = nn.Linear(main_grid.x_dim * main_grid.y_dim * 9 * main_grid.out_channels, 10)\n",
    "    main_grid.attach_head(\"aux_classifier\", alt_classifier)\n",
    "\n",
    "    print(\"Starting full training on MNIST...\")\n",
    "    train_on_mnist(\n",
    "        main_grid,\n",
    "        mnist_train,\n",
    "        epochs=2,\n",
    "        learn_rate=1e-3,\n",
    "        checkpoint_path=\"main_checkpoint.pt\"\n",
    "    )\n",
    "    print(\"Training complete.\\n\")\n",
    "\n",
    "    # (L94) Evaluate on test set\n",
    "    test_on_mnist(main_grid, checkpoint_path=\"main_checkpoint.pt\")\n",
    "\n",
    "    # (L95) Show final embeddings for curiosity\n",
    "    main_grid.run_full_sequence()\n",
    "    embeddings = main_grid.get_final_embeddings()\n",
    "    print(f\"Embeddings shape after final time step: {embeddings.shape}\")\n",
    "\n",
    "    # (L96) Attempt new task detection => if true => expand the main grid\n",
    "    next_sample = next(iter(mnist_train))[0].view(1, 28 * 28)\n",
    "    is_new_task = meta_grid.detect_new_task(next_sample)\n",
    "    if is_new_task:\n",
    "        print(\"Meta-grid detected a new task, expanding main grid by 1 in X dimension.\")\n",
    "        main_grid.expand_grid(expand_x=1)\n",
    "    else:\n",
    "        print(\"Meta-grid indicates no new task.\")\n",
    "\n",
    "    # (L97) Final resource usage\n",
    "    log_resource_usage(tag=\"Post-MNIST-Training\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# USAGE AND INTEGRATION OF MetaTaskGrid: DETECTING NEW TASKS AND EXPANDING Eidos\n",
    "###############################################################################\n",
    "def main_task_detection_example():\n",
    "    \"\"\"\n",
    "    A simple demonstration of how to use MetaTaskGrid to detect new tasks and\n",
    "    expand the Eidos model. We keep modifications minimal so that the rest of\n",
    "    the code and functionalities stay intact.\n",
    "    \"\"\"\n",
    "\n",
    "    # (A) Instantiate your Eidos model (already done in your main code).\n",
    "    #     For demonstration, we do a small 2×2×1 grid with 3 time steps.\n",
    "    main_grid = Eidos(\n",
    "        x_dim=2,\n",
    "        y_dim=2,\n",
    "        z_dim=1,\n",
    "        t_steps=3,\n",
    "        in_channels=28 * 28,\n",
    "        out_channels=64\n",
    "    )\n",
    "\n",
    "    # (B) Instantiate the improved MetaTaskGrid with embedding-based detection.\n",
    "    #     We pick embedding_dim=16 (or any suitable dimension).\n",
    "    meta_grid = MetaTaskGrid(in_channels=28 * 28, embedding_dim=16)\n",
    "\n",
    "    # (C) Assume we have a new sample in 'img' (e.g., from MNIST).\n",
    "    #     Flatten it to shape [1, 28*28]. If multiple samples, shape could be [N, 28*28].\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    mnist_train = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "\n",
    "    # (D) Just take a handful of samples to illustrate detecting new tasks.\n",
    "    for i, (img, label) in enumerate(mnist_train):\n",
    "        if i >= 5:\n",
    "            break  # limit to 5 samples for demonstration\n",
    "\n",
    "        flattened = img.view(1, 28 * 28)  # shape [1, 784]\n",
    "\n",
    "        # (E) Use meta_grid to detect a possible new task\n",
    "        #     If new task => expand the Eidos grid\n",
    "        if meta_grid.detect_new_task(flattened):\n",
    "            print(f\"[Sample {i}] Meta-grid detected a NEW or novel task -> expanding Eidos.\")\n",
    "            # Here, we choose an axis based on resource constraints or a simple round-robin\n",
    "            chosen_axis = \"x\"  # could also be \"y\" or \"z\"\n",
    "            expand_for_new_task(main_grid, axis=chosen_axis)\n",
    "        else:\n",
    "            print(f\"[Sample {i}] No new task. Re-using existing cluster of tasks.\")\n",
    "\n",
    "    print(\"\\nTask detection demonstration complete. The Eidos grid may have expanded.\")\n",
    "\n",
    "###############################################################################\n",
    "# 11. BOILERPLATE\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    # (L98) If run directly => execute main\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
