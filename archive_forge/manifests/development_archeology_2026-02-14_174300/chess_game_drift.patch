diff -ruN /tmp/user/1000/tmp.b0mW28NpTM/chess_game/__init__.py /home/lloyd/eidosian_forge/game_forge/src/chess_game/__init__.py
--- /tmp/user/1000/tmp.b0mW28NpTM/chess_game/__init__.py	1970-01-01 10:00:00.000000000 +1000
+++ /home/lloyd/eidosian_forge/game_forge/src/chess_game/__init__.py	2026-02-05 19:53:14.062818306 +1000
@@ -0,0 +1 @@
+"""Chess game prototypes and utilities."""
diff -ruN /tmp/user/1000/tmp.b0mW28NpTM/chess_game/__main__.py /home/lloyd/eidosian_forge/game_forge/src/chess_game/__main__.py
--- /tmp/user/1000/tmp.b0mW28NpTM/chess_game/__main__.py	1970-01-01 10:00:00.000000000 +1000
+++ /home/lloyd/eidosian_forge/game_forge/src/chess_game/__main__.py	2026-02-05 19:53:46.029442659 +1000
@@ -0,0 +1,77 @@
+"""Module entrypoint for chess_game prototypes."""
+
+from __future__ import annotations
+
+import argparse
+import runpy
+import sys
+from pathlib import Path
+from typing import Sequence
+
+
+def _package_dir() -> Path:
+    return Path(__file__).resolve().parent
+
+
+def _ensure_local_imports() -> None:
+    package_dir = _package_dir()
+    if str(package_dir) not in sys.path:
+        sys.path.insert(0, str(package_dir))
+
+
+def _script_path(name: str) -> Path:
+    return _package_dir() / f"{name}.py"
+
+
+def parse_args(argv: Sequence[str]) -> argparse.Namespace:
+    parser = argparse.ArgumentParser(
+        description="Chess game prototype launcher",
+        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
+    )
+    parser.add_argument(
+        "--variant",
+        choices=["prototype", "prototype_old", "init"],
+        default="prototype",
+        help="Which chess game entrypoint to run",
+    )
+    parser.add_argument("--list", action="store_true", help="List available variants")
+    parser.add_argument("variant_args", nargs=argparse.REMAINDER, help="Arguments for the variant")
+    return parser.parse_args(argv)
+
+
+def _print_list() -> None:
+    print("INFO available variants:")
+    print("- prototype: personality-driven chess prototype")
+    print("- prototype_old: legacy prototype")
+    print("- init: init.py self-tests (no GUI)")
+
+
+def _run_variant(args: argparse.Namespace) -> int:
+    _ensure_local_imports()
+    variant_args = list(args.variant_args)
+    if variant_args[:1] == ["--"]:
+        variant_args = variant_args[1:]
+    script = _script_path(args.variant)
+    sys.argv = [str(script), *variant_args]
+    globals_dict = runpy.run_path(str(script))
+    main_fn = globals_dict.get("main")
+    if main_fn is None:
+        return 0
+    return int(main_fn() or 0)
+
+
+def main(argv: Sequence[str] | None = None) -> int:
+    if argv is None:
+        argv = sys.argv[1:]
+    if not argv:
+        _print_list()
+        return 0
+    args = parse_args(argv)
+    if args.list:
+        _print_list()
+        return 0
+    return _run_variant(args)
+
+
+if __name__ == "__main__":
+    raise SystemExit(main())
Binary files /tmp/user/1000/tmp.b0mW28NpTM/chess_game/__pycache__/__init__.cpython-312.pyc and /home/lloyd/eidosian_forge/game_forge/src/chess_game/__pycache__/__init__.cpython-312.pyc differ
Binary files /tmp/user/1000/tmp.b0mW28NpTM/chess_game/__pycache__/__main__.cpython-312.pyc and /home/lloyd/eidosian_forge/game_forge/src/chess_game/__pycache__/__main__.cpython-312.pyc differ
Binary files /tmp/user/1000/tmp.b0mW28NpTM/chess_game/__pycache__/color_mapper.cpython-312.pyc and /home/lloyd/eidosian_forge/game_forge/src/chess_game/__pycache__/color_mapper.cpython-312.pyc differ
Binary files /tmp/user/1000/tmp.b0mW28NpTM/chess_game/__pycache__/concept_semantic_emoji_emotion_map.cpython-312.pyc and /home/lloyd/eidosian_forge/game_forge/src/chess_game/__pycache__/concept_semantic_emoji_emotion_map.cpython-312.pyc differ
Binary files /tmp/user/1000/tmp.b0mW28NpTM/chess_game/__pycache__/emoji_map.cpython-312.pyc and /home/lloyd/eidosian_forge/game_forge/src/chess_game/__pycache__/emoji_map.cpython-312.pyc differ
Binary files /tmp/user/1000/tmp.b0mW28NpTM/chess_game/__pycache__/init.cpython-312.pyc and /home/lloyd/eidosian_forge/game_forge/src/chess_game/__pycache__/init.cpython-312.pyc differ
Binary files /tmp/user/1000/tmp.b0mW28NpTM/chess_game/__pycache__/local_model_network.cpython-312.pyc and /home/lloyd/eidosian_forge/game_forge/src/chess_game/__pycache__/local_model_network.cpython-312.pyc differ
Binary files /tmp/user/1000/tmp.b0mW28NpTM/chess_game/__pycache__/modular_local_model_network.cpython-312.pyc and /home/lloyd/eidosian_forge/game_forge/src/chess_game/__pycache__/modular_local_model_network.cpython-312.pyc differ
Binary files /tmp/user/1000/tmp.b0mW28NpTM/chess_game/__pycache__/piecerenderer.cpython-312.pyc and /home/lloyd/eidosian_forge/game_forge/src/chess_game/__pycache__/piecerenderer.cpython-312.pyc differ
Binary files /tmp/user/1000/tmp.b0mW28NpTM/chess_game/__pycache__/prototype.cpython-312.pyc and /home/lloyd/eidosian_forge/game_forge/src/chess_game/__pycache__/prototype.cpython-312.pyc differ
Binary files /tmp/user/1000/tmp.b0mW28NpTM/chess_game/__pycache__/prototype_old.cpython-312.pyc and /home/lloyd/eidosian_forge/game_forge/src/chess_game/__pycache__/prototype_old.cpython-312.pyc differ
Binary files /tmp/user/1000/tmp.b0mW28NpTM/chess_game/__pycache__/textprocessor.cpython-312.pyc and /home/lloyd/eidosian_forge/game_forge/src/chess_game/__pycache__/textprocessor.cpython-312.pyc differ
Binary files /tmp/user/1000/tmp.b0mW28NpTM/chess_game/__pycache__/usermanager.cpython-312.pyc and /home/lloyd/eidosian_forge/game_forge/src/chess_game/__pycache__/usermanager.cpython-312.pyc differ
diff -ruN /tmp/user/1000/tmp.b0mW28NpTM/chess_game/color_mapper.py /home/lloyd/eidosian_forge/game_forge/src/chess_game/color_mapper.py
--- /tmp/user/1000/tmp.b0mW28NpTM/chess_game/color_mapper.py	2025-03-12 21:54:26.000000000 +1000
+++ /home/lloyd/eidosian_forge/game_forge/src/chess_game/color_mapper.py	2026-01-25 08:57:34.370540761 +1000
@@ -13,6 +13,7 @@
 from flashtext import KeywordProcessor  # type: ignore
 import colorspacious  # type: ignore
 import shutil
+from eidosian_core import eidosian
 
 # Configure logging - consider moving this to a central configuration if needed for broader application
 logging.basicConfig(
@@ -514,6 +515,7 @@
 
     @staticmethod
     def validate_text_input(func):
+        @eidosian()
         @wraps(func)
         def wrapper(instance, text: str, *args, **kwargs):
             if not isinstance(text, str) or not text.strip():
@@ -897,6 +899,7 @@
             int(final_rgb[2]),
         )  # Explicit 3-tuple
 
+    @eidosian()
     def batch_update_color_map(self, traits: List[str]) -> None:
         """Process multiple traits in batch with single save operation"""
         with ThreadPoolExecutor() as executor:
@@ -911,6 +914,7 @@
     def _get_trait_color(self, trait: str) -> Tuple[int, int, int]:
         return tuple(self.trait_color_map[trait]["color"])  # type: ignore
 
+    @eidosian()
     def add_personality_entry(
         self,
         name: str,
@@ -928,6 +932,7 @@
         }
         self._save_color_map()
 
+    @eidosian()
     def get_emotion_color(
         self, emotion: str, intensity: str = "base"
     ) -> Tuple[int, int, int]:
@@ -939,6 +944,7 @@
             )
         )
 
+    @eidosian()
     def find_related_concepts(self, trait: str) -> Dict[str, List]:
         """Find connections across the knowledge graph"""
         return {
@@ -955,6 +961,7 @@
         }
 
 
+@eidosian()
 def main():
     try:
         from PIL import ImageFont
@@ -988,11 +995,13 @@
             except json.JSONDecodeError:
                 processed_urls = set()  # Handle empty or corrupted JSON
 
+    @eidosian()
     def save_processed_url(url):
         processed_urls.add(url)
         with open(PROCESSED_LOG, "w") as f:
             json.dump(list(processed_urls), f, indent=2)
 
+    @eidosian()
     def search_searxng(query, num_results=5):  # Reduced num_results for demonstration
         from urllib.parse import urljoin
 
@@ -1015,6 +1024,7 @@
             print(f"Search error: {e}")
             return {"results": []}  # Return empty results to avoid further errors
 
+    @eidosian()
     def extract_content_with_tika(url):
         try:
             response = requests.get(
diff -ruN /tmp/user/1000/tmp.b0mW28NpTM/chess_game/local_model_network.py /home/lloyd/eidosian_forge/game_forge/src/chess_game/local_model_network.py
--- /tmp/user/1000/tmp.b0mW28NpTM/chess_game/local_model_network.py	2025-03-12 21:54:26.000000000 +1000
+++ /home/lloyd/eidosian_forge/game_forge/src/chess_game/local_model_network.py	2026-02-05 20:50:56.108996218 +1000
@@ -21,6 +21,7 @@
     AutoTokenizer,
     pipeline,
 )
+from eidosian_core import eidosian
 from accelerate import disk_offload  # type: ignore[import]
 from functools import lru_cache
 from concurrent.futures import ThreadPoolExecutor, as_completed
@@ -90,6 +91,7 @@
             ),
         }
 
+    @eidosian()
     def build_prompt(self, prompt_type, **kwargs):
         """Generate validated prompts with LangChain templates"""
         if prompt_type not in self.templates:
@@ -101,6 +103,7 @@
 # ------------------
 # CORE FUNCTIONALITY
 # ------------------
+@eidosian()
 @lru_cache(maxsize=1)
 def load_model(model_name):
     """
@@ -124,6 +127,7 @@
     return model
 
 
+@eidosian()
 @lru_cache(maxsize=1)
 def load_tokenizer(model_name):
     """
@@ -135,6 +139,7 @@
     )  # Tokenizer is loaded from the same model_name
 
 
+@eidosian()
 @lru_cache(maxsize=1)
 def load_text_generation_pipeline(model, tokenizer):
     """
@@ -162,12 +167,14 @@
     )
 
 
+@eidosian()
 def remove_think_tags(text):
     """Utility function to remove <think> and </think> tags."""
     return re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL).strip()
 
 
 # Add as utility function
+@eidosian()
 def sanitize_response(response: str) -> str:
     """Remove unsafe patterns from model responses"""
     patterns = [
@@ -196,6 +203,7 @@
         )
         self.device = "cuda" if torch.cuda.is_available() else "cpu"
 
+    @eidosian()
     def initialize_submodules(self):
         """
         Initialize submodules with proper async event loop handling
@@ -221,6 +229,7 @@
             loop.close()
 
     # Main workflow
+    @eidosian()
     def run(self, input_text):
         """
         Main method to run the orchestrator.
@@ -233,6 +242,7 @@
         update_queue.put("All tasks have been processed.")
         self.compile_results()
 
+    @eidosian()
     def break_into_topics(self, input_text: str) -> list:
         """Split input into focused topics using the language model"""
         prompt = f"""Break this input into focused discussion topics:
@@ -251,6 +261,7 @@
         except json.JSONDecodeError:
             return [input_text]  # Fallback to original input
 
+    @eidosian()
     def distribute_tasks(self, topics):
         """
         Distribute each topic to a submodule for processing.
@@ -260,6 +271,7 @@
             task_queue.put(task)
             update_queue.put(f"Task for topic '{topic}' has been queued.")
 
+    @eidosian()
     def compile_results(self):
         """Compile all results into a structured JSON output with history"""
         update_queue.put("Compiling all results into JSON output.")
@@ -301,6 +313,7 @@
         update_queue.put(f"JSON output saved to {OUTPUT_JSON_PATH}")
 
     # Result handling
+    @eidosian()
     def collect_results(self):
         """
         Collect results from submodules.
@@ -315,6 +328,7 @@
             except queue.Empty:
                 break
 
+    @eidosian()
     def send_updates(self):
         """
         Send frequent updates to the user about the progress of tasks.
@@ -439,6 +453,7 @@
             logging.error(f"Ollama failed: {e}")
             return ""
 
+    @eidosian()
     async def emotional_context(self, topic: str) -> dict:
         """
         Analyzes the emotional context of a given topic and returns a structured JSON.
@@ -539,6 +554,7 @@
         return data
 
     # Web/search functionality
+    @eidosian()
     async def perform_web_searches(self, web_queries):
         """
         Enhanced with LangChain document processing to perform web searches and store results.
@@ -640,6 +656,7 @@
         """Placeholder for filtering new documents - implement actual filtering logic"""
         return documents  # For now, return all documents without filtering
 
+    @eidosian()
     async def run(self):
         """
         Continuously processes tasks from the task queue until stopped.
@@ -831,6 +848,7 @@
             logging.error(f"JSON update failed: {str(e)}")
 
     # Core processing pipeline
+    @eidosian()
     async def identify_python_code(self, topic):
         """
         Identifies fundamental Python code for a given topic using a language model.
@@ -872,6 +890,7 @@
             )
             return ""
 
+    @eidosian()
     async def construct_web_queries(self, topic):
         """
         Constructs web queries to gather reference material for a given topic.
@@ -908,6 +927,7 @@
             )
             return []
 
+    @eidosian()
     async def analyze_query(self, topic):
         """
         Analyzes the given query to determine better approaches or validity.
@@ -938,6 +958,7 @@
             )
             return ""
 
+    @eidosian()
     async def compare_outputs(
         self, topic, python_code, web_queries, analysis, emotional_context
     ):
@@ -980,6 +1001,7 @@
             )
             return ""
 
+    @eidosian()
     async def fill_gaps(self, gaps):
         """
         Fills in the identified gaps in the query processing.
@@ -1008,6 +1030,7 @@
             logging.error(f"Error in fill_gaps: {e}", exc_info=True)
             return ""
 
+    @eidosian()
     async def summarize(
         self, topic, python_code, web_queries, analysis, emotional_context, filled_gaps
     ):
@@ -1050,6 +1073,7 @@
             logging.error(f"Error in summarize for topic '{topic}': {e}", exc_info=True)
             return ""
 
+    @eidosian()
     async def construct_code(self, topic):
         """
         Constructs the necessary Python code to solve the query based on summarized information.
@@ -1085,6 +1109,7 @@
             )
             return ""
 
+    @eidosian()
     async def final_summary(self, complete_outputs):
         """
         Summarizes all complete outputs to produce a final summary.
@@ -1113,6 +1138,7 @@
             logging.error(f"Error in final_summary: {e}", exc_info=True)
             return ""
 
+    @eidosian()
     def compile_all(self, final_summary, complete_outputs):
         """
         Compiles the final summary and detailed outputs into a formatted string.
@@ -1133,6 +1159,7 @@
         """Processes the query before searching (currently just returns the query)"""
         return query
 
+    @eidosian()
     def search_searxng(self, query, num_results=5):
         """
         Performs a SearxNG search against the configured SEARXNG_URL.
@@ -1157,6 +1184,7 @@
             print(f"Error during SearxNG search: {e}")
             return {"results": []}  # Return empty results in case of error
 
+    @eidosian()
     def scrape_and_save_content(self, search_results, query):
         """
         Scrapes content from SearxNG search results and saves it.
@@ -1175,6 +1203,7 @@
             urls
         )  # Directly use perform_web_searches to process URLs
 
+    @eidosian()
     def extract_content_with_tika(self, url):
         """
         Fetches content from a URL and extracts text and metadata using Tika.
@@ -1204,6 +1233,7 @@
         return tika_data
 
     # Output management
+    @eidosian()
     def provide_complete_outputs(
         self,
         python_code,
@@ -1245,6 +1275,7 @@
             "search_results": search_results,
         }
 
+    @eidosian()
     def save_output(self, topic, compiled_output):
         """
         Robust output saving with checksum validation to ensure data integrity.
@@ -1352,6 +1383,7 @@
             logging.warning(f"Ollama call failed: {str(e)}")
             return ""
 
+    @eidosian()
     async def extract_content(self, url: str) -> str:
         """Unified content extraction"""
         try:
@@ -1367,6 +1399,7 @@
                 else ""
             )
 
+    @eidosian()
     async def score_relevance(self, content: str, query: str) -> float:
         """Score content relevance using local model"""
         prompt = f"Rate relevance (0-1) between:\nQ: {query}\nC: {content[:2000]}"
diff -ruN /tmp/user/1000/tmp.b0mW28NpTM/chess_game/modular_local_model_network.py /home/lloyd/eidosian_forge/game_forge/src/chess_game/modular_local_model_network.py
--- /tmp/user/1000/tmp.b0mW28NpTM/chess_game/modular_local_model_network.py	2025-03-12 21:54:26.000000000 +1000
+++ /home/lloyd/eidosian_forge/game_forge/src/chess_game/modular_local_model_network.py	2026-02-08 05:41:38.543789979 +1000
@@ -10,6 +10,7 @@
     AutoTokenizer,
     pipeline,
 )
+from eidosian_core import eidosian
 from accelerate import disk_offload  # type: ignore[import]
 from typing import (
     Any,
@@ -56,9 +57,6 @@
 import json
 import hashlib
 import requests
-from tika import (  # type: ignore[import]
-    parser,
-)  # Keeping this import as it might be used by tika library internally, even if not directly used here.
 from urllib.parse import urljoin, urlparse
 from bs4 import BeautifulSoup  # type: ignore[import]
 import logging
@@ -767,24 +765,28 @@
     and releasing models back to the pool.
     """
 
+    @eidosian()
     def get_adaptive_model(self, model_type: str) -> Any:
         """
         Retrieves an adaptive model instance based on the specified type.
         """
         raise NotImplementedError("get_adaptive_model method must be implemented.")
 
+    @eidosian()
     def preload_models(self, model_types: List[str]) -> None:
         """
         Preloads models of specified types into the pool.
         """
         raise NotImplementedError("preload_models method must be implemented.")
 
+    @eidosian()
     def release_model(self, model_type: str) -> None:
         """
         Releases a model back to the pool, making it available for other tasks.
         """
         raise NotImplementedError("release_model method must be implemented.")
 
+    @eidosian()
     def get_optimizer(self) -> ModelOptimizerInterface:
         """Get model optimization component"""
         raise NotImplementedError
@@ -802,6 +804,7 @@
     is intended to optimize vector retrieval and reduce computational overhead.
     """
 
+    @eidosian()
     def get_store(self) -> IVectorStore:
         """
         Retrieves the vector store instance.
@@ -828,6 +831,7 @@
     various types of metrics.
     """
 
+    @eidosian()
     def start_span(
         self,
         task_id: str,
@@ -852,6 +856,7 @@
         """
         ...
 
+    @eidosian()
     def end_span(self, span: Any, attributes: Optional[Dict[str, Any]] = None) -> None:
         """
         Ends a telemetry span.
@@ -1197,6 +1202,7 @@
     This interface promotes modularity and interchangeability of submodules within the system.
     """
 
+    @eidosian()
     async def process(self, task: "Task") -> Any:
         """
         Processes a given task.
@@ -1227,6 +1233,7 @@
     pieces of information from unstructured or semi-structured data within the context.
     """
 
+    @eidosian()
     async def process(self, context: Dict[str, Any]) -> Dict[str, Any]:
         """
         Processes the context to extract content.
@@ -1253,6 +1260,7 @@
     security vulnerability detection, or code complexity assessment.
     """
 
+    @eidosian()
     async def process(self, context: Dict[str, Any]) -> Dict[str, Any]:
         """
         Processes the context to analyze code snippets.
@@ -1279,6 +1287,7 @@
     such as sentiment analysis, emotion recognition, or tone detection.
     """
 
+    @eidosian()
     async def process(self, context: Dict[str, Any]) -> Dict[str, Any]:
         """
         Processes the context to analyze emotional tone.
@@ -1305,6 +1314,7 @@
     informative summaries, preserving the key information and main points.
     """
 
+    @eidosian()
     async def process(self, context: Dict[str, Any]) -> Dict[str, Any]:
         """
         Processes the context to generate summaries.
@@ -1331,6 +1341,7 @@
     discrepancies in data, knowledge, or processes within the given context.
     """
 
+    @eidosian()
     async def process(self, context: Dict[str, Any]) -> Dict[str, Any]:
         """
         Processes the context to analyze gaps or discrepancies.
@@ -1361,6 +1372,7 @@
     system resilience and graceful degradation in the face of unexpected issues.
     """
 
+    @eidosian()
     async def handle_error(
         self,
         task_id: str,
@@ -1391,6 +1403,7 @@
         """
         ...
 
+    @eidosian()
     def log_error(
         self,
         error: Exception,
@@ -1480,6 +1493,7 @@
     of service. Fallback handlers are invoked when errors occur or when primary systems are down.
     """
 
+    @eidosian()
     async def handle_fallback(
         self, task_id: str, context: Dict[str, Any], strategy_name: Optional[str] = None
     ) -> Any:
@@ -1518,6 +1532,7 @@
     and improve system capabilities.
     """
 
+    @eidosian()
     async def find_optimal_config(
         self, task: "Task", context: Dict[str, Any]
     ) -> Dict[str, Any]:
@@ -1860,6 +1875,7 @@
                 f"Failed to load config from {self.config_path}"
             ) from e
 
+    @eidosian()
     def get_config(self) -> Dict[str, Any]:
         """
         Returns the current configuration dictionary.
@@ -1869,6 +1885,7 @@
         """
         return self.config
 
+    @eidosian()
     def set_config(self, new_config: Dict[str, Any]) -> None:
         """
         Sets a new configuration and notifies all registered listeners.
@@ -1885,6 +1902,7 @@
         self.config = new_config
         self._notify_listeners()
 
+    @eidosian()
     def add_listener(self, listener: Callable[[Dict[str, Any]], None]) -> None:
         """
         Adds a listener function to be notified when the configuration changes.
@@ -1913,6 +1931,7 @@
             except Exception as e:
                 logging.error(f"Error notifying listener {listener}: {e}")
 
+    @eidosian()
     def hot_reload_config(self) -> None:
         """
         Reloads the configuration from the file and notifies listeners.
@@ -1972,6 +1991,7 @@
         self.feature_flag_prefix = feature_flag_prefix
         """Prefix used to identify feature flags in the configuration."""
 
+    @eidosian()
     def is_enabled(self, flag_name: str) -> bool:
         """
         Checks if a feature flag is enabled.
@@ -2132,6 +2152,7 @@
                         f"Error instantiating plugin {obj.__name__} from module {module.__name__}: {e}"
                     )
 
+    @eidosian()
     def load_plugin(self, plugin_path: str) -> None:
         """
         Loads a single plugin from a given path and registers it.
@@ -2155,6 +2176,7 @@
         except Exception as e:
             logging.error(f"Error loading single plugin from {plugin_path}: {e}")
 
+    @eidosian()
     def register_plugins(self) -> None:
         """
         Registers hooks for all loaded plugins.
@@ -2194,6 +2216,7 @@
         self.singletons: Dict[Type, Any] = {}
         """Dictionary storing singleton instances, keyed by interface type."""
 
+    @eidosian()
     def register_singleton(self, interface: Type, implementation: Type) -> None:
         """
         Registers a dependency as a singleton.
@@ -2223,6 +2246,7 @@
         self.dependencies[interface] = implementation
         self.singletons[interface] = None  # Initialize singleton instance to None
 
+    @eidosian()
     def register_transient(self, interface: Type, implementation: Type) -> None:
         """
         Registers a dependency as transient.
@@ -2272,6 +2296,7 @@
                 f"Implementation must be a type, got {type(implementation)}"
             )
 
+    @eidosian()
     def resolve(self, interface: Type) -> Any:
         """
         Resolves a dependency and returns an instance of the implementation.
@@ -2359,6 +2384,7 @@
         """
         self._state: Dict[str, Any] = {}
 
+    @eidosian()
     def load_state(self) -> Dict[str, Any]:
         """
         Loads state data from in-memory storage.
@@ -2371,6 +2397,7 @@
         """
         return self._state
 
+    @eidosian()
     def save_state(
         self, state_data: Dict[str, Any], task_id: Optional[str] = None
     ) -> None:
@@ -2428,6 +2455,7 @@
         self._cluster_state: Dict[str, Any] = loaded_state.get("cluster_state", {})
         self._node_state: Dict[str, Any] = loaded_state.get("node_state", {})
 
+    @eidosian()
     def get_cluster_state(self) -> Dict[str, Any]:
         """
         Retrieves the current cluster state.
@@ -2441,6 +2469,7 @@
         """
         return self._cluster_state
 
+    @eidosian()
     def update_cluster_state(self, update_data: Dict[str, Any]) -> None:
         """
         Updates the cluster state and persists it using the backend.
@@ -2455,6 +2484,7 @@
         self._cluster_state.update(update_data)
         self._persist_state()
 
+    @eidosian()
     def get_node_state(self, node_id: str) -> Dict[str, Any]:
         """
         Retrieves the state for a specific node.
@@ -2471,6 +2501,7 @@
         """
         return self._node_state.get(node_id, {})
 
+    @eidosian()
     def update_node_state(self, node_id: str, update_data: Dict[str, Any]) -> None:
         """
         Updates the state for a specific node and persists it using the backend.
@@ -2541,6 +2572,7 @@
         """Initializes the InMemoryMetricsBackend with an empty metrics store."""
         self.metrics: Dict[str, Any] = {}
 
+    @eidosian()
     def create_counter(
         self,
         name: str,
@@ -2566,6 +2598,7 @@
         self.metrics[name] = {"type": "counter", "value": 0}
         return self.metrics[name]
 
+    @eidosian()
     def create_gauge(
         self,
         name: str,
@@ -2591,6 +2624,7 @@
         self.metrics[name] = {"type": "gauge", "value": 0.0}
         return self.metrics[name]
 
+    @eidosian()
     def create_histogram(
         self,
         name: str,
@@ -2628,6 +2662,7 @@
         }
         return self.metrics[name]
 
+    @eidosian()
     def increment_counter(
         self, name: str, value: int = 1, labels: Optional[Dict[str, str]] = None
     ) -> None:
@@ -2649,6 +2684,7 @@
             raise ValueError(f"Metric '{name}' is not a counter.")
         metric_data["value"] += value
 
+    @eidosian()
     def set_gauge(
         self, name: str, value: float, labels: Optional[Dict[str, str]] = None
     ) -> None:
@@ -2670,6 +2706,7 @@
             raise ValueError(f"Metric '{name}' is not a gauge.")
         metric_data["value"] = value
 
+    @eidosian()
     def observe_histogram(
         self, name: str, value: float, labels: Optional[Dict[str, str]] = None
     ) -> None:
@@ -2735,6 +2772,7 @@
         """Backend for storing and exporting metrics data."""
         self.metrics: Dict[str, Any] = {}  # Metric name to metric object mapping
 
+    @eidosian()
     def create_metric(
         self,
         name: str,
@@ -2779,6 +2817,7 @@
         self.metrics[name] = metric
         return metric  # Return the created metric object
 
+    @eidosian()
     def increment_counter(
         self, name: str, value: int = 1, labels: Optional[Dict[str, str]] = None
     ) -> None:
@@ -2797,6 +2836,7 @@
         """
         self.metrics_backend.increment_counter(name, value, labels)
 
+    @eidosian()
     def set_gauge(
         self, name: str, value: float, labels: Optional[Dict[str, str]] = None
     ) -> None:
@@ -2815,6 +2855,7 @@
         """
         self.metrics_backend.set_gauge(name, value, labels)
 
+    @eidosian()
     def observe_histogram(
         self, name: str, value: float, labels: Optional[Dict[str, str]] = None
     ) -> None:
@@ -2875,6 +2916,7 @@
         self._is_baseline_established = False
         self.anomaly_thresholds = anomaly_thresholds or {}
 
+    @eidosian()
     def start_monitoring(self, interval: int = 5) -> None:
         """
         Starts resource monitoring with a specified interval and establishes baselines.
@@ -2891,6 +2933,7 @@
         # Baselines are established upon the first call to get_usage or monitor_resources
         # or explicitly calling establish_baselines if needed immediately after start_monitoring.
 
+    @eidosian()
     async def establish_baselines(self) -> None:
         """
         Asynchronously measures and establishes initial system resource baselines.
@@ -2993,6 +3036,7 @@
             logging.error(f"Error getting GPU utilization: {e}")
             return 0.0
 
+    @eidosian()
     async def monitor_resources(self) -> Dict[str, float]:
         """
         Asynchronously monitors current system resources and compares them against established baselines.
@@ -3034,6 +3078,7 @@
                         f"Resource anomaly detected: {resource_name} deviation from baseline is {deviation:.2f}%, exceeding threshold {threshold:.2f}%."
                     )
 
+    @eidosian()
     async def get_usage(self) -> Dict[str, float]:  # Keep async implementation
         """
         Asynchronously retrieves the current resource usage.
@@ -3063,6 +3108,7 @@
         self.events: List[Dict[str, Any]] = []  # Initialize events storage
         self.logging = logging.getLogger(__name__)
 
+    @eidosian()
     def start_span(
         self,
         task_id: str,
@@ -3088,6 +3134,7 @@
         )
         return span
 
+    @eidosian()
     def end_span(
         self, span: "TelemetrySpan", attributes: Optional[Dict[str, Any]] = None
     ) -> None:
@@ -3109,6 +3156,7 @@
                 f"Attempted to end non-existent telemetry span for task: {span.task_id}"
             )
 
+    @eidosian()
     def record_metric(
         self, name: str, value: float, tags: Optional[Dict[str, str]] = None
     ) -> None:
@@ -3126,6 +3174,7 @@
         tags_str = f" with tags: {tags}" if tags else ""
         self.logging.debug(f"Metric recorded: {name}={value}{tags_str}")
 
+    @eidosian()
     def capture_event(
         self, event_name: str, properties: Optional[Dict[str, Any]] = None
     ) -> None:
@@ -3167,6 +3216,7 @@
             {}
         )  # Maps task IDs to active TelemetrySpan instances
 
+    @eidosian()
     def start_span(
         self,
         task_id: str,
@@ -3192,6 +3242,7 @@
         )
         return span
 
+    @eidosian()
     def end_span(
         self, span: "TelemetrySpan", attributes: Optional[Dict[str, Any]] = None
     ) -> None:
@@ -3213,6 +3264,7 @@
                 f"Attempted to end non-existent telemetry span for task: {span.task_id}"
             )
 
+    @eidosian()
     def record_metric(
         self, name: str, value: float, tags: Optional[Dict[str, str]] = None
     ) -> None:
@@ -3228,6 +3280,7 @@
         """
         self.telemetry_backend.record_metric(name, value, tags)
 
+    @eidosian()
     def capture_event(
         self, event_name: str, properties: Optional[Dict[str, Any]] = None
     ) -> None:
@@ -3342,6 +3395,7 @@
             self.output_path, exist_ok=True
         )  # Ensure the output directory exists
 
+    @eidosian()
     def start_profiling(self, context: Dict[str, Any]) -> cProfile.Profile:
         """
         Starts profiling using cProfile.
@@ -3356,6 +3410,7 @@
         logging.debug(f"Profiling started with context: {context}")
         return profile
 
+    @eidosian()
     def stop_profiling(
         self, profiler: cProfile.Profile, context: Dict[str, Any]
     ) -> Dict[str, Any]:
@@ -3388,6 +3443,7 @@
             sys.stdout = sys.__stdout__  # Restore standard output
         return {}
 
+    @eidosian()
     def profile_task(
         self, task: Callable[..., Any], task_id: str, sort_by: str = "cumulative"
     ) -> Any:
@@ -3443,6 +3499,7 @@
         self.logger = logger if logger is not None else logging.getLogger(__name__)
         self.logging_level = logging_level
 
+    @eidosian()
     def trace(
         self, task_id: str, message: str, attributes: Optional[Dict[str, Any]] = None
     ) -> None:
@@ -3491,6 +3548,7 @@
             tracing_backend or InMemoryTracingBackend()
         )
 
+    @eidosian()
     def trace(
         self,
         task_id: str,
@@ -3549,6 +3607,7 @@
         self.logging_level = logging_level
         self.logging = logging.getLogger(__name__)
 
+    @eidosian()
     def register_validator(
         self,
         validator_name: str,
@@ -3579,6 +3638,7 @@
             )
         self.validators[validator_name] = validator_func
 
+    @eidosian()
     def validate(
         self,
         output: Any,
@@ -3685,6 +3745,7 @@
         self.logging_level = logging_level
         self.logging = logging.getLogger(__name__)
 
+    @eidosian()
     def register_fallback_strategy(
         self, strategy_name: str, fallback_func: Callable[[str], Any]
     ) -> None:
@@ -3711,6 +3772,7 @@
             )
         self.fallback_strategies[strategy_name] = fallback_func
 
+    @eidosian()
     async def handle_fallback(
         self,
         task_id: str,
@@ -3759,6 +3821,7 @@
             )
             return {"status": "fallback_error", "error_message": str(e)}
 
+    @eidosian()
     def execute_fallback(
         self, task_id: str, strategy_name: Optional[str] = None
     ) -> Any:
@@ -3803,6 +3866,7 @@
         self.logging_level: int = logging_level
         self.logging = logging.getLogger(__name__)
 
+    @eidosian()
     async def handle_error(
         self,
         task_id: str,  # Interface-required parameter: Unique ID of the task
@@ -3922,6 +3986,7 @@
 
         return error_info  # Return error info if no fallback handler or fallback not executed
 
+    @eidosian()
     def log_error(
         self,
         error: Exception,
@@ -4006,6 +4071,7 @@
         self.format_string = format_string
         self.logging = logging.getLogger(__name__)
 
+    @eidosian()
     def format_message(
         self, message: str, context: Optional[Dict[str, Any]] = None
     ) -> str:
@@ -4096,6 +4162,7 @@
         self.enable_keyword_lookup = enable_keyword_lookup
         self.logging = logging.getLogger(__name__)
 
+    @eidosian()
     def color_message(self, message: str, severity: str = "debug") -> str:
         """
         Colours a message based on keywords found in the message and the configured color map.
@@ -4199,6 +4266,7 @@
 
     ##### Logging Functionality
 
+    @eidosian()
     def log_message(self, message: str, level: int = logging.INFO) -> None:
         """
         Logs a message, applying formatting and coloring if configured.
@@ -4234,6 +4302,7 @@
 
     ##### Resource Monitoring Functionality
 
+    @eidosian()
     async def monitor_resources(self) -> Dict[str, float]:
         """
         Asynchronously monitors system resources.
@@ -4250,6 +4319,7 @@
 
     ##### Profiling Functionality
 
+    @eidosian()
     def profile_task(
         self,
         task: Callable[..., Any],
@@ -4294,6 +4364,7 @@
 
     ##### Tracing Functionality
 
+    @eidosian()
     def trace_event(
         self,
         task_id: str,
@@ -4324,6 +4395,7 @@
 
     ##### Telemetry Functionality
 
+    @eidosian()
     def start_telemetry_span(
         self,
         task_id: str,
@@ -4362,6 +4434,7 @@
             operation_name, task_id, attributes=attributes
         )
 
+    @eidosian()
     def end_telemetry_span(
         self,
         task_id: str,
@@ -4395,6 +4468,7 @@
 
     ##### Error Handling Functionality
 
+    @eidosian()
     async def handle_error(
         self,
         task_id: str,
@@ -4485,6 +4559,7 @@
 
     ##### LoggingAndMonitoring Methods
 
+    @eidosian()
     def log_message(self, message: str, level: int = logging.INFO) -> None:
         """
         Logs a message using the integrated :class:`LoggingAndMonitoring` component.
@@ -4493,6 +4568,7 @@
         """
         self.logging_monitoring.log_message(message, level)
 
+    @eidosian()
     async def monitor_resources(self) -> Dict[str, float]:
         """
         Monitors system resources using the integrated :class:`LoggingAndMonitoring` component.
@@ -4501,6 +4577,7 @@
         """
         return await self.logging_monitoring.monitor_resources()
 
+    @eidosian()
     def profile_task(
         self,
         task: Callable[..., Any],
@@ -4514,6 +4591,7 @@
         """
         return self.logging_monitoring.profile_task(task, context, sort_by=sort_by)
 
+    @eidosian()
     def trace_event(self, task_id: str, message: str) -> None:
         """
         Traces an event using the integrated :class:`LoggingAndMonitoring` component.
@@ -4522,6 +4600,7 @@
         """
         self.logging_monitoring.trace_event(task_id, message)
 
+    @eidosian()
     def start_telemetry_span(
         self, task_id: str, operation_name: str
     ) -> "TelemetrySpan":
@@ -4532,6 +4611,7 @@
         """
         return self.logging_monitoring.start_telemetry_span(operation_name, task_id)
 
+    @eidosian()
     def end_telemetry_span(self, task_id: str) -> None:
         """
         Ends a telemetry span using the integrated :class:`LoggingAndMonitoring` component.
@@ -4540,6 +4620,7 @@
         """
         self.logging_monitoring.end_telemetry_span(task_id)
 
+    @eidosian()
     async def handle_error(
         self,
         task_id: str,
@@ -4559,6 +4640,7 @@
 
     ##### MetricsRegistry Methods
 
+    @eidosian()
     def create_metric(
         self,
         name: str,
@@ -4575,6 +4657,7 @@
             name, metric_type, description=description, labels=labels
         )
 
+    @eidosian()
     def increment_counter(
         self, name: str, value: int = 1, labels: Optional[Dict[str, str]] = None
     ) -> None:
@@ -4585,6 +4668,7 @@
         """
         self.metrics_registry.increment_counter(name, value, labels=labels)
 
+    @eidosian()
     def set_gauge(
         self, name: str, value: float, labels: Optional[Dict[str, str]] = None
     ) -> None:
@@ -4595,6 +4679,7 @@
         """
         self.metrics_registry.set_gauge(name, value, labels=labels)
 
+    @eidosian()
     def observe_histogram(
         self, name: str, value: float, labels: Optional[Dict[str, str]] = None
     ) -> None:
@@ -4645,6 +4730,7 @@
         )
         self.default_model_type = default_model_type
 
+    @eidosian()
     def load_model_for_type(self, model_type: str) -> Any:
         """
         Loads a model for a specific model type.
@@ -4691,6 +4777,7 @@
             # Placeholder for default model loading logic
             return f"default-model-{model_type}"  # Placeholder return value
 
+    @eidosian()
     def load_optimal_model(self, current_mem: int, gpu_mem: int) -> Any:
         """
         Selects and loads the optimal model based on available system resources.
@@ -4757,6 +4844,7 @@
         else:
             self.model_loader = ModelLoader(**loader_kwargs)
 
+    @eidosian()
     def get_loader(self) -> ModelLoader:
         """
         Retrieves the ModelLoader instance managed by this factory.
@@ -4766,6 +4854,7 @@
         """
         return self.model_loader
 
+    @eidosian()
     def load_model_for_type(self, model_type: str) -> Any:
         """
         Loads a model for a specific type using the associated ModelLoader.
@@ -4780,6 +4869,7 @@
         """
         return self.model_loader.load_model_for_type(model_type)
 
+    @eidosian()
     def load_optimal_model(self, current_mem: int, gpu_mem: int) -> Any:
         """
         Loads the optimal model based on available resources using the associated ModelLoader.
@@ -4796,6 +4886,7 @@
         """
         return self.model_loader.load_optimal_model(current_mem, gpu_mem)
 
+    @eidosian()
     def preload_models(self, model_types: List[str]) -> None:
         """
         Preloads models for specified module types using the associated ModelLoader.
@@ -4832,6 +4923,7 @@
         self.model_versions: Dict[str, Dict[str, Any]] = {}
         print("ModelVersionManager initialized.")
 
+    @eidosian()
     def register_version(self, model_type: str, version: str, model: Any) -> None:
         """
         Registers a specific version of a model for a given model type.
@@ -4854,6 +4946,7 @@
         self.model_versions[model_type][version] = model
         print(f"Registered version '{version}' for model type '{model_type}'.")
 
+    @eidosian()
     def get_version(self, model_type: str, version: str) -> Optional[Any]:
         """
         Retrieves a specific version of a model for a given model type.
@@ -4877,6 +4970,7 @@
             print(f"Version '{version}' not found for model type '{model_type}'.")
             return None
 
+    @eidosian()
     def get_latest_version(self, model_type: str) -> Optional[Any]:
         """
         Retrieves the latest registered version of a model for a given model type.
@@ -4944,6 +5038,7 @@
             f"ModelServingLayer initialized with ModelFactory, caching up to {max_cached_models} models."
         )
 
+    @eidosian()
     def get_adaptive_model(self) -> Any:
         """
         Retrieves the optimal model based on current system resources, using caching.
@@ -5057,6 +5152,7 @@
         """
         self.tasks: Dict[str, Task] = {}
 
+    @eidosian()
     def create_task(self, task_payload: Dict[str, Any]) -> Task:
         """
         Creates a new task and registers it with the TaskManager.
@@ -5078,6 +5174,7 @@
         self.tasks[task_id] = task
         return task
 
+    @eidosian()
     def get_task(self, task_id: str) -> "Task":
         """
         Retrieves a task from the TaskManager by its unique ID.
@@ -5096,6 +5193,7 @@
             raise KeyError(f"Task with id '{task_id}' not found.")
         return task
 
+    @eidosian()
     def update_task_status(self, task_id: str, status: str) -> None:
         """
         Updates the status of a task.
@@ -5115,6 +5213,7 @@
         task = self.get_task(task_id)  # Will raise KeyError if task not found
         task.status = status
 
+    @eidosian()
     def complete_task(self, task_id: str, result: Any) -> None:
         """
         Marks a task as completed and stores its result.
@@ -5171,6 +5270,7 @@
         """
         return bool(task.task_type)
 
+    @eidosian()
     def validate_task(self, task: "Task") -> bool:
         """
         Validates the given task against all configured validation criteria.
@@ -5220,6 +5320,7 @@
                 "Ensure to configure them for actual quality assurance."
             )
 
+    @eidosian()
     def ensure_quality(self, task: "Task") -> bool:
         """
         Executes quality assurance processes for the given task.
@@ -5277,6 +5378,7 @@
                 "Verification will always pass by default."
             )
 
+    @eidosian()
     def verify_quality(self, task: "Task") -> bool:
         """
         Verifies the quality of the processed task using configured verification criteria.
@@ -5334,6 +5436,7 @@
                 "Validation will always pass by default."
             )
 
+    @eidosian()
     def validate_quality(self, task: "Task") -> bool:
         """
         Validates the quality of the processed task against high-level validation criteria.
@@ -5394,6 +5497,7 @@
         )
         self._stage = ContentExtractorStage()  # Instantiate the stage class
 
+    @eidosian()
     async def process(self, task: Task) -> Task:
         """
         Processes the task to extract content from its context.
@@ -5450,6 +5554,7 @@
         )
         self._stage = CodeAnalyzerStage()  # Instantiate the stage class
 
+    @eidosian()
     async def process(self, task: Task) -> Task:
         """
         Processes the task to analyze code snippets within its context.
@@ -5506,6 +5611,7 @@
         )
         self._stage = EmotionalAnalyzerStage()  # Instantiate the stage class
 
+    @eidosian()
     async def process(self, task: Task) -> Task:
         """
         Processes the task to analyze emotional tone within its context.
@@ -5564,6 +5670,7 @@
         )
         self._stage = GapAnalyzerStage()  # Instantiate the stage class
 
+    @eidosian()
     async def process(self, task: Task) -> Task:
         """
         Processes the task to analyze gaps or discrepancies within its context.
@@ -5620,6 +5727,7 @@
         )
         self._stage = SummaryGeneratorStage()  # Instantiate the stage class
 
+    @eidosian()
     async def process(self, task: Task) -> Task:
         """
         Processes the task to generate summaries from its context.
@@ -5717,6 +5825,7 @@
         )
         logging.info("SubModuleAllocator initialized.")
 
+    @eidosian()
     def register_submodule_type(
         self, module_type: str, submodule_class: Type["BaseSubModule"]
     ) -> None:
@@ -5741,6 +5850,7 @@
         self.submodule_classes[module_type] = submodule_class
         logging.info(f"Submodule type '{module_type}' registered.")
 
+    @eidosian()
     def create_submodule(self, module_type: str) -> "BaseSubModule":
         """
         Creates and registers a submodule of the specified type.
@@ -5822,6 +5932,7 @@
             f"BaseSubModule initialized by allocator: {allocator.__class__.__name__}"
         )
 
+    @eidosian()
     async def process(self, task: Task) -> Any:
         """
         Processes a task through the submodule's lifecycle.
@@ -6029,6 +6140,7 @@
         )
         self._stage = OptimizationStage()  # Instantiate the stage class
 
+    @eidosian()
     async def tune_parameters(self, task: Task) -> Any:
         """
         Tunes model parameters for optimal performance using benchmarking.
@@ -6153,6 +6265,7 @@
 class RoutingRuleBook:
     """Rule book for routing tasks to modules."""
 
+    @eidosian()
     def select_module(self, task: "Task") -> str:
         """Selects a module type based on the task type."""
         task_type = task.task_type
@@ -6181,6 +6294,7 @@
         )  # Gate for quality checks on incoming tasks (defined elsewhere)
         self.task_manager = TaskManager()  # Manages tasks lifecycle (defined elsewhere)
 
+    @eidosian()
     def distribute_task(
         self, task_payload: dict, load_balancer: "LoadBalancer"
     ):  # Accepts task payload as dict and LoadBalancer instance
@@ -6226,6 +6340,7 @@
         )
         self.resource_monitor = ResourceMonitor()
 
+    @eidosian()
     async def initialize(self):
         """Cold start initialization with resource pre-warming.
         Establishes resource baselines and preloads models into the ModelPool.
@@ -6235,6 +6350,7 @@
             ["research", "codegen", "analysis"]
         )  # Preload models for common module types
 
+    @eidosian()
     def create_submodule_for_task(self, target_module: str):
         """Creates submodule instance using SubModuleAllocator based on resource availability.
         Args:
@@ -6269,6 +6385,7 @@
             {}
         )  # Dictionary to store pipeline templates, e.g., by task type
 
+    @eidosian()
     def register_template(self, template_id: str, template_config: Dict[str, list]):
         """Registers a new pipeline template.
         Args:
@@ -6278,6 +6395,7 @@
         """
         self.templates[template_id] = template_config
 
+    @eidosian()
     def get_template(self, template_id: str) -> Dict[str, list]:
         """Retrieves a pipeline template by its ID.
         Args:
@@ -6287,6 +6405,7 @@
         """
         return self.templates.get(template_id)
 
+    @eidosian()
     def has_template(self, template_id: str) -> bool:
         """Checks if a template with the given ID exists.
         Args:
@@ -6320,6 +6439,7 @@
             }
         )
 
+    @eidosian()
     def build_pipeline(self, template_id: str) -> List[Any]:
         """Constructs a pipeline (list of stages) from a template.
         Args:
@@ -6354,6 +6474,7 @@
     Optimizes a given pipeline based on runtime conditions and task characteristics.
     """
 
+    @eidosian()
     def optimize_pipeline(self, pipeline: List[Any], task: "Task") -> List[Any]:
         """Optimizes the pipeline based on task and system conditions.
         Args:
@@ -6374,6 +6495,7 @@
             "OptimizationStagePlaceholder": 1,
         }  # Higher number means higher priority (executed later)
 
+        @eidosian()
         def get_stage_order(stage_instance):
             stage_name = stage_instance.__class__.__name__
             return stage_priority.get(stage_name, 0)  # Default priority 0 if not found
@@ -6435,6 +6557,7 @@
             {"stages": ["OptimizationStagePlaceholder"]},
         )
 
+    @eidosian()
     def route_task(self, task: "Task") -> List[Any]:
         """Routes the task to a pipeline based on task type and routing rules.
         Returns:
@@ -6461,6 +6584,7 @@
 
         return optimized_pipeline
 
+    @eidosian()
     def get_system_load(self):
         """Simulated system load (replace with actual metrics).
         Placeholder for a method to retrieve or calculate the current system load.
@@ -6473,6 +6597,7 @@
 class TaskPipeline(TaskPipelineInterface):
     """Executes a dynamically built pipeline for task processing."""
 
+    @eidosian()
     async def execute(self, pipeline: List[Any], task: "Task") -> Dict[str, Any]:
         """Executes a given pipeline (list of stages) sequentially on the task context.
         Args:
@@ -6486,6 +6611,7 @@
             context = await stage.process(context)
         return context
 
+    @eidosian()
     async def process(self, task: "Task") -> "Task":
         """Processes a task using a dynamically determined and executed pipeline.
         Args:
@@ -6512,6 +6638,7 @@
     Manages vector database operations, including storing and retrieving vector embeddings.
     """
 
+    @eidosian()
     def store(self, documents):
         """Incorporates FAISS logic from original _store_documents.
         Placeholder for storing documents and their vector embeddings in the vector database.
@@ -6531,6 +6658,7 @@
     Provides access to a VectorDBManager instance, potentially using caching mechanisms.
     """
 
+    @eidosian()
     def get_store(self):
         """Gets a VectorDBManager instance, potentially from a cache.
         For now, it returns a new instance each time. Caching logic can be added here.
@@ -6548,6 +6676,7 @@
 class GraphManager:
     """Knowledge graph operations"""
 
+    @eidosian()
     def create_entity(self):
         pass
 
@@ -6555,6 +6684,7 @@
 class ExplainabilityEngine:
     """Model decision explanations"""
 
+    @eidosian()
     def generate_shap_values(self):
         pass
 
@@ -6690,6 +6820,7 @@
         except Exception as e:
             logging.error(f"Error crawling {start_url}: {e}")
 
+    @eidosian()
     async def execute_search(self, query):
         """
         Executes a web search for the given query using SearxNG,
@@ -6749,6 +6880,7 @@
         )
         self.hf_optimizer_name = "adamw_hf_name"  # Example name
 
+    @eidosian()
     def get_adaptive_model(self, model_type: str = "qwen") -> Any:
         """
         Retrieves a model instance from the pool. If the model_type is not found
@@ -6765,6 +6897,7 @@
         logging.info(f"Serving model of type: {model_type} from ModelPool.")
         return self.models[model_type]
 
+    @eidosian()
     def get_optimizer(self, optimizer_type: str = "adamw") -> Any:
         """
         Retrieves an optimizer model instance from the pool.
@@ -6842,6 +6975,7 @@
             logging.error(f"Error loading optimizer {optimizer_type}: {e}")
             raise
 
+    @eidosian()
     def preload_models(self, model_types: List[str]) -> None:
         """
         Preloads specified model types into the ModelPool. For each model type,
@@ -6874,6 +7008,7 @@
                     f"Model type '{model_type}' already loaded. Skipping preload."
                 )
 
+    @eidosian()
     def preload_optimizers(self, optimizer_types: List[str]) -> None:
         """
         Preloads specified optimizer types into the ModelPool.
@@ -6902,6 +7037,7 @@
                     f"Optimizer type '{optimizer_type}' already loaded. Skipping preload."
                 )
 
+    @eidosian()
     def release_model(self, model_type: str) -> None:
         """
         Releases a model type back to the pool, conceptually making it available for reuse.
@@ -6915,6 +7051,7 @@
         else:
             logging.warning(f"Attempted to release unknown model type: {model_type}")
 
+    @eidosian()
     def release_optimizer(self, optimizer_type: str) -> None:
         """
         Releases an optimizer type back to the pool.
@@ -6928,24 +7065,28 @@
                 f"Attempted to release unknown optimizer type: {optimizer_type}"
             )
 
+    @eidosian()
     def is_model_loaded(self, model_type: str) -> bool:
         """
         Checks if a model type is currently loaded in the pool.
         """
         return model_type in self.models
 
+    @eidosian()
     def is_optimizer_loaded(self, optimizer_type: str) -> bool:
         """
         Checks if an optimizer type is currently loaded in the pool.
         """
         return optimizer_type in self.optimizers
 
+    @eidosian()
     def get_available_model_types(self) -> List[str]:
         """
         Returns a list of currently available model types in the pool.
         """
         return list(self.models.keys())
 
+    @eidosian()
     def get_available_optimizer_types(self) -> List[str]:
         """
         Returns a list of currently available optimizer types in the pool.
@@ -6956,6 +7097,7 @@
 class QualityGateway:
     """Temporary quality gate implementation"""
 
+    @eidosian()
     def validate_task(
         self, task: "Task"
     ) -> (
@@ -6974,12 +7116,14 @@
         return True
 
 
+@eidosian()
 def main():
     import asyncio
     import logging
 
     logging.basicConfig(level=logging.INFO)
 
+    @eidosian()
     async def test_architecture():
         """Tests the modular local model network architecture with the updated ModelPool."""
         try:
diff -ruN /tmp/user/1000/tmp.b0mW28NpTM/chess_game/piecerenderer.py /home/lloyd/eidosian_forge/game_forge/src/chess_game/piecerenderer.py
--- /tmp/user/1000/tmp.b0mW28NpTM/chess_game/piecerenderer.py	2025-03-12 21:54:26.000000000 +1000
+++ /home/lloyd/eidosian_forge/game_forge/src/chess_game/piecerenderer.py	2026-02-05 20:51:11.360513480 +1000
@@ -13,6 +13,7 @@
     COLOR_TEXT,
     UNICODE_MAP,
 )  # Import game-specific constants from init.py.
+from eidosian_core import eidosian
 
 
 class PieceRenderer:
@@ -69,6 +70,7 @@
             f"PieceRenderer initialized with image size: {image_size}, font: {self.font}"
         )
 
+    @eidosian()
     def clear_cache(self):
         """
         Clears the piece surface cache.
@@ -84,6 +86,7 @@
             {}
         )  # Reset the cache to an empty dictionary, effectively clearing it.
 
+    @eidosian()
     def get_piece_surface(self, symbol: str) -> pygame.Surface:
         """
         Retrieves a Pygame Surface for the given chess piece symbol.
diff -ruN /tmp/user/1000/tmp.b0mW28NpTM/chess_game/prototype.py /home/lloyd/eidosian_forge/game_forge/src/chess_game/prototype.py
--- /tmp/user/1000/tmp.b0mW28NpTM/chess_game/prototype.py	2025-03-12 21:54:26.000000000 +1000
+++ /home/lloyd/eidosian_forge/game_forge/src/chess_game/prototype.py	2026-01-25 08:57:34.364777582 +1000
@@ -31,6 +31,7 @@
 from typing import Dict, List, Optional, Any, Tuple
 import threading
 import random
+from eidosian_core import eidosian
 
 # -------------------------------------------------------------------------------------
 # GLOBAL CONFIGURATION SECTION
@@ -193,6 +194,7 @@
         self.lock = threading.Lock()
         self.cache: Dict[int, pygame.Surface] = {}
 
+    @eidosian()
     def process_text(self, raw_text: str, personalities: Dict[str, Any]) -> None:
         """
         Runs text through emoji insertion, color coding, and line wrapping, then auto-scrolls.
@@ -289,12 +291,14 @@
         self.piece_cache: Dict[str, pygame.Surface] = {}
         self.image_size = SQUARE_SIZE
 
+    @eidosian()
     def clear_cache(self):
         """
         Clears the piece surface cache to force re-render or re-load images.
         """
         self.piece_cache = {}
 
+    @eidosian()
     def get_piece_surface(self, symbol: str) -> pygame.Surface:
         """
         Returns a surface for the given piece symbol, using cache, images, or Unicode fallback.
@@ -340,6 +344,7 @@
 # -------------------------------------------------------------------------------------
 
 
+@eidosian()
 def load_player_profiles() -> Dict[str, Dict[str, Any]]:
     """
     Loads player profiles from a JSON file, applying defaults for any missing fields.
@@ -363,6 +368,7 @@
         return {}
 
 
+@eidosian()
 def save_player_profiles(profiles: Dict[str, Dict[str, Any]]) -> None:
     """
     Saves player profiles to a JSON file using an atomic write strategy.
@@ -383,6 +389,7 @@
 # -------------------------------------------------------------------------------------
 
 
+@eidosian()
 def get_ollama_endpoint() -> Optional[str]:
     """
     Checks for an available Ollama endpoint, returning the first responsive one.
@@ -406,6 +413,7 @@
 OLLAMA_API_BASE_URL = get_ollama_endpoint()
 
 
+@eidosian()
 def generate_llm_story(
     board: chess.Board,
     personalities: Dict[str, Dict[str, Any]],
@@ -482,6 +490,7 @@
         )
 
 
+@eidosian()
 def warmup_llm_connection(screen: pygame.Surface):
     """
     Simple check to ensure LLM is reachable and loaded. Exits if no connection available.
@@ -577,6 +586,7 @@
 
         logging.info("PersonalityChessGame initialization complete.")
 
+    @eidosian()
     def draw_board(self) -> None:
         """
         Renders the chessboard squares and pieces, plus the story panel at the bottom.
@@ -664,6 +674,7 @@
                 x += surface.get_width() + 5
             y_start += line_height
 
+    @eidosian()
     def highlight_moves(self, moves: List[int]) -> None:
         """
         Highlights legal moves with a semi-transparent overlay.
@@ -679,6 +690,7 @@
             )
             self.screen.blit(highlight_surface, (col * SQUARE_SIZE, row * SQUARE_SIZE))
 
+    @eidosian()
     def get_square_under_mouse(self) -> Optional[int]:
         """
         Returns the chess square index (0-63) under the mouse cursor, or None if outside.
@@ -690,6 +702,7 @@
             return None
         return (7 - row) * 8 + col
 
+    @eidosian()
     def get_legal_moves_for_square(self, square: int) -> List[int]:
         """
         Returns a list of target squares (0-63) for legal moves from the given square.
@@ -700,6 +713,7 @@
                 moves.append(m.to_square)
         return moves
 
+    @eidosian()
     def handle_resize(self) -> None:
         """
         Adjusts UI elements and squares after a window resize event.
@@ -720,6 +734,7 @@
         self.selected_square = None
         logging.info("UI resized and adjusted.")
 
+    @eidosian()
     def main_loop(self) -> None:
         """
         Main game loop for handling events, rendering, and updating.
@@ -795,6 +810,7 @@
         logging.info("Pygame quit. Exiting main loop.")
         sys.exit()
 
+    @eidosian()
     def handle_click(self) -> None:
         """
         Manages piece selection and move attempts when the user clicks on the board.
@@ -813,6 +829,7 @@
         else:
             self.selected_square = clicked_square
 
+    @eidosian()
     def update_player_profile_stats(self, result: str) -> None:
         """
         Increments profile stats based on game result and player's preferred color.
@@ -834,6 +851,7 @@
 
         save_player_profiles(self.player_profiles)
 
+    @eidosian()
     def wrap_text(self, text: str, max_width: int) -> List[str]:
         """
         Wraps text into multiple lines to fit within max_width using the story font.
@@ -853,6 +871,7 @@
         lines.append(" ".join(current_line))
         return lines
 
+    @eidosian()
     def update_story(self, new_text: str) -> None:
         """
         Safely updates the story text and triggers asynchronous processing if changed.
@@ -902,6 +921,7 @@
 # -------------------------------------------------------------------------------------
 
 
+@eidosian()
 def main() -> None:
     """
     Initializes and runs the Personality Chess Game.
diff -ruN /tmp/user/1000/tmp.b0mW28NpTM/chess_game/prototype_old.py /home/lloyd/eidosian_forge/game_forge/src/chess_game/prototype_old.py
--- /tmp/user/1000/tmp.b0mW28NpTM/chess_game/prototype_old.py	2025-03-12 21:54:26.000000000 +1000
+++ /home/lloyd/eidosian_forge/game_forge/src/chess_game/prototype_old.py	2026-01-25 08:57:34.364293365 +1000
@@ -28,6 +28,7 @@
 import logging  # For detailed logging
 from typing import Dict, List, Optional, Any, Generator
 import threading
+from eidosian_core import eidosian
 
 # -------------------------------------------------------------------------------------
 # GLOBAL CONFIGURATION
@@ -196,6 +197,7 @@
         self.lock = threading.Lock()
         self.cache: dict[int, pygame.Surface] = {}
 
+    @eidosian()
     def process_text(self, raw_text: str, personalities: Dict[str, Any]) -> None:
         """Thread-safe text processing with emojis and colors"""
         processed = self._add_emojis(raw_text)
@@ -268,6 +270,7 @@
         self.image_size = image_size
         self.piece_cache: dict[str, pygame.Surface] = {}  # Cache loaded images
 
+    @eidosian()
     def get_piece_surface(self, symbol: str) -> pygame.Surface:
         """Get graphical representation for a piece symbol"""
         if symbol in self.piece_cache:
@@ -291,6 +294,7 @@
 # -------------------------------------------------------------------------------------
 
 
+@eidosian()
 def load_player_profiles() -> Dict[str, Dict[str, Any]]:
     """
     Load player profiles from a JSON file, with enhanced validation and default structure.
@@ -329,6 +333,7 @@
         return {}
 
 
+@eidosian()
 def save_player_profiles(profiles: Dict[str, Dict[str, Any]]) -> None:
     """
     Save player profiles to a JSON file using an atomic write pattern to prevent corruption.
@@ -359,6 +364,7 @@
 # -------------------------------------------------------------------------------------
 
 
+@eidosian()
 def get_ollama_endpoint():
     """Check available Ollama endpoints with timeout"""
     endpoints = ["http://192.168.4.73:11434"]
@@ -379,6 +385,7 @@
 OLLAMA_API_BASE_URL = get_ollama_endpoint()
 
 
+@eidosian()
 def generate_llm_story(
     board: chess.Board,
     personalities: Dict[str, Dict[str, Any]],
@@ -445,6 +452,7 @@
         yield "The battlefield clouds my mind..."
 
 
+@eidosian()
 def warmup_llm_connection():
     """Ensure LLM connection is working before game starts"""
     logging.info("Initializing LLM connection...")
@@ -550,6 +558,7 @@
         self.rendered_lines: list[list[dict]] = []
         self.last_processed_text = ""
 
+    @eidosian()
     def draw_board(self) -> None:
         """
         Draw the chess board squares and the pieces on the screen.
@@ -632,6 +641,7 @@
                 x += segment["width"]
             y_start += 20
 
+    @eidosian()
     def highlight_moves(self, moves: List[int]) -> None:
         """
         Highlight possible moves (list of squares) on the board.
@@ -658,6 +668,7 @@
             self.screen.blit(highlight_surface, (rect_x, rect_y))
         logging.debug("Moves highlighted.")
 
+    @eidosian()
     def get_square_under_mouse(self) -> Optional[int]:
         """
         Get the board square index (0..63 in python-chess coordinates) under the current mouse position.
@@ -685,6 +696,7 @@
         logging.debug(f"Square index under mouse: {square_idx}")
         return square_idx
 
+    @eidosian()
     def get_legal_moves_for_square(self, square: int) -> List[int]:
         """
         Return a list of target squares (integers) that the piece on 'square' can move to legally.
@@ -706,6 +718,7 @@
         logging.debug(f"Legal moves for square {square}: {moves}")
         return moves
 
+    @eidosian()
     def main_loop(self) -> None:
         """
         The main game loop that keeps the window open, handles events, draws the board, etc.
@@ -824,6 +837,7 @@
         logging.info("Pygame quit.")
         sys.exit()
 
+    @eidosian()
     def handle_click(self) -> None:
         """
         Handle the user clicking on the board.
@@ -858,6 +872,7 @@
             self.selected_square = clicked_square
             logging.debug(f"Selected square set to: {self.selected_square}")
 
+    @eidosian()
     def update_player_profile_stats(self, result: str) -> None:
         """
         Update the current player's stats based on the game result.
@@ -890,6 +905,7 @@
         save_player_profiles(self.player_profiles)
         logging.info("Player profile stats updated and saved.")
 
+    @eidosian()
     def wrap_text(self, text: str, max_width: int) -> List[str]:
         """Wrap text into multiple lines"""
         words = text.split()
@@ -906,6 +922,7 @@
         lines.append(" ".join(current_line))
         return lines
 
+    @eidosian()
     def update_story(self, new_text: str) -> None:
         """Thread-safe story text update with processing"""
         if new_text != self.last_processed_text:
@@ -946,6 +963,7 @@
 # -------------------------------------------------------------------------------------
 
 
+@eidosian()
 def main() -> None:
     """
     The main entry point that initializes Pygame, creates the PersonalityChessGame, and starts it.
diff -ruN /tmp/user/1000/tmp.b0mW28NpTM/chess_game/textprocessor.py /home/lloyd/eidosian_forge/game_forge/src/chess_game/textprocessor.py
--- /tmp/user/1000/tmp.b0mW28NpTM/chess_game/textprocessor.py	2025-03-12 21:54:26.000000000 +1000
+++ /home/lloyd/eidosian_forge/game_forge/src/chess_game/textprocessor.py	2026-01-25 08:57:34.370026739 +1000
@@ -5,6 +5,7 @@
 from typing import Dict, List, Any, Tuple, Optional
 
 from init import EMOJI_MAP, COLOR_MAP, PANEL_HEIGHT
+from eidosian_core import eidosian
 
 logging.basicConfig(
     level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
@@ -49,6 +50,7 @@
             raise TypeError("segments must contain TextSegment")
         self.segments: List[TextSegment] = segments if segments else []
 
+    @eidosian()
     def add_segment(self, segment: TextSegment) -> None:
         if not isinstance(segment, TextSegment):
             raise TypeError(f"segment must be a TextSegment, not {type(segment)}")
@@ -83,6 +85,7 @@
         self.space_width: int = self.font.size(" ")[0]
         logging.info("TextProcessor initialized.")
 
+    @eidosian()
     def process_text(
         self, raw_text: str, personalities: Optional[Dict[str, Any]] = None
     ) -> None:
@@ -124,6 +127,7 @@
         if not self.emoji_pattern:
             return text
 
+        @eidosian()
         def replace_emoji(match: re.Match) -> str:
             keyword = match.group(0).lower()
             if emoji := EMOJI_MAP.get(keyword):
@@ -148,6 +152,7 @@
         trait_priority = {trait: index for index, trait in enumerate(traits)}
         tokens = self._tokenize_text(text)
 
+        @eidosian()
         def get_segment_color(token: str) -> Tuple[int, int, int]:
             best_match_trait = None
             highest_priority = -1
@@ -293,6 +298,7 @@
         self.scroll_offset = max(0, text_height - PANEL_HEIGHT)
         logging.debug(f"Auto scroll adjusted. Scroll offset: {self.scroll_offset}")
 
+    @eidosian()
     def render_lines(self, surface: pygame.Surface, start_y: int) -> None:
         y_offset = start_y - self.scroll_offset
         try:
@@ -359,6 +365,7 @@
 
     text_processor = TextProcessor(max_width, font)
 
+    @eidosian()
     def apply_nuanced_coloring(text, personalities):
         """Applies nuanced coloring based on personalities and context."""
         if not personalities:
@@ -392,6 +399,7 @@
             colored_segments.append((word, color))
         return colored_segments
 
+    @eidosian()
     def run_test_case(test_name, raw_text, personalities=None, expected_output=None):
         screen.fill((255, 255, 255))
         print(f"\n--- {test_name} ---")
diff -ruN /tmp/user/1000/tmp.b0mW28NpTM/chess_game/usermanager.py /home/lloyd/eidosian_forge/game_forge/src/chess_game/usermanager.py
--- /tmp/user/1000/tmp.b0mW28NpTM/chess_game/usermanager.py	2025-03-12 21:54:26.000000000 +1000
+++ /home/lloyd/eidosian_forge/game_forge/src/chess_game/usermanager.py	2026-02-05 20:51:15.782663098 +1000
@@ -9,6 +9,7 @@
     Dict,
     Any,
 )  # For type hinting, improving code readability and maintainability.
+from eidosian_core import eidosian
 
 
 DEFAULT_PROFILE = {  # Define a default profile here for testing purposes if init.py is not accessible in test context
@@ -48,6 +49,7 @@
             f"UserManager initialized with profiles file: {self.profiles_file_path}"
         )
 
+    @eidosian()
     def load_profiles(self) -> Dict[str, Dict[str, Any]]:
         """
         Load player profiles from a JSON file, with enhanced validation and default structure.
@@ -130,6 +132,7 @@
             )
             return {}  # Return empty profiles in case of any unhandled exception.
 
+    @eidosian()
     def save_profiles(self, profiles: Dict[str, Dict[str, Any]]) -> None:
         """
         Save player profiles to a JSON file using an atomic write pattern to prevent data corruption.
