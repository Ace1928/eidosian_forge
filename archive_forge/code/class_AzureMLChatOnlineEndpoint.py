import json
import warnings
from typing import (
from langchain_core.callbacks import (
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import (
from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult
from langchain_community.llms.azureml_endpoint import (
class AzureMLChatOnlineEndpoint(BaseChatModel, AzureMLBaseEndpoint):
    """Azure ML Online Endpoint chat models.

    Example:
        .. code-block:: python
            azure_llm = AzureMLOnlineEndpoint(
                endpoint_url="https://<your-endpoint>.<your_region>.inference.ml.azure.com/v1/chat/completions",
                endpoint_api_type=AzureMLApiType.serverless,
                endpoint_api_key="my-api-key",
                content_formatter=chat_content_formatter,
            )
    """

    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """Get the identifying parameters."""
        _model_kwargs = self.model_kwargs or {}
        return {**{'model_kwargs': _model_kwargs}}

    @property
    def _llm_type(self) -> str:
        """Return type of llm."""
        return 'azureml_chat_endpoint'

    def _generate(self, messages: List[BaseMessage], stop: Optional[List[str]]=None, run_manager: Optional[CallbackManagerForLLMRun]=None, **kwargs: Any) -> ChatResult:
        """Call out to an AzureML Managed Online endpoint.
        Args:
            messages: The messages in the conversation with the chat model.
            stop: Optional list of stop words to use when generating.
        Returns:
            The string generated by the model.
        Example:
            .. code-block:: python
                response = azureml_model("Tell me a joke.")
        """
        _model_kwargs = self.model_kwargs or {}
        _model_kwargs.update(kwargs)
        if stop:
            _model_kwargs['stop'] = stop
        request_payload = self.content_formatter.format_messages_request_payload(messages, _model_kwargs, self.endpoint_api_type)
        response_payload = self.http_client.call(body=request_payload, run_manager=run_manager)
        generations = self.content_formatter.format_response_payload(response_payload, self.endpoint_api_type)
        return ChatResult(generations=[generations])

    def _stream(self, messages: List[BaseMessage], stop: Optional[List[str]]=None, run_manager: Optional[CallbackManagerForLLMRun]=None, **kwargs: Any) -> Iterator[ChatGenerationChunk]:
        self.endpoint_url = self.endpoint_url.replace('/chat/completions', '')
        timeout = None if 'timeout' not in kwargs else kwargs['timeout']
        import openai
        params = {}
        client_params = {'api_key': self.endpoint_api_key.get_secret_value(), 'base_url': self.endpoint_url, 'timeout': timeout, 'default_headers': None, 'default_query': None, 'http_client': None}
        client = openai.OpenAI(**client_params)
        message_dicts = [CustomOpenAIChatContentFormatter._convert_message_to_dict(m) for m in messages]
        params = {'stream': True, 'stop': stop, 'model': None, **kwargs}
        default_chunk_class = AIMessageChunk
        for chunk in client.chat.completions.create(messages=message_dicts, **params):
            if not isinstance(chunk, dict):
                chunk = chunk.dict()
            if len(chunk['choices']) == 0:
                continue
            choice = chunk['choices'][0]
            chunk = _convert_delta_to_message_chunk(choice['delta'], default_chunk_class)
            generation_info = {}
            if (finish_reason := choice.get('finish_reason')):
                generation_info['finish_reason'] = finish_reason
            logprobs = choice.get('logprobs')
            if logprobs:
                generation_info['logprobs'] = logprobs
            default_chunk_class = chunk.__class__
            chunk = ChatGenerationChunk(message=chunk, generation_info=generation_info or None)
            if run_manager:
                run_manager.on_llm_new_token(chunk.text, chunk=chunk, logprobs=logprobs)
            yield chunk

    async def _astream(self, messages: List[BaseMessage], stop: Optional[List[str]]=None, run_manager: Optional[AsyncCallbackManagerForLLMRun]=None, **kwargs: Any) -> AsyncIterator[ChatGenerationChunk]:
        self.endpoint_url = self.endpoint_url.replace('/chat/completions', '')
        timeout = None if 'timeout' not in kwargs else kwargs['timeout']
        import openai
        params = {}
        client_params = {'api_key': self.endpoint_api_key.get_secret_value(), 'base_url': self.endpoint_url, 'timeout': timeout, 'default_headers': None, 'default_query': None, 'http_client': None}
        async_client = openai.AsyncOpenAI(**client_params)
        message_dicts = [CustomOpenAIChatContentFormatter._convert_message_to_dict(m) for m in messages]
        params = {'stream': True, 'stop': stop, 'model': None, **kwargs}
        default_chunk_class = AIMessageChunk
        async for chunk in await async_client.chat.completions.create(messages=message_dicts, **params):
            if not isinstance(chunk, dict):
                chunk = chunk.dict()
            if len(chunk['choices']) == 0:
                continue
            choice = chunk['choices'][0]
            chunk = _convert_delta_to_message_chunk(choice['delta'], default_chunk_class)
            generation_info = {}
            if (finish_reason := choice.get('finish_reason')):
                generation_info['finish_reason'] = finish_reason
            logprobs = choice.get('logprobs')
            if logprobs:
                generation_info['logprobs'] = logprobs
            default_chunk_class = chunk.__class__
            chunk = ChatGenerationChunk(message=chunk, generation_info=generation_info or None)
            if run_manager:
                await run_manager.on_llm_new_token(token=chunk.text, chunk=chunk, logprobs=logprobs)
            yield chunk