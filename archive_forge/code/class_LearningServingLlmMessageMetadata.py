from __future__ import absolute_import
from apitools.base.protorpclite import messages as _messages
from apitools.base.py import encoding
from apitools.base.py import extra_types
class LearningServingLlmMessageMetadata(_messages.Message):
    """LINT.IfChange This metadata contains additional information required for
  debugging.

  Enums:
    FinishReasonValueValuesEnum: NOT YET IMPLEMENTED.

  Fields:
    classifierSummary: Summary of classifier output. We attach this to all
      messages regardless of whether classification rules triggered or not.
    codeyOutput: Contains metadata related to Codey Processors.
    currentStreamTextLength: A integer attribute.
    deleted: Whether the corresponding message has been deleted.
    filterMeta: Metadata for filters that triggered.
    finalMessageScore: This score is finally used for ranking the message.
      This will be same as the score present in `Message.score` field.
    finishReason: NOT YET IMPLEMENTED.
    groundingMetadata: A LearningGenaiRootGroundingMetadata attribute.
    isCode: Applies to streaming response message only. Whether the message is
      a code.
    isFallback: Applies to Response message only. Indicates whether the
      message is a fallback and the response would have otherwise been empty.
    langidResult: Result from nlp_saft DetectLanguage method. Currently the
      predicted language code and language probability is used.
    language: Detected language.
    lmPrefix: The LM prefix used to generate this response.
    originalText: The original text generated by LLM. This is the raw output
      for debugging purposes.
    perStreamDecodedTokenCount: NOT YET IMPLEMENTED. Applies to streaming
      only. Number of tokens decoded / emitted by the model as part of this
      stream. This may be different from token_count, which contains number of
      tokens returned in this response after any response rewriting /
      truncation.
    raiOutputs: Results of running RAI on the query or this response
      candidate. One output per rai_config. It will be populated regardless of
      whether the threshold is exceeded or not.
    recitationResult: Recitation Results. It will be populated as long as
      Recitation processing is enabled, regardless of recitation outcome.
    returnTokenCount: NOT YET IMPLEMENTED. Number of tokens returned as part
      of this candidate.
    scores: All the different scores for a message are logged here.
    streamTerminated: Whether the response is terminated during streaming
      return. Only used for streaming requests.
    totalDecodedTokenCount: NOT YET IMPLEMENTED. Aggregated number of total
      tokens decoded so far. For streaming, this is sum of all the tokens
      decoded so far i.e. aggregated count.
    translatedUserPrompts: Translated user-prompt used for RAI post
      processing. This is for internal processing only. We will translate in
      pre-processor and pass the translated text to the post processor using
      this field. It will be empty if non of the signals requested need
      translation.
    vertexRaiResult: The metadata from Vertex SafetyCat processors
  """

    class FinishReasonValueValuesEnum(_messages.Enum):
        """NOT YET IMPLEMENTED.

    Values:
      UNSPECIFIED: <no description>
      RETURN: Return all the tokens back. This typically implies no filtering
        or stop sequence was triggered.
      STOP: Finished due to provided stop sequence.
      MAX_TOKENS: Model has emitted the maximum number of tokens as specified
        by max_decoding_steps.
      FILTER: Finished due to triggering some post-processing filter.
      TOP_N_FILTERED: Filtered out due to Top_N < Response_Candidates.Size()
    """
        UNSPECIFIED = 0
        RETURN = 1
        STOP = 2
        MAX_TOKENS = 3
        FILTER = 4
        TOP_N_FILTERED = 5
    classifierSummary = _messages.MessageField('LearningGenaiRootClassifierOutputSummary', 1)
    codeyOutput = _messages.MessageField('LearningGenaiRootCodeyOutput', 2)
    currentStreamTextLength = _messages.IntegerField(3, variant=_messages.Variant.UINT32)
    deleted = _messages.BooleanField(4)
    filterMeta = _messages.MessageField('LearningGenaiRootFilterMetadata', 5, repeated=True)
    finalMessageScore = _messages.MessageField('LearningGenaiRootScore', 6)
    finishReason = _messages.EnumField('FinishReasonValueValuesEnum', 7)
    groundingMetadata = _messages.MessageField('LearningGenaiRootGroundingMetadata', 8)
    isCode = _messages.BooleanField(9)
    isFallback = _messages.BooleanField(10)
    langidResult = _messages.MessageField('NlpSaftLangIdResult', 11)
    language = _messages.StringField(12)
    lmPrefix = _messages.StringField(13)
    originalText = _messages.StringField(14)
    perStreamDecodedTokenCount = _messages.IntegerField(15, variant=_messages.Variant.INT32)
    raiOutputs = _messages.MessageField('LearningGenaiRootRAIOutput', 16, repeated=True)
    recitationResult = _messages.MessageField('LearningGenaiRecitationRecitationResult', 17)
    returnTokenCount = _messages.IntegerField(18, variant=_messages.Variant.INT32)
    scores = _messages.MessageField('LearningGenaiRootScore', 19, repeated=True)
    streamTerminated = _messages.BooleanField(20)
    totalDecodedTokenCount = _messages.IntegerField(21, variant=_messages.Variant.INT32)
    translatedUserPrompts = _messages.StringField(22, repeated=True)
    vertexRaiResult = _messages.MessageField('CloudAiNlLlmProtoServiceRaiResult', 23)