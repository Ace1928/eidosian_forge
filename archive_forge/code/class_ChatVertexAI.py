from __future__ import annotations
import base64
import logging
import re
from dataclasses import dataclass, field
from typing import TYPE_CHECKING, Any, Dict, Iterator, List, Optional, Union, cast
from urllib.parse import urlparse
import requests
from langchain_core._api.deprecation import deprecated
from langchain_core.callbacks import (
from langchain_core.language_models.chat_models import (
from langchain_core.messages import (
from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult
from langchain_core.pydantic_v1 import root_validator
from langchain_community.llms.vertexai import (
from langchain_community.utilities.vertexai import (
@deprecated(since='0.0.12', removal='0.2.0', alternative_import='langchain_google_vertexai.ChatVertexAI')
class ChatVertexAI(_VertexAICommon, BaseChatModel):
    """`Vertex AI` Chat large language models API."""
    model_name: str = 'chat-bison'
    'Underlying model name.'
    examples: Optional[List[BaseMessage]] = None

    @classmethod
    def is_lc_serializable(self) -> bool:
        return True

    @classmethod
    def get_lc_namespace(cls) -> List[str]:
        """Get the namespace of the langchain object."""
        return ['langchain', 'chat_models', 'vertexai']

    @root_validator()
    def validate_environment(cls, values: Dict) -> Dict:
        """Validate that the python package exists in environment."""
        is_gemini = is_gemini_model(values['model_name'])
        cls._try_init_vertexai(values)
        try:
            from vertexai.language_models import ChatModel, CodeChatModel
            if is_gemini:
                from vertexai.preview.generative_models import GenerativeModel
        except ImportError:
            raise_vertex_import_error()
        if is_gemini:
            values['client'] = GenerativeModel(model_name=values['model_name'])
        else:
            if is_codey_model(values['model_name']):
                model_cls = CodeChatModel
            else:
                model_cls = ChatModel
            values['client'] = model_cls.from_pretrained(values['model_name'])
        return values

    def _generate(self, messages: List[BaseMessage], stop: Optional[List[str]]=None, run_manager: Optional[CallbackManagerForLLMRun]=None, stream: Optional[bool]=None, **kwargs: Any) -> ChatResult:
        """Generate next turn in the conversation.

        Args:
            messages: The history of the conversation as a list of messages. Code chat
                does not support context.
            stop: The list of stop words (optional).
            run_manager: The CallbackManager for LLM run, it's not used at the moment.
            stream: Whether to use the streaming endpoint.

        Returns:
            The ChatResult that contains outputs generated by the model.

        Raises:
            ValueError: if the last message in the list is not from human.
        """
        should_stream = stream if stream is not None else self.streaming
        if should_stream:
            stream_iter = self._stream(messages, stop=stop, run_manager=run_manager, **kwargs)
            return generate_from_stream(stream_iter)
        question = _get_question(messages)
        params = self._prepare_params(stop=stop, stream=False, **kwargs)
        msg_params = {}
        if 'candidate_count' in params:
            msg_params['candidate_count'] = params.pop('candidate_count')
        if self._is_gemini_model:
            history_gemini = _parse_chat_history_gemini(messages, project=self.project)
            message = history_gemini.pop()
            chat = self.client.start_chat(history=history_gemini)
            response = chat.send_message(message, generation_config=params)
        else:
            history = _parse_chat_history(messages[:-1])
            examples = kwargs.get('examples') or self.examples
            if examples:
                params['examples'] = _parse_examples(examples)
            chat = self._start_chat(history, **params)
            response = chat.send_message(question.content, **msg_params)
        generations = [ChatGeneration(message=AIMessage(content=r.text)) for r in response.candidates]
        return ChatResult(generations=generations)

    async def _agenerate(self, messages: List[BaseMessage], stop: Optional[List[str]]=None, run_manager: Optional[AsyncCallbackManagerForLLMRun]=None, **kwargs: Any) -> ChatResult:
        """Asynchronously generate next turn in the conversation.

        Args:
            messages: The history of the conversation as a list of messages. Code chat
                does not support context.
            stop: The list of stop words (optional).
            run_manager: The CallbackManager for LLM run, it's not used at the moment.

        Returns:
            The ChatResult that contains outputs generated by the model.

        Raises:
            ValueError: if the last message in the list is not from human.
        """
        if 'stream' in kwargs:
            kwargs.pop('stream')
            logger.warning('ChatVertexAI does not currently support async streaming.')
        params = self._prepare_params(stop=stop, **kwargs)
        msg_params = {}
        if 'candidate_count' in params:
            msg_params['candidate_count'] = params.pop('candidate_count')
        if self._is_gemini_model:
            history_gemini = _parse_chat_history_gemini(messages, project=self.project)
            message = history_gemini.pop()
            chat = self.client.start_chat(history=history_gemini)
            response = await chat.send_message_async(message, generation_config=params)
        else:
            question = _get_question(messages)
            history = _parse_chat_history(messages[:-1])
            examples = kwargs.get('examples', None)
            if examples:
                params['examples'] = _parse_examples(examples)
            chat = self._start_chat(history, **params)
            response = await chat.send_message_async(question.content, **msg_params)
        generations = [ChatGeneration(message=AIMessage(content=r.text)) for r in response.candidates]
        return ChatResult(generations=generations)

    def _stream(self, messages: List[BaseMessage], stop: Optional[List[str]]=None, run_manager: Optional[CallbackManagerForLLMRun]=None, **kwargs: Any) -> Iterator[ChatGenerationChunk]:
        params = self._prepare_params(stop=stop, stream=True, **kwargs)
        if self._is_gemini_model:
            history_gemini = _parse_chat_history_gemini(messages, project=self.project)
            message = history_gemini.pop()
            chat = self.client.start_chat(history=history_gemini)
            responses = chat.send_message(message, stream=True, generation_config=params)
        else:
            question = _get_question(messages)
            history = _parse_chat_history(messages[:-1])
            examples = kwargs.get('examples', None)
            if examples:
                params['examples'] = _parse_examples(examples)
            chat = self._start_chat(history, **params)
            responses = chat.send_message_streaming(question.content, **params)
        for response in responses:
            chunk = ChatGenerationChunk(message=AIMessageChunk(content=response.text))
            if run_manager:
                run_manager.on_llm_new_token(response.text, chunk=chunk)
            yield chunk

    def _start_chat(self, history: _ChatHistory, **kwargs: Any) -> Union[ChatSession, CodeChatSession]:
        if not self.is_codey_model:
            return self.client.start_chat(context=history.context, message_history=history.history, **kwargs)
        else:
            return self.client.start_chat(message_history=history.history, **kwargs)