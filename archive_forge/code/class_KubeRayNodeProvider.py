import json
import logging
import os
from collections import defaultdict
from typing import Any, Dict, List, Optional, Tuple
import requests
from ray.autoscaler._private.constants import (
from ray.autoscaler._private.util import NodeID, NodeIP, NodeKind, NodeStatus, NodeType
from ray.autoscaler.batching_node_provider import (
from ray.autoscaler.tags import (
class KubeRayNodeProvider(BatchingNodeProvider):

    def __init__(self, provider_config: Dict[str, Any], cluster_name: str, _allow_multiple: bool=False):
        logger.info('Creating KubeRayNodeProvider.')
        self.namespace = provider_config['namespace']
        self.cluster_name = cluster_name
        self.headers, self.verify = load_k8s_secrets()
        assert provider_config.get(WORKER_LIVENESS_CHECK_KEY, True) is False, f'To use KubeRayNodeProvider, must set `{WORKER_LIVENESS_CHECK_KEY}:False`.'
        assert provider_config.get(WORKER_RPC_DRAIN_KEY, False) is True, f'To use KubeRayNodeProvider, must set `{WORKER_RPC_DRAIN_KEY}:True`.'
        BatchingNodeProvider.__init__(self, provider_config, cluster_name, _allow_multiple)

    def get_node_data(self) -> Dict[NodeID, NodeData]:
        """Queries K8s for pods in the RayCluster. Converts that pod data into a
        map of pod name to Ray NodeData, as required by BatchingNodeProvider.
        """
        self._raycluster = self._get(f'rayclusters/{self.cluster_name}')
        resource_version = self._get_pods_resource_version()
        if resource_version:
            logger.info(f'Listing pods for RayCluster {self.cluster_name} in namespace {self.namespace} at pods resource version >= {resource_version}.')
        label_selector = requests.utils.quote(f'ray.io/cluster={self.cluster_name}')
        resource_path = f'pods?labelSelector={label_selector}'
        if resource_version:
            resource_path += f'&resourceVersion={resource_version}' + '&resourceVersionMatch=NotOlderThan'
        pod_list = self._get(resource_path)
        fetched_resource_version = pod_list['metadata']['resourceVersion']
        logger.info(f'Fetched pod data at resource version {fetched_resource_version}.')
        node_data_dict = {}
        for pod in pod_list['items']:
            if 'deletionTimestamp' in pod['metadata']:
                continue
            pod_name = pod['metadata']['name']
            node_data_dict[pod_name] = node_data_from_pod(pod)
        return node_data_dict

    def submit_scale_request(self, scale_request: ScaleRequest):
        """Converts the scale request generated by BatchingNodeProvider into
        a patch that modifies the RayCluster CR's replicas and/or workersToDelete
        fields. Then submits the patch to the K8s API server.
        """
        patch_payload = self._scale_request_to_patch_payload(scale_request, self._raycluster)
        logger.info(f'Autoscaler is submitting the following patch to RayCluster {self.cluster_name} in namespace {self.namespace}.')
        logger.info(patch_payload)
        self._submit_raycluster_patch(patch_payload)

    def safe_to_scale(self) -> bool:
        """Returns False iff non_terminated_nodes contains any pods in the RayCluster's
        workersToDelete lists.

        Explanation:
        If there are any workersToDelete which are non-terminated,
        we should wait for the operator to do its job and delete those
        pods. Therefore, we back off the autoscaler update.

        If, on the other hand, all of the workersToDelete have already been cleaned up,
        then we patch away the workersToDelete lists and return True.
        In the future, we may consider having the operator clean up workersToDelete
        on it own:
        https://github.com/ray-project/kuberay/issues/733

        Note (Dmitri):
        It is stylistically bad that this function has a side effect.
        """
        node_set = set(self.node_data_dict.keys())
        worker_groups = self._raycluster['spec'].get('workerGroupSpecs', [])
        non_empty_worker_group_indices = []
        for group_index, worker_group in enumerate(worker_groups):
            workersToDelete = worker_group.get('scaleStrategy', {}).get('workersToDelete', [])
            if workersToDelete:
                non_empty_worker_group_indices.append(group_index)
            for worker in workersToDelete:
                if worker in node_set:
                    logger.warning(f'Waiting for operator to remove worker {worker}.')
                    return False
        patch_payload = []
        for group_index in non_empty_worker_group_indices:
            patch = worker_delete_patch(group_index, workers_to_delete=[])
            patch_payload.append(patch)
        if patch_payload:
            logger.info('Cleaning up workers to delete.')
            logger.info(f'Submitting patch {patch_payload}.')
            self._submit_raycluster_patch(patch_payload)
        return True

    def _get_pods_resource_version(self) -> str:
        """
        Extract a recent pods resource version by reading the head pod's
        metadata.resourceVersion of the response.
        """
        if not RAY_HEAD_POD_NAME:
            return None
        pod_resp = self._get(f'pods/{RAY_HEAD_POD_NAME}')
        return pod_resp['metadata']['resourceVersion']

    def _scale_request_to_patch_payload(self, scale_request: ScaleRequest, raycluster: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Converts autoscaler scale request into a RayCluster CR patch payload."""
        patch_payload = []
        for node_type, target_replicas in scale_request.desired_num_workers.items():
            group_index = _worker_group_index(raycluster, node_type)
            group_max_replicas = _worker_group_max_replicas(raycluster, group_index)
            if group_max_replicas is not None and group_max_replicas < target_replicas:
                logger.warning('Autoscaler attempted to create ' + 'more than maxReplicas pods of type {}.'.format(node_type))
                target_replicas = group_max_replicas
            if target_replicas == _worker_group_replicas(raycluster, group_index):
                continue
            patch = worker_replica_patch(group_index, target_replicas)
            patch_payload.append(patch)
        deletion_groups = defaultdict(list)
        for worker in scale_request.workers_to_delete:
            node_type = self.node_tags(worker)[TAG_RAY_USER_NODE_TYPE]
            deletion_groups[node_type].append(worker)
        for node_type, workers_to_delete in deletion_groups.items():
            group_index = _worker_group_index(raycluster, node_type)
            patch = worker_delete_patch(group_index, workers_to_delete)
            patch_payload.append(patch)
        return patch_payload

    def _submit_raycluster_patch(self, patch_payload: List[Dict[str, Any]]):
        """Submits a patch to modify a RayCluster CR."""
        path = 'rayclusters/{}'.format(self.cluster_name)
        self._patch(path, patch_payload)

    def _url(self, path: str) -> str:
        """Convert resource path to REST URL for Kubernetes API server."""
        if path.startswith('pods'):
            api_group = '/api/v1'
        elif path.startswith('rayclusters'):
            api_group = '/apis/ray.io/' + KUBERAY_CRD_VER
        else:
            raise NotImplementedError('Tried to access unknown entity at {}'.format(path))
        return 'https://kubernetes.default:443' + api_group + '/namespaces/' + self.namespace + '/' + path

    def _get(self, path: str) -> Dict[str, Any]:
        """Wrapper for REST GET of resource with proper headers."""
        url = url_from_resource(namespace=self.namespace, path=path)
        result = requests.get(url, headers=self.headers, verify=self.verify)
        if not result.status_code == 200:
            result.raise_for_status()
        return result.json()

    def _patch(self, path: str, payload: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Wrapper for REST PATCH of resource with proper headers."""
        url = url_from_resource(namespace=self.namespace, path=path)
        result = requests.patch(url, json.dumps(payload), headers={**self.headers, 'Content-type': 'application/json-patch+json'}, verify=self.verify)
        if not result.status_code == 200:
            result.raise_for_status()
        return result.json()