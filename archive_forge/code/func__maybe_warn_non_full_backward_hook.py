from collections import OrderedDict, namedtuple
import itertools
import warnings
import functools
import weakref
import torch
from torch._prims_common import DeviceLikeType
from ..parameter import Parameter
import torch.utils.hooks as hooks
from torch import Tensor, device, dtype
from typing import Union, Tuple, Any, Callable, Iterator, Set, Optional, overload, TypeVar, Mapping, Dict, List
from typing_extensions import Self
from ...utils.hooks import RemovableHandle
def _maybe_warn_non_full_backward_hook(self, inputs, result, grad_fn):
    if not isinstance(result, torch.Tensor):
        if not (isinstance(result, tuple) and all((isinstance(r, torch.Tensor) for r in result))):
            warnings.warn('Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.')
            return
    else:
        result = (result,)
    if not isinstance(inputs, torch.Tensor):
        if not (isinstance(inputs, tuple) and all((isinstance(i, torch.Tensor) for i in inputs))):
            warnings.warn('Using non-full backward hooks on a Module that does not take as input a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_input. Please use register_full_backward_hook to get the documented behavior.')
            return
    else:
        inputs = (inputs,)
    out_grad_fn = {r.grad_fn for r in result if r.grad_fn is not None}
    if len(out_grad_fn) == 0 or (len(out_grad_fn) == 1 and grad_fn not in out_grad_fn):
        warnings.warn('Using a non-full backward hook when outputs are nested in python data structure is deprecated and will be removed in future versions. This hook will be missing some grad_output.')
    elif len(out_grad_fn) > 1:
        warnings.warn('Using a non-full backward hook when outputs are generated by different autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_output. Please use register_full_backward_hook to get the documented behavior.')
    else:
        inputs_grad_fn = {i.grad_fn for i in inputs if i.grad_fn is not None}
        next_functions = {n[0] for n in grad_fn.next_functions}
        if inputs_grad_fn != next_functions:
            warnings.warn('Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.')