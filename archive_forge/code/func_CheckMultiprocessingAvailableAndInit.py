from __future__ import absolute_import
from __future__ import print_function
from __future__ import division
from __future__ import unicode_literals
import collections
import errno
import logging
import multiprocessing
import threading
import traceback
from gslib.utils import constants
from gslib.utils import system_util
from six.moves import queue as Queue
def CheckMultiprocessingAvailableAndInit(logger=None):
    """Checks if multiprocessing is available, and if so performs initialization.

  There are some environments in which there is no way to use multiprocessing
  logic that's built into Python (e.g., if /dev/shm is not available, then
  we can't create semaphores). This simply tries out a few things that will be
  needed to make sure the environment can support the pieces of the
  multiprocessing module that we need.

  See gslib.command.InitializeMultiprocessingVariables for
  an explanation of why this is necessary.

  Args:
    logger: (logging.Logger) Logger to use for debug output.

  Returns:
    (MultiprocessingIsAvailableResult) A namedtuple with the following attrs:
      - multiprocessing_is_available: True iff the multiprocessing module is
            available for use.
      - stack_trace: The stack trace generated by the call we tried that
            failed.
  """
    global _cached_multiprocessing_is_available
    global _cached_multiprocessing_check_stack_trace
    global _cached_multiprocessing_is_available_message
    if _cached_multiprocessing_is_available is not None:
        if logger:
            logger.debug(_cached_multiprocessing_check_stack_trace)
            logger.warn(_cached_multiprocessing_is_available_message)
        return MultiprocessingIsAvailableResult(is_available=_cached_multiprocessing_is_available, stack_trace=_cached_multiprocessing_check_stack_trace)
    should_prohibit_multiprocessing, os_name = ShouldProhibitMultiprocessing()
    if should_prohibit_multiprocessing:
        message = '\nMultiple processes are not supported on %s. Operations requesting\nparallelism will be executed with multiple threads in a single process only.\n' % os_name
        if logger:
            logger.warn(message)
        return MultiprocessingIsAvailableResult(is_available=False, stack_trace=None)
    stack_trace = None
    multiprocessing_is_available = True
    message = "\nYou have requested multiple processes for an operation, but the\nrequired functionality of Python's multiprocessing module is not available.\nOperations requesting parallelism will be executed with multiple threads in a\nsingle process only.\n"
    try:
        try:
            multiprocessing_context.Value('i', 0)
        except:
            message += '\nPlease ensure that you have write access to both /dev/shm and /run/shm.\n'
            raise
        global top_level_manager
        top_level_manager = multiprocessing_context.Manager()
        limit = -1
        if _HAS_RESOURCE_MODULE:
            try:
                limit = max(limit, _IncreaseSoftLimitForResource(resource.RLIMIT_NOFILE, constants.MIN_ACCEPTABLE_OPEN_FILES_LIMIT))
            except AttributeError:
                pass
            try:
                limit = max(limit, _IncreaseSoftLimitForResource(resource.RLIMIT_OFILE, constants.MIN_ACCEPTABLE_OPEN_FILES_LIMIT))
            except AttributeError:
                pass
        if limit < constants.MIN_ACCEPTABLE_OPEN_FILES_LIMIT:
            message += '\nYour max number of open files, %s, is too low to allow safe multiprocessing.\nOn Linux you can fix this by adding something like "ulimit -n 10000" to your\n~/.bashrc or equivalent file and opening a new terminal.\n\nOn macOS, you may also need to run a command like this once (in addition to the\nabove instructions), which might require a restart of your system to take\neffect:\n  launchctl limit maxfiles 10000\n\nAlternatively, edit /etc/launchd.conf with something like:\n  limit maxfiles 10000 10000\n\n' % limit
            raise Exception('Max number of open files, %s, is too low.' % limit)
    except:
        stack_trace = traceback.format_exc()
        multiprocessing_is_available = False
        if logger is not None:
            logger.debug(stack_trace)
            logger.warn(message)
    _cached_multiprocessing_is_available = multiprocessing_is_available
    _cached_multiprocessing_check_stack_trace = stack_trace
    _cached_multiprocessing_is_available_message = message
    return MultiprocessingIsAvailableResult(is_available=_cached_multiprocessing_is_available, stack_trace=_cached_multiprocessing_check_stack_trace)