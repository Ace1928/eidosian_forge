from dataclasses import dataclass, field
from typing import Any, Dict, List
from mlflow.metrics.genai.base import EvaluationExample
from mlflow.metrics.genai.prompt_template import PromptTemplate
@dataclass
class AnswerCorrectnessMetric:
    definition = 'Answer correctness is evaluated on the accuracy of the provided output based on the provided targets, which is the ground truth. Scores can be assigned based on the degree of semantic similarity and factual correctness of the provided output to the provided targets, where a higher score indicates higher degree of accuracy.'
    grading_prompt = 'Answer Correctness: Below are the details for different scores:\n- Score 1: The output is completely incorrect. It is completely different from or contradicts the provided targets.\n- Score 2: The output demonstrates some degree of semantic similarity and includes partially correct information. However, the output still has significant discrepancies with the provided targets or inaccuracies.\n- Score 3: The output addresses a couple of aspects of the input accurately, aligning with the provided targets. However, there are still omissions or minor inaccuracies.\n- Score 4: The output is mostly correct. It provides mostly accurate information, but there may be one or more minor omissions or inaccuracies.\n- Score 5: The output is correct. It demonstrates a high degree of accuracy and semantic similarity to the targets.'
    grading_context_columns = ['targets']
    parameters = default_parameters
    default_model = default_model
    example_score_2 = EvaluationExample(input='How is MLflow related to Databricks?', output='Databricks is a data engineering and analytics platform designed to help organizations process and analyze large amounts of data. Databricks is a company specializing in big data and machine learning solutions.', score=2, justification="The output provided by the model does demonstrate some degree of semantic similarity to the targets, as it correctly identifies Databricks as a company specializing in big data and machine learning solutions. However, it fails to address the main point of the input question, which is the relationship between MLflow and Databricks. The output does not mention MLflow at all, which is a significant discrepancy with the provided targets. Therefore, the model's answer_correctness score is 2.", grading_context={'targets': 'MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.'})
    example_score_4 = EvaluationExample(input='How is MLflow related to Databricks?', output='MLflow is a product created by Databricks to enhance the efficiency of machine learning processes.', score=4, justification='The output provided by the model is mostly correct. It correctly identifies that MLflow is a product created by Databricks. However, it does not mention that MLflow is an open-source platform for managing the end-to-end machine learning lifecycle, which is a significant part of its function. Therefore, while the output is mostly accurate, it has a minor omission, which is why it gets a score of 4 according to the grading rubric.', grading_context={'targets': 'MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.'})
    default_examples = [example_score_2, example_score_4]