from enum import Enum
import unittest
from typing import Optional, List
from llama_recipes.data.llama_guard.finetuning_data_formatter import (
@staticmethod
def create_most_conservative_formatter_configs(agent_type_to_check: AgentType) -> FormatterConfigs:
    return FormatterConfigs(guidelines=Guidelines(categories=[Category(name='cat V', description='cat V description'), Category(name='cat W', description='cat W description'), Category(name='cat X', description='cat X description'), Category(name='cat Y', description='cat Y description'), Category(name='cat Z', description='cat Z description')], category_code_prefix='O'), llama_guard_prompt_configs=LlamaGuardPromptConfigs(instructions_format_string=f"[INST] Task: Check if there is unsafe content in '{agent_type_to_check}' messages in conversations according our safety policy with the below categories.\n<BEGIN UNSAFE CONTENT CATEGORIES>\n{{guidelines}}\n<END UNSAFE CONTENT CATEGORIES>\n\n<BEGIN CONVERSATION>\n\n{{conversation}}\n\n<END CONVERSATION>\n\nProvide your safety assessment for {agent_type_to_check} in the above conversation:\n- First line must read 'safe' or 'unsafe'.\n- If unsafe, a second line must include a comma-separated list of violated categories. [/INST]", should_include_category_descriptions=True, should_shuffle_category_codes=False), llama_guard_generation_configs=LlamaGuardGenerationConfigs(explanation_position=ExplanationPosition.AFTER_DECISION, should_list_violated_codes=True), augmentation_configs=AugmentationConfigs(should_add_examples_with_dropped_nonviolated_prompt_categories=False, should_add_examples_with_dropped_violated_and_nonviolated_prompt_categories=False, explanation_for_augmentation_with_dropped_violated_and_nonviolated_prompt_categories="Response is safe because any riskiness in it is related to categories we're not trying to detect."), random_seed=42)