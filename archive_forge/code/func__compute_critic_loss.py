from typing import Any, Dict, Mapping, Tuple
import gymnasium as gym
from ray.rllib.algorithms.dreamerv3.dreamerv3_learner import (
from ray.rllib.core.rl_module.marl_module import ModuleID
from ray.rllib.core.learner.learner import ParamDict
from ray.rllib.core.learner.tf.tf_learner import TfLearner
from ray.rllib.policy.sample_batch import DEFAULT_POLICY_ID, SampleBatch
from ray.rllib.utils.annotations import override
from ray.rllib.utils.framework import try_import_tf, try_import_tfp
from ray.rllib.utils.tf_utils import symlog, two_hot, clip_gradients
from ray.rllib.utils.typing import TensorType
def _compute_critic_loss(self, *, module_id: ModuleID, hps: DreamerV3LearnerHyperparameters, dream_data: Dict[str, TensorType], value_targets_t0_to_Hm1_BxT: TensorType) -> TensorType:
    """Helper method computing the critic's loss terms.

        Args:
            module_id: The ModuleID for which to compute the critic loss.
            hps: The DreamerV3LearnerHyperparameters to use.
            dream_data: The data generated by dreaming for H steps (horizon) starting
                from any BxT state (sampled from the buffer for the train batch).
            value_targets_t0_to_Hm1_BxT: The computed value function targets of the
                shape (t0 to H-1, BxT).

        Returns:
            The total critic loss tensor.
        """
    H, B = dream_data['rewards_dreamed_t0_to_H_BxT'].shape[:2]
    Hm1 = H - 1
    value_targets_t0_to_Hm1_B = tf.stop_gradient(value_targets_t0_to_Hm1_BxT)
    value_symlog_targets_t0_to_Hm1_B = symlog(value_targets_t0_to_Hm1_B)
    value_symlog_targets_HxB = tf.reshape(value_symlog_targets_t0_to_Hm1_B, (-1,))
    value_symlog_targets_two_hot_HxB = two_hot(value_symlog_targets_HxB)
    value_symlog_targets_two_hot_t0_to_Hm1_B = tf.reshape(value_symlog_targets_two_hot_HxB, shape=[Hm1, B, value_symlog_targets_two_hot_HxB.shape[-1]])
    value_symlog_logits_HxB = dream_data['values_symlog_dreamed_logits_t0_to_HxBxT']
    value_symlog_logits_t0_to_Hm1_B = tf.reshape(value_symlog_logits_HxB, shape=[H, B, value_symlog_logits_HxB.shape[-1]])[:-1]
    values_log_pred_Hm1_B = value_symlog_logits_t0_to_Hm1_B - tf.math.reduce_logsumexp(value_symlog_logits_t0_to_Hm1_B, axis=-1, keepdims=True)
    value_loss_two_hot_H_B = -tf.reduce_sum(values_log_pred_Hm1_B * value_symlog_targets_two_hot_t0_to_Hm1_B, axis=-1)
    value_symlog_ema_t0_to_Hm1_B = tf.stop_gradient(dream_data['v_symlog_dreamed_ema_t0_to_H_BxT'])[:-1]
    value_symlog_ema_HxB = tf.reshape(value_symlog_ema_t0_to_Hm1_B, (-1,))
    value_symlog_ema_two_hot_HxB = two_hot(value_symlog_ema_HxB)
    value_symlog_ema_two_hot_t0_to_Hm1_B = tf.reshape(value_symlog_ema_two_hot_HxB, shape=[Hm1, B, value_symlog_ema_two_hot_HxB.shape[-1]])
    ema_regularization_loss_H_B = -tf.reduce_sum(values_log_pred_Hm1_B * value_symlog_ema_two_hot_t0_to_Hm1_B, axis=-1)
    L_critic_H_B = value_loss_two_hot_H_B + ema_regularization_loss_H_B
    L_critic_H_B *= tf.stop_gradient(dream_data['dream_loss_weights_t0_to_H_BxT'])[:-1]
    L_critic = tf.reduce_mean(L_critic_H_B)
    self.register_metrics(module_id=module_id, metrics_dict={'CRITIC_L_total': L_critic, 'CRITIC_L_neg_logp_of_value_targets': tf.reduce_mean(value_loss_two_hot_H_B), 'CRITIC_L_slow_critic_regularization': tf.reduce_mean(ema_regularization_loss_H_B)})
    if hps.report_individual_batch_item_stats:
        self.register_metrics(module_id=module_id, metrics_dict={'VALUE_TARGETS_symlog_H_BxT': value_symlog_targets_t0_to_Hm1_B, 'CRITIC_L_total_H_BxT': L_critic_H_B, 'CRITIC_L_neg_logp_of_value_targets_H_BxT': value_loss_two_hot_H_B, 'CRITIC_L_slow_critic_regularization_H_BxT': ema_regularization_loss_H_B})
    return L_critic