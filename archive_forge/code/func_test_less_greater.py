from itertools import product
import numpy as np
import random
import functools
import pytest
from numpy.testing import (assert_, assert_equal, assert_allclose,
from pytest import raises as assert_raises
import scipy.stats as stats
from scipy.stats import distributions
from scipy.stats._hypotests import (epps_singleton_2samp, cramervonmises,
from scipy.stats._mannwhitneyu import mannwhitneyu, _mwu_state
from .common_tests import check_named_results
from scipy._lib._testutils import _TestPythranFunc
@pytest.mark.parametrize('input_sample,expected', [([[2, 7], [8, 2]], (-2.518474945157, 0.009886140845)), ([[7, 200], [300, 8]], (-21.32003669846, 0.0)), ([[21, 28], [1957, 6]], (-30.489638143953, 0.0))])
@pytest.mark.parametrize('alternative', ['greater', 'less'])
def test_less_greater(self, input_sample, expected, alternative):
    """
        "The expected values have been generated by R, using a resolution
        for the nuisance parameter of 1e-6 :
        ```R
        library(Barnard)
        options(digits=10)
        a = barnard.test(2, 7, 8, 2, dp=1e-6, pooled=TRUE)
        a$p.value[1]
        ```
        In this test, we are using the "one-sided" return value `a$p.value[1]`
        to test our pvalue.
        """
    expected_stat, less_pvalue_expect = expected
    if alternative == 'greater':
        input_sample = np.array(input_sample)[:, ::-1]
        expected_stat = -expected_stat
    res = barnard_exact(input_sample, alternative=alternative)
    statistic, pvalue = (res.statistic, res.pvalue)
    assert_allclose([statistic, pvalue], [expected_stat, less_pvalue_expect], atol=1e-07)