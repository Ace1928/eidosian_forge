import copy
import math
from typing import Optional, Tuple, Union
import torch
from torch import nn
from torch.nn import CrossEntropyLoss
from transformers.generation import GenerationConfig
from ...activations import ACT2FN
from ...modeling_outputs import (
from ...modeling_utils import PreTrainedModel
from ...pytorch_utils import ALL_LAYERNORM_LAYERS, find_pruneable_heads_and_indices, prune_linear_layer
from ...utils import (
from .configuration_pop2piano import Pop2PianoConfig
def get_mel_conditioner_outputs(self, input_features: torch.FloatTensor, composer: str, generation_config: GenerationConfig, attention_mask: torch.FloatTensor=None):
    """
        This method is used to concatenate mel conditioner tokens at the front of the input_features in order to
        control the type of MIDI token generated by the model.

        Args:
            input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):
                input features extracted from the feature extractor.
            composer (`str`):
                composer token which determines the type of MIDI tokens to be generated.
            generation_config (`~generation.GenerationConfig`):
                The generation is used to get the composer-feature_token pair.
            attention_mask (``, *optional*):
                For batched generation `input_features` are padded to have the same shape across all examples.
                `attention_mask` helps to determine which areas were padded and which were not.
                - 1 for tokens that are **not padded**,
                - 0 for tokens that are **padded**.
        """
    composer_to_feature_token = generation_config.composer_to_feature_token
    if composer not in composer_to_feature_token.keys():
        raise ValueError(f'Please choose a composer from {list(composer_to_feature_token.keys())}. Composer received - {composer}')
    composer_value = composer_to_feature_token[composer]
    composer_value = torch.tensor(composer_value, device=self.device)
    composer_value = composer_value.repeat(input_features.shape[0])
    embedding_offset = min(composer_to_feature_token.values())
    input_features = self.mel_conditioner(feature=input_features, index_value=composer_value, embedding_offset=embedding_offset)
    if attention_mask is not None:
        input_features[~attention_mask[:, 0].bool()] = 0.0
        attention_mask = torch.concatenate([attention_mask[:, 0].view(-1, 1), attention_mask], axis=1)
        return (input_features, attention_mask)
    return (input_features, None)