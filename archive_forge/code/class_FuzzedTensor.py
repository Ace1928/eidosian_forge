import functools
import itertools as it
from typing import Any, Callable, Dict, List, Optional, Tuple, Union
import numpy as np
import torch
class FuzzedTensor:

    def __init__(self, name: str, size: Tuple[Union[str, int], ...], steps: Optional[Tuple[Union[str, int], ...]]=None, probability_contiguous: float=0.5, min_elements: Optional[int]=None, max_elements: Optional[int]=None, max_allocation_bytes: Optional[int]=None, dim_parameter: Optional[str]=None, roll_parameter: Optional[str]=None, dtype=torch.float32, cuda=False, tensor_constructor: Optional[Callable]=None):
        """
        Args:
            name:
                A string identifier for the generated Tensor.
            size:
                A tuple of integers or strings specifying the size of the generated
                Tensor. String values will replaced with a concrete int during the
                generation process, while ints are simply passed as literals.
            steps:
                An optional tuple with the same length as `size`. This indicates
                that a larger Tensor should be allocated, and then sliced to
                produce the generated Tensor. For instance, if size is (4, 8)
                and steps is (1, 4), then a tensor `t` of size (4, 32) will be
                created and then `t[:, ::4]` will be used. (Allowing one to test
                Tensors with strided memory.)
            probability_contiguous:
                A number between zero and one representing the chance that the
                generated Tensor has a contiguous memory layout. This is achieved by
                randomly permuting the shape of a Tensor, calling `.contiguous()`,
                and then permuting back. This is applied before `steps`, which can
                also cause a Tensor to be non-contiguous.
            min_elements:
                The minimum number of parameters that this Tensor must have for a
                set of parameters to be valid. (Otherwise they are resampled.)
            max_elements:
                Like `min_elements`, but setting an upper bound.
            max_allocation_bytes:
                Like `max_elements`, but for the size of Tensor that must be
                allocated prior to slicing for `steps` (if applicable). For
                example, a FloatTensor with size (1024, 1024) and steps (4, 4)
                would have 1M elements, but would require a 64 MB allocation.
            dim_parameter:
                The length of `size` and `steps` will be truncated to this value.
                This allows Tensors of varying dimensions to be generated by the
                Fuzzer.
            dtype:
                The PyTorch dtype of the generated Tensor.
            cuda:
                Whether to place the Tensor on a GPU.
            tensor_constructor:
                Callable which will be used instead of the default Tensor
                construction method. This allows the author to enforce properties
                of the Tensor (e.g. it can only have certain values). The dtype and
                concrete shape of the Tensor to be created will be passed, and
                concrete values of all parameters will be passed as kwargs. Note
                that transformations to the result (permuting, slicing) will be
                performed by the Fuzzer; the tensor_constructor is only responsible
                for creating an appropriately sized Tensor.
        """
        self._name = name
        self._size = size
        self._steps = steps
        self._probability_contiguous = probability_contiguous
        self._min_elements = min_elements
        self._max_elements = max_elements
        self._max_allocation_bytes = max_allocation_bytes
        self._dim_parameter = dim_parameter
        self._dtype = dtype
        self._cuda = cuda
        self._tensor_constructor = tensor_constructor

    @property
    def name(self):
        return self._name

    @staticmethod
    def default_tensor_constructor(size, dtype, **kwargs):
        if dtype.is_floating_point or dtype.is_complex:
            return torch.rand(size=size, dtype=dtype, device='cpu')
        else:
            return torch.randint(1, 127, size=size, dtype=dtype, device='cpu')

    def _make_tensor(self, params, state):
        size, steps, allocation_size = self._get_size_and_steps(params)
        constructor = self._tensor_constructor or self.default_tensor_constructor
        raw_tensor = constructor(size=allocation_size, dtype=self._dtype, **params)
        if self._cuda:
            raw_tensor = raw_tensor.cuda()
        dim = len(size)
        order = np.arange(dim)
        if state.rand() > self._probability_contiguous:
            while dim > 1 and np.all(order == np.arange(dim)):
                order = state.permutation(raw_tensor.dim())
            raw_tensor = raw_tensor.permute(tuple(order)).contiguous()
            raw_tensor = raw_tensor.permute(tuple(np.argsort(order)))
        slices = [slice(0, size * step, step) for size, step in zip(size, steps)]
        tensor = raw_tensor[slices]
        properties = {'numel': int(tensor.numel()), 'order': order, 'steps': steps, 'is_contiguous': tensor.is_contiguous(), 'dtype': str(self._dtype)}
        return (tensor, properties)

    def _get_size_and_steps(self, params):
        dim = params[self._dim_parameter] if self._dim_parameter is not None else len(self._size)

        def resolve(values, dim):
            """Resolve values into concrete integers."""
            values = tuple((params.get(i, i) for i in values))
            if len(values) > dim:
                values = values[:dim]
            if len(values) < dim:
                values = values + tuple((1 for _ in range(dim - len(values))))
            return values
        size = resolve(self._size, dim)
        steps = resolve(self._steps or (), dim)
        allocation_size = tuple((size_i * step_i for size_i, step_i in zip(size, steps)))
        return (size, steps, allocation_size)

    def satisfies_constraints(self, params):
        size, _, allocation_size = self._get_size_and_steps(params)
        num_elements = prod(size)
        assert num_elements >= 0
        allocation_bytes = prod(allocation_size, base=dtype_size(self._dtype))

        def nullable_greater(left, right):
            if left is None or right is None:
                return False
            return left > right
        return not any((nullable_greater(num_elements, self._max_elements), nullable_greater(self._min_elements, num_elements), nullable_greater(allocation_bytes, self._max_allocation_bytes)))