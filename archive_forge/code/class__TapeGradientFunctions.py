import collections
import pprint
from tensorflow.core.framework import attr_value_pb2
from tensorflow.core.function.polymorphism import function_type as function_type_lib
from tensorflow.python import pywrap_tfe
from tensorflow.python.eager import backprop_util
from tensorflow.python.eager import context
from tensorflow.python.eager import forwardprop_util
from tensorflow.python.eager import record
from tensorflow.python.eager.graph_only_ops import graph_placeholder
from tensorflow.python.eager.polymorphic_function import atomic_function
from tensorflow.python.eager.polymorphic_function import attributes as attributes_lib
from tensorflow.python.eager.polymorphic_function import function_type_utils
from tensorflow.python.eager.polymorphic_function import saved_model_exported_concrete
from tensorflow.python.framework import composite_tensor
from tensorflow.python.framework import dtypes
from tensorflow.python.framework import errors
from tensorflow.python.framework import func_graph as func_graph_module
from tensorflow.python.framework import indexed_slices
from tensorflow.python.framework import ops
from tensorflow.python.framework import tensor as tensor_lib
from tensorflow.python.framework import tensor_shape
from tensorflow.python.framework import type_spec
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import default_gradient
from tensorflow.python.ops import gradients_util
from tensorflow.python.ops import handle_data_util
from tensorflow.python.ops import resource_variable_ops
from tensorflow.python.platform import tf_logging as logging
from tensorflow.python.profiler import trace
from tensorflow.python.trackable import base as trackable
from tensorflow.python.types import core
from tensorflow.python.util import _pywrap_utils
from tensorflow.python.util import compat
from tensorflow.python.util import nest
from tensorflow.python.util import object_identity
class _TapeGradientFunctions(object):
    """Caches forward and backward functions compatible with eager gradients.

  In contrast to the delayed-rewrite approach in
  `_DelayedRewriteGradientFunctions` which only works with delayed execution,
  the forward function generated by this class has a fixed set of outputs which
  may be preserved by a tape in order to compute gradients later.

  This class is abstract; its child classes differ in how many side outputs of
  the forward function their backward function accepts gradients for, which
  determines whether higher-order tape gradients are possible.
  """

    def __init__(self, func_graph, attrs, func_graph_deleter, forwardprop_input_indices, delayed_rewrite_functions, need_gradients_for_jvps):
        self._func_graph = func_graph
        self._forward_graph = None
        self._attrs = attrs
        self._forward = None
        self._backward = None
        self._num_outputs = len(func_graph.outputs)
        self._func_graph_deleter = func_graph_deleter
        self._forwardprop_input_indices = forwardprop_input_indices
        self._forwardprop_output_indices = None
        self._num_forwardprop_outputs = 0
        self._num_inference_outputs = len(func_graph.outputs)
        self._num_trainable_inference_outputs = len([t for t in func_graph.outputs if backprop_util.IsTrainable(t)])
        self._delayed_rewrite_functions = delayed_rewrite_functions
        self._need_gradients_for_jvps = need_gradients_for_jvps

    def _build_functions_for_outputs(self, outputs, inference_args, input_tangents):
        """Forward+backward functions where the backward function sees `outputs`."""
        trainable_outputs = []
        trainable_indices = []
        for index, output in enumerate(outputs):
            if backprop_util.IsTrainable(output):
                trainable_outputs.append(output)
                trainable_indices.append(index)
        backwards_graph = func_graph_module.FuncGraph(_backward_name(self._func_graph.name))
        with backwards_graph.as_default():
            gradients_wrt_outputs = []
            for output in trainable_outputs:
                gradient_shape, gradient_dtype = default_gradient.shape_and_dtype(output)
                gradient_placeholder = graph_placeholder(gradient_dtype, gradient_shape)
                handle_data_util.copy_handle_data(output, gradient_placeholder)
                gradients_wrt_outputs.append(gradient_placeholder)
            with ops.device(None):
                gradients_wrt_inputs = gradients_util._GradientsHelper(trainable_outputs, self._func_graph.inputs, grad_ys=gradients_wrt_outputs, src_graph=self._func_graph)
            if input_tangents:
                gradients_wrt_inputs = nest.map_structure(lambda x: ops.convert_to_tensor(x) if x is not None else None, gradients_wrt_inputs)
            captures_from_forward = [c for c in backwards_graph.external_captures if not isinstance(c, ops.EagerTensor) and c.graph is self._func_graph]
            existing_outputs = object_identity.ObjectIdentitySet(self._func_graph.outputs)
            for capture in captures_from_forward:
                if capture not in existing_outputs:
                    existing_outputs.add(capture)
                    self._func_graph.outputs.append(capture)
        backwards_graph.inputs = gradients_wrt_outputs + backwards_graph.internal_captures
        backwards_graph.outputs.extend((grad for grad in nest.flatten(gradients_wrt_inputs, expand_composites=True) if grad is not None))
        backwards_graph.structured_outputs = gradients_wrt_inputs
        forward_function, backward_function = _create_forward_backward_with_graph(self._attrs, self._func_graph, backwards_graph)
        if not input_tangents:
            return (forward_function, self._func_graph, backward_function, None, 0)
        forward_wrapper = self._wrap_forward_function_with_jvps(forward_function, backward_function, inference_args, input_tangents)
        wrapped_backwards_graph, forward_wrapper = self._wrap_backward_function_with_jvp_backprop(backward_function, gradients_wrt_outputs, forward_wrapper)
        forward_wrapper = self._shuffle_forward_outputs(forward_wrapper)
        wrapped_forward_function, wrapped_backward_function = _create_forward_backward_with_graph(self._attrs, forward_wrapper.graph, wrapped_backwards_graph)
        if len(inference_args) + len(input_tangents) != len(forward_wrapper.graph.inputs):
            raise errors.InternalError(f'The forward graph had {len(forward_wrapper.graph.inputs)} inputs, but we expected {len(inference_args) + len(input_tangents)} ({len(inference_args)} inference inputs and {len(input_tangents)} input tangents).')
        return (wrapped_forward_function, forward_wrapper.graph, wrapped_backward_function, forward_wrapper.output_indices, len(forward_wrapper.output_tangents))

    def _wrap_forward_function_with_jvps(self, forward_function, backward_function, inference_args, input_tangents):
        """Adds inline JVP computation to a forward function."""
        forward_wrapper_graph = func_graph_module.FuncGraph(_forward_name(self._func_graph.name))
        with forward_wrapper_graph.as_default():
            with forwardprop_util.push_forwardprop_state():
                forward_captures = {ops.tensor_id(internal): external for external, internal in self._func_graph.captures}
                for input_index, real_input in enumerate(self._func_graph.inputs):
                    input_placeholder = array_ops.placeholder(dtype=real_input.dtype, shape=real_input.shape)
                    capture = forward_captures.get(ops.tensor_id(real_input))
                    if capture is not None:
                        forward_wrapper_graph.add_capture(capture, input_placeholder)
                        if capture.dtype == dtypes.resource:
                            handle_data_util.copy_handle_data(capture, input_placeholder)
                    else:
                        forward_wrapper_graph.inputs.append(input_placeholder)
                for inp, arg in zip(forward_wrapper_graph.inputs, inference_args):
                    record.record_operation('captured_value', [inp], [arg], backward_function=lambda x: [x], forward_function=lambda x: [x])
                num_inference_inputs = len(inference_args)
                for tape_indices in self._forwardprop_input_indices:
                    for input_index, jvp_index in tape_indices:
                        input_placeholder = forward_wrapper_graph.inputs[input_index]
                        if len(forward_wrapper_graph.inputs) != jvp_index:
                            raise errors.InternalError(f'Expected {jvp_index} forward graph inputs, got {len(forward_wrapper_graph.inputs)}.')
                        gradient_shape, gradient_dtype = default_gradient.shape_and_dtype(input_placeholder)
                        jvp_placeholder = graph_placeholder(gradient_dtype, gradient_shape)
                        external_jvp = input_tangents[jvp_index - num_inference_inputs]
                        forward_wrapper_graph.add_capture(external_jvp, jvp_placeholder)
                        tensor_shape.TensorShape(external_jvp.shape).assert_is_compatible_with(jvp_placeholder.shape)
                        record.record_operation('captured_value', [jvp_placeholder], [external_jvp], backward_function=lambda x: [x], forward_function=lambda x: [x])
                forward_inputs = forward_wrapper_graph.inputs[:num_inference_inputs]
                gradient_function = self._delayed_rewrite_functions._rewrite_forward_and_call_backward
                with ops.get_default_graph()._override_gradient_function({'PartitionedCall': gradient_function, 'StatefulPartitionedCall': gradient_function}):
                    forward_outputs = forward_function(*forward_inputs)
                    if isinstance(forward_outputs, ops.Operation):
                        forward_outputs = []
                py_backward, _ = self._wrap_backward_function(self._func_graph, backward_function, forward_outputs)
            record.record_operation_forwardprop_only(forward_function.cached_definition.signature.name, forward_outputs, forward_inputs, py_backward, None)
            output_indices, output_tangents = pywrap_tfe.TFE_Py_PackJVPs(forward_outputs)
            output_tangents = [forward_wrapper_graph.capture(t) for t in output_tangents]
        return _ForwardWrapper(graph=forward_wrapper_graph, outputs=forward_outputs, output_indices=output_indices, output_tangents=output_tangents)

    def _wrap_backward_function_with_jvp_backprop(self, backward_function, gradients_wrt_outputs, forward_wrapper):
        """Wraps `backward_function` to include gradients for JVPs."""
        wrapped_backwards_graph = func_graph_module.FuncGraph(_backward_name(self._func_graph.name))
        with wrapped_backwards_graph.as_default():
            py_backward, recorded_outputs = self._wrap_backward_function(self._func_graph, backward_function, forward_wrapper.outputs)
            trainable_index = 0
            forward_doutputs = []
            doutput_args = []
            for output in recorded_outputs:
                if backprop_util.IsTrainable(output):
                    doutput = gradients_wrt_outputs[trainable_index]
                    doutput_placeholder = graph_placeholder(doutput.dtype, doutput.shape)
                    doutput_args.append(doutput_placeholder)
                    forward_doutputs.append(doutput_placeholder)
                    trainable_index += 1
                else:
                    doutput_args.append(None)
            dinputs = py_backward(*doutput_args)
            existing_outputs = object_identity.ObjectIdentitySet(forward_wrapper.outputs + forward_wrapper.output_tangents)
            num_processed_output_tangents = 0
            gradients_wrt_output_tangents = []
            tangent_doutputs = []
            output_tangents = forward_wrapper.output_tangents
            output_indices = forward_wrapper.output_indices
            if self._need_gradients_for_jvps:
                while num_processed_output_tangents != len(output_tangents):
                    for output in output_tangents[num_processed_output_tangents:]:
                        gradient_shape, gradient_dtype = default_gradient.shape_and_dtype(output)
                        placeholder = graph_placeholder(gradient_dtype, gradient_shape)
                        gradients_wrt_output_tangents.append(placeholder)
                        tangent_doutputs.append(placeholder)
                    num_processed_output_tangents = len(output_tangents)
                    with ops.device(None):
                        gradients_wrt_inputs = gradients_util._GradientsHelper(output_tangents, forward_wrapper.graph.inputs, grad_ys=gradients_wrt_output_tangents, src_graph=forward_wrapper.graph)
                    dinputs = [backprop_util.AggregateIndexedSlicesGradients((existing, new)) for existing, new in zip(dinputs, gradients_wrt_inputs) if existing is not None or new is not None]
                    dinputs.extend(gradients_wrt_inputs[len(dinputs):])
                    captures_from_forward = [c for c in wrapped_backwards_graph.external_captures if not isinstance(c, ops.EagerTensor) and c.graph is forward_wrapper.graph]
                    for capture in captures_from_forward:
                        if capture not in existing_outputs:
                            existing_outputs.add(capture)
                            forward_wrapper.outputs.append(capture)
                    output_indices, output_tangents = forwardprop_util.pack_tangents(forward_wrapper.outputs)
                    output_tangents = [forward_wrapper.graph.capture(t) for t in output_tangents]
                    for t in output_tangents:
                        existing_outputs.add(t)
        wrapped_backwards_graph.inputs = forward_doutputs[:self._num_trainable_inference_outputs] + tangent_doutputs + forward_doutputs[self._num_trainable_inference_outputs:] + wrapped_backwards_graph.internal_captures
        wrapped_backwards_graph.structured_outputs = dinputs
        wrapped_backwards_graph.outputs = [t for t in dinputs if t is not None]
        return (wrapped_backwards_graph, forward_wrapper._replace(output_indices=output_indices, output_tangents=output_tangents))

    def _shuffle_forward_outputs(self, forward_wrapper):
        """Reorders function outputs so captures are last."""

        def _index_map(original):
            if original < self._num_inference_outputs:
                return original
            if original >= len(forward_wrapper.outputs):
                return original - len(forward_wrapper.outputs) + self._num_inference_outputs
            return original + len(forward_wrapper.output_tangents)
        output_indices = nest.map_structure(_index_map, forward_wrapper.output_indices)
        forward_wrapper.graph.outputs = forward_wrapper.outputs[:self._num_inference_outputs] + forward_wrapper.output_tangents + forward_wrapper.outputs[self._num_inference_outputs:]
        return forward_wrapper._replace(output_indices=output_indices)

    def forward(self, inference_args, input_tangents):
        """Construct or fetch a forward function with side-outputs.

    When graph building without a tape active, symbolic gradients rely on
    regenerating the backward function for higher-order gradients (to account
    for new side outputs of the rewritten forward function call). Thus there is
    no fixed backward function for this case. However, when a tape is active
    (eager or graph building), we generate fixed backward and forward functions
    at forward function call time.

    This difference between the tape and non-tape cases is to avoid building
    unneeded backward functions while graph building (where we may or may not
    eventually need gradients).

    Args:
      inference_args: A flat list of Tensors, arguments to the inference
        function.
      input_tangents: A flat list of Tensors, jvps associated with
        `inference_args`.

    Returns:
      A forward atomic_function.AtomicFunction.
    """
        if self._forward is None:
            self._forward, self._forward_graph, self._backward, self._forwardprop_output_indices, self._num_forwardprop_outputs = self._forward_and_backward_functions(inference_args, input_tangents)
        return self._forward

    def _wrap_backward_function(self, forward_graph, backward, outputs):
        """Create a backward function given `outputs` from the forward function."""
        capture_mapping = dict(zip((ops.tensor_id(t) for t in forward_graph.outputs), outputs))
        captured_inputs = backward.captured_inputs
        remapped_captures = [capture_mapping.get(ops.tensor_id(capture), capture) for capture in captured_inputs]
        if any((t.graph is forward_graph for t in remapped_captures if not isinstance(t, ops.EagerTensor))):
            incorrect_mapping = [t for t in remapped_captures if not isinstance(t, ops.EagerTensor) and t.graph is not forward_graph]
            raise errors.InternalError(f'Failed to map all backward graph captures to the forward graph. Incorrectly mapped: {incorrect_mapping}.')
        variant_zeros_like = {}
        backward_function_inputs = len(backward.inputs) - len(captured_inputs)
        recorded_outputs = []
        trainable_recorded_outputs = 0
        skip_positions = []
        if self._num_forwardprop_outputs and (not self._need_gradients_for_jvps):
            relevant_outputs = outputs[:self._num_inference_outputs] + outputs[self._num_inference_outputs + self._num_forwardprop_outputs:]
        else:
            relevant_outputs = outputs
        for output_index, output in enumerate(relevant_outputs):
            if trainable_recorded_outputs < backward_function_inputs:
                recorded_outputs.append(output)
            if backprop_util.IsTrainable(output):
                trainable_recorded_outputs += 1
            else:
                skip_positions.append(output_index)
            if output.dtype == dtypes.variant:
                variant_zeros_like[output_index] = default_gradient.zeros_like(output)

        def _backward_function_wrapper(*args):
            """Process output gradients and call the backward function."""
            if not backward.outputs:
                return backward.structured_outputs
            processed_args = []
            input_index = 0
            for output_index, arg in enumerate(args):
                if isinstance(arg, indexed_slices.IndexedSlices):
                    arg = ops.convert_to_tensor(arg)
                if output_index in skip_positions:
                    continue
                if arg is None:
                    input_placeholder = backward.inputs[input_index]
                    if input_placeholder.dtype == dtypes.variant:
                        arg = variant_zeros_like[output_index]
                    else:
                        arg = array_ops.zeros(*default_gradient.shape_and_dtype(input_placeholder))
                processed_args.append(arg)
                input_index += 1
                if input_index >= backward_function_inputs:
                    break
            return backward._call_flat(processed_args, remapped_captures)
        return (_backward_function_wrapper, recorded_outputs)

    def record(self, flat_outputs, inference_args, input_tangents):
        """Record the function call operation.

    For backprop, indicates the backward function to use and which new Tensors
    must be watched. For forwardprop from eager, the function call itself will
    have produced tangents which need to be recorded.

    Args:
      flat_outputs: The result of running `forward`.
      inference_args: A flat list of Tensors with inference inputs to the
        operation.
      input_tangents: A flat list of Tensors with input tangents consumed by the
        operation.
    """
        backward_function, to_record = self._wrap_backward_function(self._forward_graph, self._backward, flat_outputs)
        if self._forwardprop_output_indices:
            record.record_operation_backprop_only(self._forward.cached_definition.signature.name, to_record, inference_args, backward_function)
            record.record_operation_forwardprop_only(self._forward.cached_definition.signature.name, flat_outputs, inference_args + input_tangents, backward_function, self._forwardprop_output_indices)
        else:
            record.record_operation(self._forward.cached_definition.signature.name, to_record, inference_args + input_tangents, backward_function)