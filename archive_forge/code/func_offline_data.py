import copy
import logging
import math
import os
import sys
from typing import (
from packaging import version
import ray
from ray.rllib.algorithms.callbacks import DefaultCallbacks
from ray.rllib.core.learner.learner import LearnerHyperparameters
from ray.rllib.core.learner.learner_group_config import LearnerGroupConfig, ModuleSpec
from ray.rllib.core.rl_module.marl_module import MultiAgentRLModuleSpec
from ray.rllib.core.rl_module.rl_module import ModuleID, SingleAgentRLModuleSpec
from ray.rllib.core.learner.learner import TorchCompileWhatToCompile
from ray.rllib.env.env_context import EnvContext
from ray.rllib.env.multi_agent_env import MultiAgentEnv
from ray.rllib.env.wrappers.atari_wrappers import is_atari
from ray.rllib.evaluation.collectors.sample_collector import SampleCollector
from ray.rllib.evaluation.collectors.simple_list_collector import SimpleListCollector
from ray.rllib.evaluation.rollout_worker import RolloutWorker
from ray.rllib.models import MODEL_DEFAULTS
from ray.rllib.policy.policy import Policy, PolicySpec
from ray.rllib.policy.sample_batch import DEFAULT_POLICY_ID
from ray.rllib.utils import deep_update, merge_dicts
from ray.rllib.utils.annotations import (
from ray.rllib.utils.deprecation import (
from ray.rllib.utils.framework import try_import_tf, try_import_torch
from ray.rllib.utils.from_config import NotProvided, from_config
from ray.rllib.utils.gym import (
from ray.rllib.utils.policy import validate_policy_id
from ray.rllib.utils.schedules.scheduler import Scheduler
from ray.rllib.utils.serialization import (
from ray.rllib.utils.torch_utils import TORCH_COMPILE_REQUIRED_VERSION
from ray.rllib.utils.typing import (
from ray.tune.logger import Logger
from ray.tune.registry import get_trainable_cls
from ray.tune.result import TRIAL_INFO
from ray.tune.tune import _Config
def offline_data(self, *, input_=NotProvided, input_config=NotProvided, actions_in_input_normalized=NotProvided, input_evaluation=NotProvided, postprocess_inputs=NotProvided, shuffle_buffer_size=NotProvided, output=NotProvided, output_config=NotProvided, output_compress_columns=NotProvided, output_max_file_size=NotProvided, offline_sampling=NotProvided) -> 'AlgorithmConfig':
    """Sets the config's offline data settings.

        Args:
            input_: Specify how to generate experiences:
                - "sampler": Generate experiences via online (env) simulation (default).
                - A local directory or file glob expression (e.g., "/tmp/*.json").
                - A list of individual file paths/URIs (e.g., ["/tmp/1.json",
                "s3://bucket/2.json"]).
                - A dict with string keys and sampling probabilities as values (e.g.,
                {"sampler": 0.4, "/tmp/*.json": 0.4, "s3://bucket/expert.json": 0.2}).
                - A callable that takes an `IOContext` object as only arg and returns a
                ray.rllib.offline.InputReader.
                - A string key that indexes a callable with tune.registry.register_input
            input_config: Arguments that describe the settings for reading the input.
                If input is `sample`, this will be environment configuation, e.g.
                `env_name` and `env_config`, etc. See `EnvContext` for more info.
                If the input is `dataset`, this will be e.g. `format`, `path`.
            actions_in_input_normalized: True, if the actions in a given offline "input"
                are already normalized (between -1.0 and 1.0). This is usually the case
                when the offline file has been generated by another RLlib algorithm
                (e.g. PPO or SAC), while "normalize_actions" was set to True.
            postprocess_inputs: Whether to run postprocess_trajectory() on the
                trajectory fragments from offline inputs. Note that postprocessing will
                be done using the *current* policy, not the *behavior* policy, which
                is typically undesirable for on-policy algorithms.
            shuffle_buffer_size: If positive, input batches will be shuffled via a
                sliding window buffer of this number of batches. Use this if the input
                data is not in random enough order. Input is delayed until the shuffle
                buffer is filled.
            output: Specify where experiences should be saved:
                 - None: don't save any experiences
                 - "logdir" to save to the agent log dir
                 - a path/URI to save to a custom output directory (e.g., "s3://bckt/")
                 - a function that returns a rllib.offline.OutputWriter
            output_config: Arguments accessible from the IOContext for configuring
                custom output.
            output_compress_columns: What sample batch columns to LZ4 compress in the
                output data.
            output_max_file_size: Max output file size (in bytes) before rolling over
                to a new file.
            offline_sampling: Whether sampling for the Algorithm happens via
                reading from offline data. If True, EnvRunners will NOT limit the
                number of collected batches within the same `sample()` call based on
                the number of sub-environments within the worker (no sub-environments
                present).

        Returns:
            This updated AlgorithmConfig object.
        """
    if input_ is not NotProvided:
        self.input_ = input_
    if input_config is not NotProvided:
        if not isinstance(input_config, dict):
            raise ValueError(f'input_config must be a dict, got {type(input_config)}.')
        msg = '{} should not be set in the input_config. RLlib will use {} instead.'
        if input_config.get('num_cpus_per_read_task') is not None:
            raise ValueError(msg.format('num_cpus_per_read_task', 'config.resources(num_cpus_per_worker=..)'))
        if input_config.get('parallelism') is not None:
            if self.in_evaluation:
                raise ValueError(msg.format('parallelism', 'config.evaluation(evaluation_num_workers=..)'))
            else:
                raise ValueError(msg.format('parallelism', 'config.rollouts(num_rollout_workers=..)'))
        self.input_config = input_config
    if actions_in_input_normalized is not NotProvided:
        self.actions_in_input_normalized = actions_in_input_normalized
    if input_evaluation is not NotProvided:
        deprecation_warning(old='offline_data(input_evaluation={})'.format(input_evaluation), new='evaluation(off_policy_estimation_methods={})'.format(input_evaluation), error=True, help='Running OPE during training is not recommended.')
    if postprocess_inputs is not NotProvided:
        self.postprocess_inputs = postprocess_inputs
    if shuffle_buffer_size is not NotProvided:
        self.shuffle_buffer_size = shuffle_buffer_size
    if output is not NotProvided:
        self.output = output
    if output_config is not NotProvided:
        self.output_config = output_config
    if output_compress_columns is not NotProvided:
        self.output_compress_columns = output_compress_columns
    if output_max_file_size is not NotProvided:
        self.output_max_file_size = output_max_file_size
    if offline_sampling is not NotProvided:
        self.offline_sampling = offline_sampling
    return self