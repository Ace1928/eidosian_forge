# Comprehensive and detailed definition of activation functions using PyTorch operations, encapsulated in lambda expressions
        self.activation_types = {
            "ReLU": lambda x: torch.relu(torch.tensor(x, dtype=torch.complex64)).item(),
            "Sigmoid": lambda x: torch.sigmoid(torch.tensor(x, dtype=torch.complex64)).item(),
            "Tanh": lambda x: torch.tanh(torch.tensor(x, dtype=torch.complex64)).item(),
            "Softmax": lambda x: torch.softmax(torch.tensor([x], dtype=torch.complex64), dim=0).tolist(),
            "Linear": lambda x: x,
            "ELU": lambda x: torch.nn.functional.elu(torch.tensor(x, dtype=torch.complex64)).item(),
            "Swish": lambda x: x * torch.sigmoid(torch.tensor(x, dtype=torch.complex64)).item(),
            "Leaky ReLU": lambda x: torch.nn.functional.leaky_relu(torch.tensor(x, dtype=torch.complex64), negative_slope=0.01).item(),
            "Parametric ReLU": lambda x, a=0.01: torch.nn.functional.prelu(torch.tensor([x], dtype=torch.complex64), torch.tensor([a])).item(),
            "ELU-PA": lambda x, a=0.01: torch.nn.functional.elu(torch.tensor(x, dtype=torch.complex64), alpha=a).item(),
            "GELU": lambda x: torch.nn.functional.gelu(torch.tensor(x, dtype=torch.complex64)).item(),
            "Softplus": lambda x: torch.nn.functional.softplus(torch.tensor(x, dtype=torch.complex64)).item(),
            "Softsign": lambda x: torch.nn.functional.softsign(torch.tensor(x, dtype=torch.complex64)).item(),
            "Bent Identity": lambda x: ((torch.sqrt(torch.tensor(x, dtype=torch.complex64) ** 2 + 1) - 1) / 2 + x).item(),
            "Hard Sigmoid": lambda x: torch.nn.functional.hardsigmoid(torch.tensor(x, dtype=torch.complex64)).item(),
            "Mish": lambda x: x * torch.tanh(torch.nn.functional.softplus(torch.tensor(x, dtype=torch.complex64))).item(),
            "SELU": lambda x: torch.nn.functional.selu(torch.tensor(x, dtype=torch.complex64)).item(),
            "SiLU": lambda x: x * torch.sigmoid(torch.tensor(x, dtype=torch.complex64)).item(),
            "Softshrink": lambda x: torch.nn.functional.softshrink(torch.tensor(x, dtype=torch.complex64)).item(),
            "Threshold": lambda x, threshold=0.1, value=0: torch.nn.functional.threshold(torch.tensor(x, dtype=torch.complex64), threshold, value).item(),
            "LogSigmoid": lambda x: torch.nn.functional.logsigmoid(torch.tensor(x, dtype=torch.complex64)).item(),
            "Hardtanh": lambda x: torch.nn.functional.hardtanh(torch.tensor(x, dtype=torch.complex64)).item(),
            "ReLU6": lambda x: torch.nn.functional.relu6(torch.tensor(x, dtype=torch.complex64)).item(),
            "RReLU": lambda x: torch.nn.functional.rrelu(torch.tensor(x, dtype=torch.complex64)).item(),
            "PReLU": lambda x, a=0.25: torch.nn.functional.prelu(torch.tensor([x], dtype=torch.complex64), torch.tensor([a])).item(),
            "CReLU": lambda x: torch.cat((torch.nn.functional.relu(torch.tensor(x, dtype=torch.complex64)), torch.nn.functional.relu(-torch.tensor(x, dtype=torch.complex64)))).item(),
            "ELiSH": lambda x: (torch.sign(torch.tensor(x, dtype=torch.complex64)) * (torch.nn.functional.elu(abs(torch.tensor(x, dtype=torch.complex64))) + 1) / 2).item(),
            "Hardshrink": lambda x: torch.nn.functional.hardshrink(torch.tensor(x, dtype=torch.complex64)).item(),
            "LogSoftmax": lambda x: torch.nn.functional.log_softmax(torch.tensor([x], dtype=torch.complex64), dim=0).tolist(),
            "Softmin": lambda x: torch.nn.functional.softmin(torch.tensor([x], dtype=torch.complex64), dim=0).tolist(),
            "Tanhshrink": lambda x: torch.nn.functional.tanhshrink(torch.tensor(x, dtype=torch.complex64)).item(),
            "LReLU": lambda x: torch.nn.functional.leaky_relu(torch.tensor(x, dtype=torch.complex64), negative_slope=0.05).item(),
            "AReLU": lambda x, a=0.1: torch.nn.functional.rrelu(torch.tensor(x, dtype=torch.complex64), lower=a, upper=a).item(),
            "Maxout": lambda x: torch.max(torch.tensor(x, dtype=torch.complex64)).item()
        }
