from typing import Any, Dict, Mapping, Tuple
import gymnasium as gym
from ray.rllib.algorithms.dreamerv3.dreamerv3_learner import (
from ray.rllib.core.rl_module.marl_module import ModuleID
from ray.rllib.core.learner.learner import ParamDict
from ray.rllib.core.learner.tf.tf_learner import TfLearner
from ray.rllib.policy.sample_batch import DEFAULT_POLICY_ID, SampleBatch
from ray.rllib.utils.annotations import override
from ray.rllib.utils.framework import try_import_tf, try_import_tfp
from ray.rllib.utils.tf_utils import symlog, two_hot, clip_gradients
from ray.rllib.utils.typing import TensorType
def _compute_actor_loss(self, *, module_id: ModuleID, hps: DreamerV3LearnerHyperparameters, dream_data: Dict[str, TensorType], value_targets_t0_to_Hm1_BxT: TensorType) -> TensorType:
    """Helper method computing the actor's loss terms.

        Args:
            module_id: The module_id for which to compute the actor loss.
            hps: The DreamerV3LearnerHyperparameters to use.
            dream_data: The data generated by dreaming for H steps (horizon) starting
                from any BxT state (sampled from the buffer for the train batch).
            value_targets_t0_to_Hm1_BxT: The computed value function targets of the
                shape (t0 to H-1, BxT).

        Returns:
            The total actor loss tensor.
        """
    actor = self.module[module_id].actor
    scaled_value_targets_t0_to_Hm1_B = self._compute_scaled_value_targets(module_id=module_id, hps=hps, value_targets_t0_to_Hm1_BxT=value_targets_t0_to_Hm1_BxT, value_predictions_t0_to_Hm1_BxT=dream_data['values_dreamed_t0_to_H_BxT'][:-1])
    actions_dreamed = tf.stop_gradient(dream_data['actions_dreamed_t0_to_H_BxT'])[:-1]
    actions_dreamed_dist_params_t0_to_Hm1_B = dream_data['actions_dreamed_dist_params_t0_to_H_BxT'][:-1]
    dist_t0_to_Hm1_B = actor.get_action_dist_object(actions_dreamed_dist_params_t0_to_Hm1_B)
    if isinstance(self.module[module_id].actor.action_space, gym.spaces.Discrete):
        logp_actions_t0_to_Hm1_B = actions_dreamed_dist_params_t0_to_Hm1_B
        logp_actions_dreamed_t0_to_Hm1_B = tf.reduce_sum(actions_dreamed * logp_actions_t0_to_Hm1_B, axis=-1)
        logp_loss_H_B = logp_actions_dreamed_t0_to_Hm1_B * tf.stop_gradient(scaled_value_targets_t0_to_Hm1_B)
    else:
        logp_actions_dreamed_t0_to_Hm1_B = dist_t0_to_Hm1_B.log_prob(actions_dreamed)
        logp_loss_H_B = scaled_value_targets_t0_to_Hm1_B
    assert len(logp_loss_H_B.shape) == 2
    entropy_H_B = dist_t0_to_Hm1_B.entropy()
    assert len(entropy_H_B.shape) == 2
    entropy = tf.reduce_mean(entropy_H_B)
    L_actor_reinforce_term_H_B = -logp_loss_H_B
    L_actor_action_entropy_term_H_B = -hps.entropy_scale * entropy_H_B
    L_actor_H_B = L_actor_reinforce_term_H_B + L_actor_action_entropy_term_H_B
    L_actor_H_B *= tf.stop_gradient(dream_data['dream_loss_weights_t0_to_H_BxT'])[:-1]
    L_actor = tf.reduce_mean(L_actor_H_B)
    self.register_metrics(module_id, metrics_dict={'ACTOR_L_total': L_actor, 'ACTOR_value_targets_pct95_ema': actor.ema_value_target_pct95, 'ACTOR_value_targets_pct5_ema': actor.ema_value_target_pct5, 'ACTOR_action_entropy': entropy, 'ACTOR_L_neglogp_reinforce_term': tf.reduce_mean(L_actor_reinforce_term_H_B), 'ACTOR_L_neg_entropy_term': tf.reduce_mean(L_actor_action_entropy_term_H_B)})
    if hps.report_individual_batch_item_stats:
        self.register_metrics(module_id, metrics_dict={'ACTOR_L_total_H_BxT': L_actor_H_B, 'ACTOR_logp_actions_dreamed_H_BxT': logp_actions_dreamed_t0_to_Hm1_B, 'ACTOR_scaled_value_targets_H_BxT': scaled_value_targets_t0_to_Hm1_B, 'ACTOR_action_entropy_H_BxT': entropy_H_B, 'ACTOR_L_neglogp_reinforce_term_H_BxT': L_actor_reinforce_term_H_B, 'ACTOR_L_neg_entropy_term_H_BxT': L_actor_action_entropy_term_H_B})
    return L_actor