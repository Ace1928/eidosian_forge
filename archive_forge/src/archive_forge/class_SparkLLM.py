from __future__ import annotations
import base64
import hashlib
import hmac
import json
import logging
import queue
import threading
from datetime import datetime
from queue import Queue
from time import mktime
from typing import Any, Dict, Generator, Iterator, List, Optional
from urllib.parse import urlencode, urlparse, urlunparse
from wsgiref.handlers import format_date_time
from langchain_core.callbacks import CallbackManagerForLLMRun
from langchain_core.language_models.llms import LLM
from langchain_core.outputs import GenerationChunk
from langchain_core.pydantic_v1 import Field, root_validator
from langchain_core.utils import get_from_dict_or_env
class SparkLLM(LLM):
    """iFlyTek Spark large language model.

    To use, you should pass `app_id`, `api_key`, `api_secret`
    as a named parameter to the constructor OR set environment
    variables ``IFLYTEK_SPARK_APP_ID``, ``IFLYTEK_SPARK_API_KEY`` and
    ``IFLYTEK_SPARK_API_SECRET``

    Example:
        .. code-block:: python

        client = SparkLLM(
            spark_app_id="<app_id>",
            spark_api_key="<api_key>",
            spark_api_secret="<api_secret>"
        )
    """
    client: Any = None
    spark_app_id: Optional[str] = None
    spark_api_key: Optional[str] = None
    spark_api_secret: Optional[str] = None
    spark_api_url: Optional[str] = None
    spark_llm_domain: Optional[str] = None
    spark_user_id: str = 'lc_user'
    streaming: bool = False
    request_timeout: int = 30
    temperature: float = 0.5
    top_k: int = 4
    model_kwargs: Dict[str, Any] = Field(default_factory=dict)

    @root_validator()
    def validate_environment(cls, values: Dict) -> Dict:
        values['spark_app_id'] = get_from_dict_or_env(values, 'spark_app_id', 'IFLYTEK_SPARK_APP_ID')
        values['spark_api_key'] = get_from_dict_or_env(values, 'spark_api_key', 'IFLYTEK_SPARK_API_KEY')
        values['spark_api_secret'] = get_from_dict_or_env(values, 'spark_api_secret', 'IFLYTEK_SPARK_API_SECRET')
        values['spark_api_url'] = get_from_dict_or_env(values, 'spark_api_url', 'IFLYTEK_SPARK_API_URL', 'wss://spark-api.xf-yun.com/v3.1/chat')
        values['spark_llm_domain'] = get_from_dict_or_env(values, 'spark_llm_domain', 'IFLYTEK_SPARK_LLM_DOMAIN', 'generalv3')
        values['model_kwargs']['temperature'] = values['temperature'] or cls.temperature
        values['model_kwargs']['top_k'] = values['top_k'] or cls.top_k
        values['client'] = _SparkLLMClient(app_id=values['spark_app_id'], api_key=values['spark_api_key'], api_secret=values['spark_api_secret'], api_url=values['spark_api_url'], spark_domain=values['spark_llm_domain'], model_kwargs=values['model_kwargs'])
        return values

    @property
    def _llm_type(self) -> str:
        """Return type of llm."""
        return 'spark-llm-chat'

    @property
    def _default_params(self) -> Dict[str, Any]:
        """Get the default parameters for calling SparkLLM API."""
        normal_params = {'spark_llm_domain': self.spark_llm_domain, 'stream': self.streaming, 'request_timeout': self.request_timeout, 'top_k': self.top_k, 'temperature': self.temperature}
        return {**normal_params, **self.model_kwargs}

    def _call(self, prompt: str, stop: Optional[List[str]]=None, run_manager: Optional[CallbackManagerForLLMRun]=None, **kwargs: Any) -> str:
        """Call out to an sparkllm for each generation with a prompt.
        Args:
            prompt: The prompt to pass into the model.
            stop: Optional list of stop words to use when generating.
        Returns:
            The string generated by the llm.

        Example:
            .. code-block:: python
                response = client("Tell me a joke.")
        """
        if self.streaming:
            completion = ''
            for chunk in self._stream(prompt, stop, run_manager, **kwargs):
                completion += chunk.text
            return completion
        completion = ''
        self.client.arun([{'role': 'user', 'content': prompt}], self.spark_user_id, self.model_kwargs, self.streaming)
        for content in self.client.subscribe(timeout=self.request_timeout):
            if 'data' not in content:
                continue
            completion = content['data']['content']
        return completion

    def _stream(self, prompt: str, stop: Optional[List[str]]=None, run_manager: Optional[CallbackManagerForLLMRun]=None, **kwargs: Any) -> Iterator[GenerationChunk]:
        self.client.run([{'role': 'user', 'content': prompt}], self.spark_user_id, self.model_kwargs, self.streaming)
        for content in self.client.subscribe(timeout=self.request_timeout):
            if 'data' not in content:
                continue
            delta = content['data']
            if run_manager:
                run_manager.on_llm_new_token(delta)
            yield GenerationChunk(text=delta['content'])