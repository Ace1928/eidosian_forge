import asyncio
import json
import warnings
from abc import ABC
from typing import (
from langchain_core._api.deprecation import deprecated
from langchain_core.callbacks import (
from langchain_core.language_models.llms import LLM
from langchain_core.outputs import GenerationChunk
from langchain_core.pydantic_v1 import BaseModel, Extra, Field, root_validator
from langchain_core.utils import get_from_dict_or_env
from langchain_community.llms.utils import enforce_stop_tokens
from langchain_community.utilities.anthropic import (
@deprecated(since='0.0.34', removal='0.3', alternative_import='langchain_aws.BedrockLLM')
class Bedrock(LLM, BedrockBase):
    """Bedrock models.

    To authenticate, the AWS client uses the following methods to
    automatically load credentials:
    https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html

    If a specific credential profile should be used, you must pass
    the name of the profile from the ~/.aws/credentials file that is to be used.

    Make sure the credentials / roles used have the required policies to
    access the Bedrock service.
    """
    '\n    Example:\n        .. code-block:: python\n\n            from bedrock_langchain.bedrock_llm import BedrockLLM\n\n            llm = BedrockLLM(\n                credentials_profile_name="default",\n                model_id="amazon.titan-text-express-v1",\n                streaming=True\n            )\n\n    '

    @root_validator()
    def validate_environment(cls, values: Dict) -> Dict:
        model_id = values['model_id']
        if model_id.startswith('anthropic.claude-3'):
            raise ValueError('Claude v3 models are not supported by this LLM.Please use `from langchain_community.chat_models import BedrockChat` instead.')
        return super().validate_environment(values)

    @property
    def _llm_type(self) -> str:
        """Return type of llm."""
        return 'amazon_bedrock'

    @classmethod
    def is_lc_serializable(cls) -> bool:
        """Return whether this model can be serialized by Langchain."""
        return True

    @classmethod
    def get_lc_namespace(cls) -> List[str]:
        """Get the namespace of the langchain object."""
        return ['langchain', 'llms', 'bedrock']

    @property
    def lc_attributes(self) -> Dict[str, Any]:
        attributes: Dict[str, Any] = {}
        if self.region_name:
            attributes['region_name'] = self.region_name
        return attributes

    class Config:
        """Configuration for this pydantic object."""
        extra = Extra.forbid

    def _stream(self, prompt: str, stop: Optional[List[str]]=None, run_manager: Optional[CallbackManagerForLLMRun]=None, **kwargs: Any) -> Iterator[GenerationChunk]:
        """Call out to Bedrock service with streaming.

        Args:
            prompt (str): The prompt to pass into the model
            stop (Optional[List[str]], optional): Stop sequences. These will
                override any stop sequences in the `model_kwargs` attribute.
                Defaults to None.
            run_manager (Optional[CallbackManagerForLLMRun], optional): Callback
                run managers used to process the output. Defaults to None.

        Returns:
            Iterator[GenerationChunk]: Generator that yields the streamed responses.

        Yields:
            Iterator[GenerationChunk]: Responses from the model.
        """
        return self._prepare_input_and_invoke_stream(prompt=prompt, stop=stop, run_manager=run_manager, **kwargs)

    def _call(self, prompt: str, stop: Optional[List[str]]=None, run_manager: Optional[CallbackManagerForLLMRun]=None, **kwargs: Any) -> str:
        """Call out to Bedrock service model.

        Args:
            prompt: The prompt to pass into the model.
            stop: Optional list of stop words to use when generating.

        Returns:
            The string generated by the model.

        Example:
            .. code-block:: python

                response = llm("Tell me a joke.")
        """
        if self.streaming:
            completion = ''
            for chunk in self._stream(prompt=prompt, stop=stop, run_manager=run_manager, **kwargs):
                completion += chunk.text
            return completion
        text, _ = self._prepare_input_and_invoke(prompt=prompt, stop=stop, run_manager=run_manager, **kwargs)
        return text

    async def _astream(self, prompt: str, stop: Optional[List[str]]=None, run_manager: Optional[AsyncCallbackManagerForLLMRun]=None, **kwargs: Any) -> AsyncGenerator[GenerationChunk, None]:
        """Call out to Bedrock service with streaming.

        Args:
            prompt (str): The prompt to pass into the model
            stop (Optional[List[str]], optional): Stop sequences. These will
                override any stop sequences in the `model_kwargs` attribute.
                Defaults to None.
            run_manager (Optional[CallbackManagerForLLMRun], optional): Callback
                run managers used to process the output. Defaults to None.

        Yields:
            AsyncGenerator[GenerationChunk, None]: Generator that asynchronously yields
            the streamed responses.
        """
        async for chunk in self._aprepare_input_and_invoke_stream(prompt=prompt, stop=stop, run_manager=run_manager, **kwargs):
            yield chunk

    async def _acall(self, prompt: str, stop: Optional[List[str]]=None, run_manager: Optional[AsyncCallbackManagerForLLMRun]=None, **kwargs: Any) -> str:
        """Call out to Bedrock service model.

        Args:
            prompt: The prompt to pass into the model.
            stop: Optional list of stop words to use when generating.

        Returns:
            The string generated by the model.

        Example:
            .. code-block:: python

                response = await llm._acall("Tell me a joke.")
        """
        if not self.streaming:
            raise ValueError('Streaming must be set to True for async operations. ')
        chunks = [chunk.text async for chunk in self._astream(prompt=prompt, stop=stop, run_manager=run_manager, **kwargs)]
        return ''.join(chunks)

    def get_num_tokens(self, text: str) -> int:
        if self._model_is_anthropic:
            return get_num_tokens_anthropic(text)
        else:
            return super().get_num_tokens(text)

    def get_token_ids(self, text: str) -> List[int]:
        if self._model_is_anthropic:
            return get_token_ids_anthropic(text)
        else:
            return super().get_token_ids(text)