import hashlib
import os
import tempfile
from ..common import _build
from ..common.backend import get_cuda_version_key
from ..common.build import is_hip
from ..runtime.cache import get_cache_manager
from .utils import generate_cu_signature
def generate_launcher(constants, signature, ids):
    signature, desc_start_idx = generate_cu_signature(constants, signature, ids)
    arg_decls = ', '.join((f'{ty_to_cpp(ty)} arg{i}' for i, ty in signature.items()))

    def _extracted_type(ty):
        if ty[0] == '*':
            return 'PyObject*'
        return {'i1': 'int32_t', 'i32': 'int32_t', 'i64': 'int64_t', 'u32': 'uint32_t', 'u64': 'uint64_t', 'fp16': 'float', 'bf16': 'float', 'fp32': 'float', 'f32': 'float', 'fp64': 'double'}[ty]

    def format_of(ty):
        return {'PyObject*': 'O', 'float': 'f', 'double': 'd', 'long': 'l', 'uint32_t': 'I', 'int32_t': 'i', 'uint64_t': 'K', 'int64_t': 'L'}[ty]
    format = 'iiiiiiiiiKKOOO' + ''.join([format_of(_extracted_type(ty)) for ty in signature.values()])
    folded_without_constexprs = [c for c in ids['ids_of_folded_args'] if c not in ids['ids_of_const_exprs']]
    params = [i for i in signature.keys() if i >= desc_start_idx or (i not in constants and i not in folded_without_constexprs)]
    src = f'\n#include "cuda.h"\n#include <stdbool.h>\n#include <Python.h>\n#include <dlfcn.h>\n\nstatic inline void gpuAssert(CUresult code, const char *file, int line)\n{{\n   if (code != CUDA_SUCCESS)\n   {{\n      const char* prefix = "Triton Error [CUDA]: ";\n      const char* str;\n      cuGetErrorString(code, &str);\n      char err[1024] = {{0}};\n      strcat(err, prefix);\n      strcat(err, str);\n      PyGILState_STATE gil_state;\n      gil_state = PyGILState_Ensure();\n      PyErr_SetString(PyExc_RuntimeError, err);\n      PyGILState_Release(gil_state);\n   }}\n}}\n\n#define CUDA_CHECK(ans) {{ gpuAssert((ans), __FILE__, __LINE__); }}\n\ntypedef CUresult (*cuLaunchKernelEx_t)(const CUlaunchConfig* config, CUfunction f, void** kernelParams, void** extra);\n\nstatic cuLaunchKernelEx_t getLaunchKernelExHandle() {{\n  // Open the shared library\n  void* handle = dlopen("libcuda.so", RTLD_LAZY);\n  if (!handle) {{\n    PyErr_SetString(PyExc_RuntimeError, "Failed to open libcuda.so");\n    return NULL;\n  }}\n  // Clear any existing error\n  dlerror();\n  cuLaunchKernelEx_t cuLaunchKernelExHandle = (cuLaunchKernelEx_t)dlsym(handle, "cuLaunchKernelEx");\n  // Check for errors\n  const char *dlsym_error = dlerror();\n  if (dlsym_error) {{\n    PyErr_SetString(PyExc_RuntimeError, "Failed to retrieve cuLaunchKernelEx from libcuda.so");\n    return NULL;\n  }}\n  return cuLaunchKernelExHandle;\n}}\n\nstatic void _launch(int gridX, int gridY, int gridZ, int num_warps, int num_ctas, int clusterDimX, int clusterDimY, int clusterDimZ, int shared_memory, CUstream stream, CUfunction function{(', ' + arg_decls if len(arg_decls) > 0 else '')}) {{\n  void *params[] = {{ {', '.join((f'&arg{i}' for i in params))} }};\n  if (gridX*gridY*gridZ > 0) {{\n    if (num_ctas == 1) {{\n      CUDA_CHECK(cuLaunchKernel(function, gridX, gridY, gridZ, 32*num_warps, 1, 1, shared_memory, stream, params, 0));\n    }} else {{\n      CUlaunchAttribute launchAttr[2];\n      launchAttr[0].id = CU_LAUNCH_ATTRIBUTE_CLUSTER_DIMENSION;\n      launchAttr[0].value.clusterDim.x = clusterDimX;\n      launchAttr[0].value.clusterDim.y = clusterDimY;\n      launchAttr[0].value.clusterDim.z = clusterDimZ;\n      launchAttr[1].id = CU_LAUNCH_ATTRIBUTE_CLUSTER_SCHEDULING_POLICY_PREFERENCE;\n      launchAttr[1].value.clusterSchedulingPolicyPreference = CU_CLUSTER_SCHEDULING_POLICY_SPREAD;\n      CUlaunchConfig config;\n      config.gridDimX = gridX * clusterDimX;\n      config.gridDimY = gridY * clusterDimY;\n      config.gridDimZ = gridZ * clusterDimZ;\n      config.blockDimX = 32 * num_warps;\n      config.blockDimY = 1;\n      config.blockDimZ = 1;\n      config.sharedMemBytes = shared_memory;\n      config.hStream = stream;\n      config.attrs = launchAttr;\n      config.numAttrs = 2;\n      static cuLaunchKernelEx_t cuLaunchKernelExHandle = NULL;\n      if (cuLaunchKernelExHandle == NULL) {{\n        cuLaunchKernelExHandle = getLaunchKernelExHandle();\n      }}\n      CUDA_CHECK(cuLaunchKernelExHandle(&config, function, params, 0));\n    }}\n  }}\n}}\n\ntypedef struct _DevicePtrInfo {{\n    CUdeviceptr dev_ptr;\n    bool valid;\n}} DevicePtrInfo;\n\nstatic inline DevicePtrInfo getPointer(PyObject *obj, int idx) {{\n  DevicePtrInfo ptr_info;\n  ptr_info.dev_ptr = 0;\n  ptr_info.valid = true;\n  if (PyLong_Check(obj)) {{\n    ptr_info.dev_ptr = PyLong_AsUnsignedLongLong(obj);\n    return ptr_info;\n  }}\n  if (obj == Py_None) {{\n    // valid nullptr\n    return ptr_info;\n  }}\n  PyObject *ptr = PyObject_GetAttrString(obj, "data_ptr");\n  if(ptr){{\n    PyObject *empty_tuple = PyTuple_New(0);\n    PyObject *ret = PyObject_Call(ptr, empty_tuple, NULL);\n    Py_DECREF(empty_tuple);\n    Py_DECREF(ptr);\n    if (!PyLong_Check(ret)) {{\n      PyErr_SetString(PyExc_TypeError, "data_ptr method of Pointer object must return 64-bit int");\n      ptr_info.valid = false;\n      return ptr_info;\n    }}\n    ptr_info.dev_ptr = PyLong_AsUnsignedLongLong(ret);\n    if(!ptr_info.dev_ptr)\n      return ptr_info;\n    uint64_t dev_ptr;\n    int status = cuPointerGetAttribute(&dev_ptr, CU_POINTER_ATTRIBUTE_DEVICE_POINTER, ptr_info.dev_ptr);\n    if (status == CUDA_ERROR_INVALID_VALUE) {{\n        PyErr_Format(PyExc_ValueError,\n                     "Pointer argument (at %d) cannot be accessed from Triton (cpu tensor?)", idx);\n        ptr_info.valid = false;\n    }}\n    ptr_info.dev_ptr = dev_ptr;\n    Py_DECREF(ret);  // Thanks ChatGPT!\n    return ptr_info;\n  }}\n  PyErr_SetString(PyExc_TypeError, "Pointer argument must be either uint64 or have data_ptr method");\n  ptr_info.valid = false;\n  return ptr_info;\n}}\n\nstatic PyObject* launch(PyObject* self, PyObject* args) {{\n  int gridX, gridY, gridZ;\n  uint64_t _stream;\n  uint64_t _function;\n  int num_warps;\n  int num_ctas;\n  int clusterDimX;\n  int clusterDimY;\n  int clusterDimZ;\n  int shared_memory;\n  PyObject *launch_enter_hook = NULL;\n  PyObject *launch_exit_hook = NULL;\n  PyObject *compiled_kernel = NULL;\n  {' '.join([f'{_extracted_type(ty)} _arg{i}; ' for i, ty in signature.items()])}\n  if(!PyArg_ParseTuple(args, "{format}", &gridX, &gridY, &gridZ, &num_warps, &num_ctas, &clusterDimX, &clusterDimY, &clusterDimZ, &shared_memory, &_stream, &_function, &launch_enter_hook, &launch_exit_hook, &compiled_kernel{(', ' + ', '.join((f'&_arg{i}' for i, ty in signature.items())) if len(signature) > 0 else '')})) {{\n    return NULL;\n  }}\n\n  if (launch_enter_hook != Py_None && !PyObject_CallObject(launch_enter_hook, args)) {{\n    return NULL;\n  }}\n\n\n  // raise exception asap\n  {'; '.join([f'DevicePtrInfo ptr_info{i} = getPointer(_arg{i}, {i}); if (!ptr_info{i}.valid) return NULL;' if ty[0] == '*' else '' for i, ty in signature.items()])};\n  Py_BEGIN_ALLOW_THREADS;\n  _launch(gridX, gridY, gridZ, num_warps, num_ctas, clusterDimX, clusterDimY, clusterDimZ, shared_memory, (CUstream)_stream, (CUfunction)_function{(', ' + ', '.join((f'ptr_info{i}.dev_ptr' if ty[0] == '*' else f'_arg{i}' for i, ty in signature.items())) if len(signature) > 0 else '')});\n  Py_END_ALLOW_THREADS;\n  if (PyErr_Occurred()) {{\n    return NULL;\n  }}\n\n  if (launch_exit_hook != Py_None && !PyObject_CallObject(launch_exit_hook, args)) {{\n    return NULL;\n  }}\n\n  // return None\n  Py_INCREF(Py_None);\n  return Py_None;\n}}\n\nstatic PyMethodDef ModuleMethods[] = {{\n  {{"launch", launch, METH_VARARGS, "Entry point for all kernels with this signature"}},\n  {{NULL, NULL, 0, NULL}} // sentinel\n}};\n\nstatic struct PyModuleDef ModuleDef = {{\n  PyModuleDef_HEAD_INIT,\n  "__triton_launcher",\n  NULL, //documentation\n  -1, //size\n  ModuleMethods\n}};\n\nPyMODINIT_FUNC PyInit___triton_launcher(void) {{\n  PyObject *m = PyModule_Create(&ModuleDef);\n  if(m == NULL) {{\n    return NULL;\n  }}\n  PyModule_AddFunctions(m, ModuleMethods);\n  return m;\n}}\n'
    return src