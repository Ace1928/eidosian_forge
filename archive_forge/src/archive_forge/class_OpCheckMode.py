import datetime
import difflib
import functools
import inspect
import json
import os
import re
import tempfile
import threading
import unittest
from typing import Any, Callable, Dict, List, Optional, Tuple, Union
import torch
import torch._dynamo
import torch.utils._pytree as pytree
from torch._dynamo.utils import clone_input
from torch._subclasses.schema_check_mode import SchemaCheckMode
from torch._utils_internal import get_file_path_2
from torch.overrides import TorchFunctionMode
from torch.testing._internal.optests import (
class OpCheckMode(TorchFunctionMode):
    """
    For a given test, OpCheckMode intercepts calls to operators and runs
    test_util(op, args, kwargs) for each intercepted (op, args, kwargs).
    """

    def __init__(self, namespaces: List[str], test_util_name: str, test_util: Callable, failures_dict: 'FailuresDict', test_name: str, failures_dict_path: str):
        self.namespaces = namespaces
        self.test_util = test_util
        self.test_util_name = test_util_name
        self.test_name = test_name
        self.failures_dict = failures_dict
        self.failures_dict_path = failures_dict_path
        self.seen_ops_to_errors = {}

    def maybe_raise_errors_on_exit(self) -> None:
        for qualname in self.seen_ops_to_errors.keys():
            option = self.failures_dict.get_status(qualname, self.test_name)
            if len(self.seen_ops_to_errors[qualname]) == 0:
                if should_update_failures_dict():
                    self.failures_dict.set_status(qualname, self.test_name, 'xsuccess', comment='')
                elif option == 'xfail':
                    raise OpCheckError(f'generate_opcheck_tests: Unexpected success for operator {qualname} on test {self.test_name}. This may mean that you have fixed this test failure. Please rerun the test with PYTORCH_OPCHECK_ACCEPT=1 to automatically update the test runner or manually remove the expected failure in the failure dict at {self.failures_dict_path}For more details, see {GDOC}')
                continue
        failed_ops = []
        for qualname in self.seen_ops_to_errors.keys():
            option = self.failures_dict.get_status(qualname, self.test_name)
            if option != 'xsuccess':
                continue
            if len(self.seen_ops_to_errors[qualname]) == 0:
                continue
            failed_ops.append(qualname)
        if not failed_ops:
            return
        if should_update_failures_dict():
            for op in failed_ops:
                self.failures_dict.set_status(op, self.test_name, 'xfail')
            return
        ex, op, args, kwargs = self.seen_ops_to_errors[failed_ops[0]][0]
        repro_command = generate_repro(self.test_util_name, op, args, kwargs, save_data=should_print_better_repro())
        raise OpCheckError(f'Test generated by `generate_opcheck_tests`, {self.test_name}, failed on operators {failed_ops}. This usually means that the operators are not implemented correctly and may lead to silently incorrect behavior. Set PYTORCH_OPCHECK_PRINT_BETTER_REPRO=1 for a standalone repro, or please see {GDOC} for more recommendations. To reproduce this problem locally, try to run the following:\n{repro_command}') from ex

    def __enter__(self, *args, **kwargs):
        self.prev_is_opcheck_mode = _is_inside_opcheck_mode.value
        self.prev_dynamo_disable = os.environ.get('TORCHDYNAMO_DISABLE', '')
        _is_inside_opcheck_mode.value = True
        os.environ['TORCHDYNAMO_DISABLE'] = '1'
        return super().__enter__(*args, **kwargs)

    def __exit__(self, *args, **kwargs):
        _is_inside_opcheck_mode.value = self.prev_is_opcheck_mode
        os.environ['TORCHDYNAMO_DISABLE'] = self.prev_dynamo_disable
        try:
            self.maybe_raise_errors_on_exit()
            if should_update_failures_dict():
                self.failures_dict.save()
        finally:
            result = super().__exit__(*args, **kwargs)
        return result

    def run_test_util(self, op, args, kwargs):
        try:
            self.test_util(op, args, kwargs, copy_inputs=False)
        except torch._subclasses.fake_tensor.UnsupportedFakeTensorException:
            pass

    def __torch_function__(self, func, types, args=(), kwargs=None):
        kwargs = kwargs if kwargs else {}
        if not isinstance(func, (torch._ops.OpOverloadPacket, torch._ops.OpOverload)):
            return func(*args, **kwargs)
        if torch.jit.is_tracing() or torch.jit.is_scripting() or torch._dynamo.is_compiling():
            return func(*args, **kwargs)
        if isinstance(func, torch._ops.OpOverloadPacket):
            func = resolve_unique_overload_or_throw(func)
        qualname = func.name()
        ns = qualname.split('::')[0]
        if ns not in self.namespaces:
            return func(*args, **kwargs)
        args_c, kwargs_c = deepcopy_tensors((args, kwargs))
        result = func(*args, **kwargs)
        option = self.failures_dict.get_status(qualname, self.test_name)
        if option == 'xsuccess' or option == 'xfail':
            try:
                if qualname not in self.seen_ops_to_errors:
                    self.seen_ops_to_errors[qualname] = []
                self.run_test_util(func, args_c, kwargs_c)
            except Exception as ex:
                if should_print_better_repro():
                    self.seen_ops_to_errors[qualname].append((ex, func, args, kwargs))
                else:
                    self.seen_ops_to_errors[qualname].append((ex, func, None, None))
        elif option == 'skip':
            pass
        return result