from abc import ABC, abstractmethod
from typing import TypeVar, List, Dict, Optional, Tuple, Set, Iterable
import math
from operator import attrgetter
import torch
import torch.nn as nn
import torch.nn.functional as F
from parlai.core.opt import Opt
from parlai.utils.distributed import is_distributed, sync_parameters
from parlai.core.torch_agent import TorchAgent, Batch, Output, DictionaryAgent
from parlai.utils.misc import warn_once
import parlai.utils.logging as logging
from parlai.core.metrics import (
from parlai.utils.fp16 import FP16SafeCrossEntropy
from parlai.utils.torch import (
class TorchGeneratorAgent(TorchAgent, ABC):
    """
    Abstract Generator agent; only meant to be extended.

    TorchGeneratorAgent aims to handle much of the bookkeeping and infrastructure work
    for any generative models, like seq2seq or transformer. It implements the train_step
    and eval_step. The only requirement is that your model *must* implemented the
    interface TorchGeneratorModel interface.
    """

    @classmethod
    def upgrade_opt(cls, opt_from_disk: Opt):
        opt_from_disk = super(TorchGeneratorAgent, cls).upgrade_opt(opt_from_disk)
        if 'inference' not in opt_from_disk:
            assert 'beam_size' in opt_from_disk
            if opt_from_disk['beam_size'] == 1:
                method = 'greedy'
            else:
                method = 'beam'
            opt_from_disk['inference'] = method
            warn_once(f'Old model inference method inferred as {method}')
        if 'beam_blacklist_filename' in opt_from_disk:
            if opt_from_disk['beam_blacklist_filename'] is not None:
                opt_from_disk['beam_block_list_filename'] = opt_from_disk['beam_blacklist_filename']
            del opt_from_disk['beam_blacklist_filename']
        if 'beam_block_full_context' not in opt_from_disk:
            warn_once('Loading model with `--beam-block-full-context false`')
            opt_from_disk['beam_block_full_context'] = False
        return opt_from_disk

    @classmethod
    def add_cmdline_args(cls, argparser):
        """
        Add command line arguments.
        """
        agent = argparser.add_argument_group('Torch Generator Agent')
        agent.add_argument('--beam-size', type=int, default=1, help='Beam size, if 1 then greedy search')
        agent.add_argument('--beam-min-length', type=int, default=1, help='Minimum length of prediction to be generated by the beam search')
        agent.add_argument('--beam-context-block-ngram', type=int, default=-1, help='Size n-grams to block in beam search from the context. val <= 0 implies no blocking')
        agent.add_argument('--beam-block-ngram', type=int, default=-1, help='Size n-grams to block in beam search. val <= 0 implies no blocking')
        agent.add_argument('--beam-block-full-context', type='bool', default=True, help='Block n-grams from the *full* history context. Specify False to block up to m tokens in the past, where m is truncation parameter for agent')
        agent.add_argument('--beam-length-penalty', type=float, default=0.65, help='Applies a length penalty. Set to 0 for no penalty.')
        agent.add_argument('--skip-generation', type='bool', default=False, hidden=True, help='Skip beam search. Useful for speeding up training, if perplexity is the validation metric.')
        agent.add_argument('--inference', choices={'beam', 'greedy', 'topk', 'nucleus', 'delayedbeam'}, default='greedy', help='Generation algorithm')
        agent.add_argument('--topk', type=int, default=10, help='K used in Top K sampling')
        agent.add_argument('--topp', type=float, default=0.9, help='p used in nucleus sampling')
        agent.add_argument('--beam-delay', type=int, default=30, help='used in delayedbeam search')
        agent.add_argument('--beam-block-list-filename', type=str, default=None, help='Load a text file of hard blocks for beam search to never say.')
        agent.add_argument('--temperature', type=float, default=1.0, help='temperature to add during decoding')
        agent.add_argument('--compute-tokenized-bleu', type='bool', default=False, help='if true, compute tokenized bleu scores')
        super(TorchGeneratorAgent, cls).add_cmdline_args(argparser)
        return agent

    def __init__(self, opt: Opt, shared=None):
        init_model, is_finetune = self._get_init_model(opt, shared)
        super().__init__(opt, shared)
        self.beam_size = opt.get('beam_size', 1)
        self.beam_min_length = opt.get('beam_min_length', 1)
        self.beam_block_ngram = opt.get('beam_block_ngram', -1)
        self.beam_context_block_ngram = opt.get('beam_context_block_ngram', -1)
        self.beam_block_full_context = opt.get('beam_block_full_context', False)
        self.temperature = opt.get('temperature', 1.0)
        assert self.temperature > 0, '--temperature must be greater than 0'
        self.output_token_losses = opt.get('verbose', False)
        self.compute_tokenized_bleu = opt.get('compute_tokenized_bleu', False)
        self.beam_block_list: Optional[SearchBlocklist] = None
        if shared:
            states = shared.get('states', {})
            self.beam_block_list = shared.get('beam_block_list')
        else:
            self.criterion = self.build_criterion()
            self.model = self.build_model()
            self.beam_block_list = self._load_beam_block_list()
            if self.model is None or self.criterion is None:
                raise AttributeError('build_model() and build_criterion() need to return the model or criterion')
            if self.use_cuda:
                if self.model_parallel:
                    self.model = PipelineHelper().make_parallel(self.model)
                else:
                    self.model.cuda()
                self.criterion.cuda()
            sync_parameters(self.model)
            train_params = trainable_parameters(self.model)
            total_params = total_parameters(self.model)
            logging.info(f'Total parameters: {total_params:,d} ({train_params:,d} trainable)')
            if self.fp16:
                self.model = self.model.half()
            if init_model is not None:
                logging.info(f'Loading existing model params from {init_model}')
                states = self.load(init_model)
            else:
                states = {}
        if shared is not None:
            if 'optimizer' in shared:
                self.optimizer = shared['optimizer']
        elif self._should_initialize_optimizer():
            self.init_optim([p for p in self.model.parameters() if p.requires_grad], optim_states=states.get('optimizer'), saved_optim_type=states.get('optimizer_type'))
            self.build_lr_scheduler(states, hard_reset=is_finetune)
        if shared is None and is_distributed():
            device_ids = None if self.model_parallel else [self.opt['gpu']]
            self.model = torch.nn.parallel.DistributedDataParallel(self.model, device_ids=device_ids, broadcast_buffers=False)
        self.reset()

    def build_criterion(self):
        """
        Construct and return the loss function.

        By default torch.nn.CrossEntropyLoss.

        If overridden, this model should produce a sum that can be used for a per-token loss.
        """
        if not self.fp16:
            return torch.nn.CrossEntropyLoss(ignore_index=self.NULL_IDX, reduction='none')
        else:
            return FP16SafeCrossEntropy(ignore_index=self.NULL_IDX, reduction='none')

    def _v2t(self, vec):
        """
        Convert token indices to string of tokens.
        """
        new_vec = []
        if hasattr(vec, 'cpu'):
            vec = vec.cpu()
        for i in vec:
            if i == self.END_IDX:
                break
            elif i != self.START_IDX:
                new_vec.append(i)
        return self.dict.vec2txt(new_vec)

    def set_interactive_mode(self, mode, shared=False):
        """
        Turn on interactive mode.
        """
        super().set_interactive_mode(mode, shared)
        if mode:
            self.skip_generation = False
        else:
            self.skip_generation = self.opt.get('skip_generation', False)

    def _dummy_batch(self, batchsize, maxlen):
        """
        Create a dummy batch.

        This is used to preinitialize the cuda buffer, or otherwise force a
        null backward pass after an OOM.

        If your model uses additional inputs beyond text_vec and label_vec,
        you will need to override it to add additional fields.
        """
        text_vec = torch.arange(1, maxlen + 1).clamp(max=3).unsqueeze(0).expand(batchsize, maxlen).cuda()
        label_vec = torch.LongTensor([self.END_IDX, self.NULL_IDX]).unsqueeze(0).expand(batchsize, 2).cuda()
        return Batch(text_vec=text_vec, label_vec=label_vec, text_lengths=[maxlen] * batchsize)

    def _init_cuda_buffer(self, batchsize, maxlen, force=False):
        """
        Pre-initialize CUDA buffer by doing fake forward pass.

        This is also used in distributed mode to force a worker to sync with others.
        """
        if self.use_cuda and (force or not hasattr(self, 'buffer_initialized')):
            try:
                self._control_local_metrics(disabled=True)
                loss = 0 * self.compute_loss(self._dummy_batch(batchsize, maxlen))
                self._control_local_metrics(enabled=True)
                self._temporarily_disable_local_metrics = False
                self.backward(loss)
                self.buffer_initialized = True
            except RuntimeError as e:
                if 'out of memory' in str(e):
                    m = 'CUDA OOM: Lower batch size (-bs) from {} or lower  max sequence length (-tr) from {}'.format(batchsize, maxlen)
                    raise RuntimeError(m)
                else:
                    raise e

    def reset_metrics(self):
        """
        Reset metrics for reporting loss and perplexity.
        """
        super().reset_metrics()

    def share(self):
        """
        Share internal states between parent and child instances.
        """
        shared = super().share()
        shared['beam_block_list'] = self.beam_block_list
        if hasattr(self, 'optimizer'):
            shared['optimizer'] = self.optimizer
        return shared

    def vectorize(self, *args, **kwargs):
        """
        Override vectorize for generative models.
        """
        kwargs['add_start'] = False
        kwargs['add_end'] = True
        return super().vectorize(*args, **kwargs)

    def _model_input(self, batch):
        """
        Create the input (x) value for the model.

        Must return a tuple.  This will be passed directly into the model via
        `*args`, i.e.,

        >>> model(*_model_input(batch))

        This is intentionally overridable so that richer models can pass the
        additional inputs.
        """
        return (batch.text_vec,)

    def _encoder_input(self, batch):
        """
        Create the input (x) value for the encoder.

        Must return a tuple.  This will be passed directly into the encoder via
        `*args`, i.e.,

        >>> model.encoder(*_encoder_input(batch))

        This is intentionally overridable so that richer models can pass the
        additional inputs directly to the encoder.
        """
        return self._model_input(batch)

    def compute_loss(self, batch, return_output=False):
        """
        Compute and return the loss for the given batch.

        Easily overridable for customized loss functions.

        If return_output is True, the full output from the call to self.model()
        is also returned, via a (loss, model_output) pair.
        """
        if batch.label_vec is None:
            raise ValueError('Cannot compute loss without a label.')
        model_output = self.model(*self._model_input(batch), ys=batch.label_vec)
        scores, preds, *_ = model_output
        score_view = scores.view(-1, scores.size(-1))
        loss = self.criterion(score_view, batch.label_vec.view(-1))
        loss = loss.view(scores.shape[:-1]).sum(dim=1)
        notnull = batch.label_vec.ne(self.NULL_IDX)
        target_tokens = notnull.long().sum(dim=-1)
        correct = ((batch.label_vec == preds) * notnull).sum(dim=-1)
        self.record_local_metric('loss', AverageMetric.many(loss, target_tokens))
        self.record_local_metric('ppl', PPLMetric.many(loss, target_tokens))
        self.record_local_metric('token_acc', AverageMetric.many(correct, target_tokens))
        loss = loss.sum()
        loss /= target_tokens.sum()
        if return_output:
            return (loss, model_output)
        else:
            return loss

    def train_step(self, batch):
        """
        Train on a single batch of examples.
        """
        self._init_cuda_buffer(self.opt['batchsize'], self.label_truncate or 256)
        self.model.train()
        self.zero_grad()
        try:
            loss = self.compute_loss(batch)
            self.backward(loss)
            self.update_params()
            oom_sync = False
        except RuntimeError as e:
            if 'out of memory' in str(e):
                oom_sync = True
                logging.error('Ran out of memory, skipping batch. if this happens frequently, decrease batchsize or truncate the inputs to the model.')
                self.global_metrics.add('skipped_batches', SumMetric(1))
            else:
                raise e
        if oom_sync:
            self._init_cuda_buffer(8, 8, True)

    def _construct_token_losses(self, labels, model_output):
        scores, _, _ = model_output
        score_view = scores.view(-1, scores.size(-1))
        losses = self.criterion(score_view, labels.view(-1)).view(len(labels), -1)
        token_losses = []
        for i, label in enumerate(labels):
            token_losses.append(list(zip([self.dict[token] for token in label.tolist()], losses[i].tolist())))
        return token_losses

    def _compute_fairseq_bleu(self, batch: Batch, preds):
        """
        Compute BLEU score between text and label, using the FAIRSeq BLEU Scorer.

        :param batch:
            Batch of observations
        :param texts:
            list of string predictions
        """
        all_results = []
        label_vec = batch.label_vec
        assert label_vec is not None, 'label_vec must exist for fairseq bleu'
        for i, t in enumerate(preds):
            result = FairseqBleuMetric.compute_many(t[1:], label_vec[i].unsqueeze(0), pad_idx=self.NULL_IDX, end_idx=self.END_IDX, unk_idx=self.dict[self.dict.unk_token])
            if result is None:
                return
            all_results.append(result)
        bleu_scores = list(zip(*all_results))
        for k in range(4):
            self.record_local_metric(f'fairseq_bleu{k + 1}', bleu_scores[k])

    def _compute_nltk_bleu(self, batch: Batch, texts: List[str]):
        """
        Compute BLEU score between text and label(s), using the NLTK BLEU Scorer.

        Note this differs from BLEU in ParlAI metrics in that the answers
        are unnormalized (no removal of stop words, etc.)

        :param batch:
            Batch of observations
        :param texts:
            list of string predictions
        """
        results: Dict[int, List[Metric]] = {}
        observations = batch.observations
        assert observations is not None, 'observations must not be none in nltk bleu'
        for i, p in enumerate(texts):
            obs = observations[i]
            references = []
            for lbl in obs['eval_labels']:
                references.append(self._v2t(self._vectorize_text(lbl, True, True, self.label_truncate, False)))
            for k in range(1, 5):
                b = BleuMetric.compute(p, references, k)
                if b is None:
                    b = BleuMetric(0)
                if k not in results:
                    results[k] = []
                results[k].append(b)
        for k in range(1, 5):
            self.record_local_metric(f'nltk_bleu{k}', results[k])

    def _add_generation_metrics(self, batch, preds):
        """
        Can be overridden to allow for some metrics on the generations calculated at
        eval.
        """
        pass

    def eval_step(self, batch):
        """
        Evaluate a single batch of examples.
        """
        if batch.text_vec is None and batch.image is None:
            return
        if batch.text_vec is not None:
            bsz = batch.text_vec.size(0)
        else:
            bsz = len(batch.image)
        self.model.eval()
        cand_scores = None
        token_losses = None
        if batch.label_vec is not None:
            loss, model_output = self.compute_loss(batch, return_output=True)
            if self.output_token_losses:
                token_losses = self._construct_token_losses(batch.label_vec, model_output)
        preds = None
        if self.skip_generation:
            warn_once('--skip-generation true produces limited metrics')
        else:
            maxlen = self.label_truncate or 256
            beam_preds_scores, _ = self._generate(batch, self.beam_size, maxlen)
            preds, scores = zip(*beam_preds_scores)
            self._add_generation_metrics(batch, preds)
        cand_choices = None
        if self.rank_candidates:
            cand_choices = []
            encoder_states = self.model.encoder(*self._encoder_input(batch))
            for i in range(bsz):
                num_cands = len(batch.candidate_vecs[i])
                enc = self.model.reorder_encoder_states(encoder_states, [i] * num_cands)
                cands, _ = self._pad_tensor(batch.candidate_vecs[i])
                scores, _ = self.model.decode_forced(enc, cands)
                cand_losses = F.cross_entropy(scores.view(num_cands * cands.size(1), -1), cands.view(-1), reduction='none').view(num_cands, cands.size(1))
                mask = (cands != self.NULL_IDX).float()
                cand_scores = (cand_losses * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-09)
                _, ordering = cand_scores.sort()
                cand_choices.append([batch.candidates[i][o] for o in ordering])
        text = [self._v2t(p) for p in preds] if preds is not None else None
        if text and self.compute_tokenized_bleu:
            self._compute_fairseq_bleu(batch, preds)
            self._compute_nltk_bleu(batch, text)
        return Output(text, cand_choices, token_losses=token_losses)

    def _treesearch_factory(self, device):
        method = self.opt.get('inference', 'greedy')
        beam_size = self.opt.get('beam_size', 1)
        if method == 'greedy':
            return GreedySearch(beam_size, min_length=0, block_ngram=self.beam_block_ngram, context_block_ngram=self.beam_context_block_ngram, length_penalty=self.opt.get('beam_length_penalty', 0.65), padding_token=self.NULL_IDX, bos_token=self.START_IDX, eos_token=self.END_IDX, device=device)
        elif method == 'beam':
            return BeamSearch(beam_size, min_length=self.beam_min_length, block_ngram=self.beam_block_ngram, context_block_ngram=self.beam_context_block_ngram, length_penalty=self.opt.get('beam_length_penalty', 0.65), padding_token=self.NULL_IDX, bos_token=self.START_IDX, eos_token=self.END_IDX, device=device)
        elif method == 'delayedbeam':
            return DelayedBeamSearch(self.opt['topk'], self.opt['beam_delay'], beam_size, min_length=self.beam_min_length, block_ngram=self.beam_block_ngram, context_block_ngram=self.beam_context_block_ngram, length_penalty=self.opt.get('beam_length_penalty', 0.65), padding_token=self.NULL_IDX, bos_token=self.START_IDX, eos_token=self.END_IDX, device=device)
        elif method == 'topk':
            return TopKSampling(self.opt['topk'], beam_size, min_length=self.beam_min_length, block_ngram=self.beam_block_ngram, context_block_ngram=self.beam_context_block_ngram, length_penalty=self.opt.get('beam_length_penalty', 0.65), padding_token=self.NULL_IDX, bos_token=self.START_IDX, eos_token=self.END_IDX, device=device)
        elif method == 'nucleus':
            return NucleusSampling(self.opt['topp'], beam_size, min_length=self.beam_min_length, block_ngram=self.beam_block_ngram, context_block_ngram=self.beam_context_block_ngram, length_penalty=self.opt.get('beam_length_penalty', 0.65), padding_token=self.NULL_IDX, bos_token=self.START_IDX, eos_token=self.END_IDX, device=device)
        else:
            raise ValueError(f"Can't use inference method {method}")

    def _get_context(self, batch, batch_idx):
        """
        Set the beam context for n-gram context blocking.

        Intentionally overridable for more complex model histories.
        """
        ctxt = batch.text_vec[batch_idx]
        if self.beam_block_full_context:
            full_ctxt = batch.observations[batch_idx].get('full_text_vec', ctxt)
            if not isinstance(full_ctxt, torch.LongTensor):
                full_ctxt = torch.LongTensor(full_ctxt).to(ctxt)
            ctxt = full_ctxt
        return ctxt

    def _get_initial_decoder_input(self, bsz: int, beam_size: int, dev: torch.device) -> torch.LongTensor:
        """
        Return initial input to the decoder.

        :param bsz:
            batchsize
        :param beam_size:
            beam size
        :param dev:
            device to send input to.

        :return initial_input:
            initial input for the decoder
        """
        return torch.LongTensor([self.START_IDX]).expand(bsz * beam_size, 1).to(dev)

    def _get_next_decoder_input(self, prev_input: torch.LongTensor, selection: torch.LongTensor, incr_state_inds: torch.LongTensor) -> torch.LongTensor:
        """
        Return next decoder input.

        :param prev_input:
            previous input to decoder
        :param selection:
            token selections for current timestep
        :param inds:
            incremental state indices

        :return decoder input:
            return decoder input for next timestep
        """
        prev_input = torch.index_select(prev_input, 0, incr_state_inds)
        decoder_input = torch.cat([prev_input, selection], dim=-1)
        return decoder_input

    def _generate(self, batch: Batch, beam_size: int, max_ts: int, prefix_tokens: Optional[torch.LongTensor]=None):
        """
        Generate an output with beam search.

        Depending on the options, this may perform greedy/topk/nucleus generation.

        :param Batch batch:
            Batch structure with input and labels
        :param int beam_size:
            Size of each beam during the search
        :param int max_ts:
            the maximum length of the decoded sequence
        :param prefix_tokens:
            if given, a tensor of tokens that must begin the decoded sequence.

        :return:
            tuple (beam_pred_scores, beams)

            - beam_preds_scores: list of (prediction, score) pairs for each sample in
              Batch
            - beams :list of Beam instances defined in Beam class, can be used for any
              following postprocessing, e.g. dot logging.
        """
        model = self.model
        if isinstance(model, torch.nn.parallel.DistributedDataParallel):
            model = self.model.module
        encoder_states = model.encoder(*self._encoder_input(batch))
        if batch.text_vec is not None:
            dev = batch.text_vec.device
        else:
            assert batch.label_vec is not None, 'need label_vec for _generate'
            dev = batch.label_vec.device
        bsz = len(batch.text_lengths) if batch.text_lengths is not None else len(batch.image)
        if batch.text_vec is not None:
            batchsize = batch.text_vec.size(0)
            beams = [self._treesearch_factory(dev).set_context(self._get_context(batch, batch_idx)).set_block_list(self.beam_block_list) for batch_idx in range(batchsize)]
        else:
            beams = [self._treesearch_factory(dev) for _ in range(bsz)]
        decoder_input = self._get_initial_decoder_input(bsz, beam_size, dev)
        inds = torch.arange(bsz).to(dev).unsqueeze(1).repeat(1, beam_size).view(-1)
        encoder_states = model.reorder_encoder_states(encoder_states, inds)
        incr_state = None
        for _ts in range(max_ts):
            if all((b.is_done() for b in beams)):
                break
            score, incr_state = model.decoder(decoder_input, encoder_states, incr_state)
            score = score[:, -1:, :]
            score = model.output(score)
            score = score.view(bsz, beam_size, -1)
            if self.temperature != 1.0:
                score.div_(self.temperature)
            score = F.log_softmax(score, dim=-1, dtype=torch.float32)
            if prefix_tokens is not None and _ts < prefix_tokens.size(1):
                prefix_toks = prefix_tokens[:, _ts].unsqueeze(-1).repeat(1, beam_size)
                prefix_score = score.gather(-1, prefix_toks.unsqueeze(-1))
                prefix_mask = prefix_toks.ne(self.NULL_IDX)
                score[prefix_mask] = neginf(score.dtype)
                score[prefix_mask] = score[prefix_mask].scatter_(-1, prefix_toks[prefix_mask].unsqueeze(-1), prefix_score[prefix_mask])
            for i, b in enumerate(beams):
                if not b.is_done():
                    b.advance(score[i])
            incr_state_inds = torch.cat([beam_size * i + b.get_backtrack_from_current_step() for i, b in enumerate(beams)])
            incr_state = model.reorder_decoder_incremental_state(incr_state, incr_state_inds)
            selection = torch.cat([b.get_output_from_current_step() for b in beams]).unsqueeze(-1)
            decoder_input = self._get_next_decoder_input(decoder_input, selection, incr_state_inds)
        n_best_beam_preds_scores = [b.get_rescored_finished() for b in beams]
        if hasattr(self, '_rerank_beams'):
            n_best_beam_preds_scores = self._rerank_beams(batch, n_best_beam_preds_scores)
        beam_preds_scores = [n_best_list[0] for n_best_list in n_best_beam_preds_scores]
        if self.opt.get('verbose'):
            for i, beams in enumerate(n_best_beam_preds_scores):
                for b, (tokens, score) in enumerate(beams):
                    gen = self._v2t(tokens)
                    logging.debug(f'Batch[{i:3d}] Beam[{b:3d}]: ({score:4.2f}): {gen}')
                logging.debug('-')
        return (beam_preds_scores, beams)

    def _load_beam_block_list(self) -> SearchBlocklist:
        """
        Load the beam block_list.

        :return: a dict mapping ngram length to different ngrams
        """
        block_list = SearchBlocklist(self.dict)
        if not self.opt.get('beam_block_list_filename'):
            return block_list
        block_list_fn = self.opt['beam_block_list_filename']
        try:
            with open(block_list_fn) as f:
                for line in f:
                    block_list.add(line.strip())
        except IOError:
            logging.error(f'Could not load beam block_list {block_list_fn}, using empty block_list.')
        return block_list