import os
import os.path as op
import re
import numpy as np
from ...utils.filemanip import load_json, save_json, split_filename
from ..base import (
from ...external.due import BibTeX
from .base import (
class LocalstatInputSpec(AFNICommandInputSpec):
    in_file = File(exists=True, mandatory=True, argstr='%s', position=-1, desc='input dataset')
    neighborhood = traits.Either(traits.Tuple(traits.Enum('SPHERE', 'RHDD', 'TOHD'), traits.Float()), traits.Tuple(traits.Enum('RECT'), traits.Tuple(traits.Float(), traits.Float(), traits.Float())), mandatory=True, desc="The region around each voxel that will be extracted for the statistics calculation. Possible regions are: 'SPHERE', 'RHDD' (rhombic dodecahedron), 'TOHD' (truncated octahedron) with a given radius in mm or 'RECT' (rectangular block) with dimensions to specify in mm.", argstr="-nbhd '%s(%s)'")
    _stat_names = ['mean', 'stdev', 'var', 'cvar', 'median', 'MAD', 'min', 'max', 'absmax', 'num', 'sum', 'FWHM', 'FWHMbar', 'rank', 'frank', 'P2skew', 'ALL', 'mMP2s', 'mmMP2s']
    stat = InputMultiObject(traits.Either(traits.Enum(_stat_names), traits.Tuple(traits.Enum('perc'), traits.Tuple(traits.Float, traits.Float, traits.Float))), mandatory=True, desc="statistics to compute. Possible names are:\n\n * mean   = average of the values\n * stdev  = standard deviation\n * var    = variance (stdev\\*stdev)\n * cvar   = coefficient of variation = stdev/fabs(mean)\n * median = median of the values\n * MAD    = median absolute deviation\n * min    = minimum\n * max    = maximum\n * absmax = maximum of the absolute values\n * num    = number of the values in the region:\n            with the use of -mask or -automask,\n            the size of the region around any given\n            voxel will vary; this option lets you\n            map that size.  It may be useful if you\n            plan to compute a t-statistic (say) from\n            the mean and stdev outputs.\n * sum    = sum of the values in the region\n * FWHM   = compute (like 3dFWHM) image smoothness\n            inside each voxel's neighborhood.  Results\n            are in 3 sub-bricks: FWHMx, FHWMy, and FWHMz.\n            Places where an output is -1 are locations\n            where the FWHM value could not be computed\n            (e.g., outside the mask).\n * FWHMbar= Compute just the average of the 3 FWHM values\n            (normally would NOT do this with FWHM also).\n * perc:P0:P1:Pstep =\n            Compute percentiles between P0 and P1 with a\n            step of Pstep.\n            Default P1 is equal to P0 and default P2 = 1\n * rank   = rank of the voxel's intensity\n * frank  = rank / number of voxels in neighborhood\n * P2skew = Pearson's second skewness coefficient\n             3 \\* (mean - median) / stdev\n * ALL    = all of the above, in that order\n            (except for FWHMbar and perc).\n * mMP2s  = Exactly the same output as:\n            median, MAD, P2skew,\n            but a little faster\n * mmMP2s = Exactly the same output as:\n            mean, median, MAD, P2skew\n\nMore than one option can be used.", argstr='-stat %s...')
    mask_file = File(exists=True, desc="Mask image file name. Voxels NOT in the mask will not be used in the neighborhood of any voxel. Also, a voxel NOT in the mask will have its statistic(s) computed as zero (0) unless the parameter 'nonmask' is set to true.", argstr='-mask %s')
    automask = traits.Bool(desc='Compute the mask as in program 3dAutomask.', argstr='-automask')
    nonmask = traits.Bool(desc='Voxels not in the mask WILL have their local statistics\ncomputed from all voxels in their neighborhood that ARE in\nthe mask. For instance, this option can be used to compute the\naverage local white matter time series, even at non-WM\nvoxels.', argstr='-use_nonmask')
    reduce_grid = traits.Either(traits.Float, traits.Tuple(traits.Float, traits.Float, traits.Float), argstr='-reduce_grid %s', xor=['reduce_restore_grid', 'reduce_max_vox'], desc="Compute output on a grid that is reduced by the specified factors. If a single value is passed, output is resampled to the specified isotropic grid. Otherwise, the 3 inputs describe the reduction in the X, Y, and Z directions. This option speeds up computations at the expense of resolution. It should only be used when the nbhd is quite large with respect to the input's resolution, and the resultant stats are expected to be smooth.")
    reduce_restore_grid = traits.Either(traits.Float, traits.Tuple(traits.Float, traits.Float, traits.Float), argstr='-reduce_restore_grid %s', xor=['reduce_max_vox', 'reduce_grid'], desc='Like reduce_grid, but also resample output back to inputgrid.')
    reduce_max_vox = traits.Float(argstr='-reduce_max_vox %s', xor=['reduce_restore_grid', 'reduce_grid'], desc='Like reduce_restore_grid, but automatically set Rx Ry Rz sothat the computation grid is at a resolution of nbhd/MAX_VOXvoxels.')
    grid_rmode = traits.Enum('NN', 'Li', 'Cu', 'Bk', argstr='-grid_rmode %s', requires=['reduce_restore_grid'], desc="Interpolant to use when resampling the output with thereduce_restore_grid option. The resampling method string RESAM should come from the set {'NN', 'Li', 'Cu', 'Bk'}. These stand for 'Nearest Neighbor', 'Linear', 'Cubic', and 'Blocky' interpolation, respectively.")
    quiet = traits.Bool(argstr='-quiet', desc='Stop the highly informative progress reports.')
    overwrite = traits.Bool(desc='overwrite output file if it already exists', argstr='-overwrite')
    out_file = File(desc='Output dataset.', argstr='-prefix %s', name_source='in_file', name_template='%s_localstat', keep_extension=True, position=0)