import re
import string
import hypothesis
import hypothesis.strategies
import pytest
import spacy
from spacy.tokenizer import Tokenizer
from spacy.util import get_lang_class
@pytest.mark.xfail
@pytest.mark.parametrize('lang', LANGUAGES)
@hypothesis.given(sentence=sentence_strategy())
def test_tokenizer_explain_fuzzy(lang: str, sentence: str) -> None:
    """
    Tests whether output of tokenizer.explain() matches tokenizer output. Input generated by hypothesis.
    lang (str): Language to test.
    text (str): Fuzzily generated sentence to tokenize.
    """
    tokenizer: Tokenizer = spacy.blank(lang).tokenizer
    sentence = re.sub('\\s+', ' ', sentence).strip()
    tokens = [t.text for t in tokenizer(sentence)]
    debug_tokens = [t[1] for t in tokenizer.explain(sentence)]
    assert tokens == debug_tokens, f'{tokens}, {debug_tokens}, {sentence}'