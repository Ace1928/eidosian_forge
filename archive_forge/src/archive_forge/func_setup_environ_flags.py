import os
import time
import yaml
from contextlib import nullcontext
from pathlib import Path
from pkg_resources import packaging
from datetime import datetime
import torch
import torch.cuda.nccl as nccl
import torch.distributed as dist
from torch.distributed.fsdp import StateDictType
from torch.distributed.fsdp.sharded_grad_scaler import ShardedGradScaler
from tqdm import tqdm
from transformers import LlamaTokenizer
import json
from llama_recipes.model_checkpointing import save_model_checkpoint, save_model_and_optimizer_sharded, save_optimizer_checkpoint
from llama_recipes.policies import fpSixteen,bfSixteen, get_llama_wrapper
from llama_recipes.utils.memory_utils import MemoryTrace
from accelerate.utils import is_xpu_available, is_ccl_available
def setup_environ_flags(rank):
    """Set environment flags for debugging purposes"""
    os.environ['TORCH_SHOW_CPP_STACKTRACES'] = str(1)
    os.environ['NCCL_ASYNC_ERROR_HANDLING'] = str(1)
    if rank == 0:
        print(f'--> Running with torch dist debug set to detail')