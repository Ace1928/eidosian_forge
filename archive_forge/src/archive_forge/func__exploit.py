import copy
import json
import logging
import math
import os
import random
import shutil
import warnings
from pathlib import Path
from typing import Callable, Dict, List, Optional, Tuple, Union, TYPE_CHECKING
from ray.air.constants import TRAINING_ITERATION
from ray.train import Checkpoint
from ray.train._internal.session import _TrainingResult, _FutureTrainingResult
from ray.tune.error import TuneError
from ray.tune.result import DEFAULT_METRIC
from ray.tune.search import SearchGenerator
from ray.tune.utils.util import SafeFallbackEncoder
from ray.tune.search.sample import Domain, Function
from ray.tune.schedulers import FIFOScheduler, TrialScheduler
from ray.tune.search.variant_generator import format_vars
from ray.tune.experiment import Trial
from ray.util import PublicAPI
from ray.util.debug import log_once
def _exploit(self, tune_controller: 'TuneController', trial: Trial, trial_to_clone: Trial):
    """Transfers perturbed state from trial_to_clone -> trial.

        If specified, also logs the updated hyperparam state.
        """
    trial_state = self._trial_state[trial]
    new_state = self._trial_state[trial_to_clone]
    class_name = self.__class__.__name__
    logger.info(f'\n\n[{class_name}] [Exploit] Cloning trial {{}} (score = {{:4f}}) into trial {{}} (score = {{:4f}})\n'.format(trial_to_clone.trial_id, new_state.last_score, trial.trial_id, trial_state.last_score))
    new_config, operations = self._get_new_config(trial, trial_to_clone)
    old_params = _filter_mutated_params_from_config(trial_to_clone.config, self._hyperparam_mutations)
    new_params = _filter_mutated_params_from_config(new_config, self._hyperparam_mutations)
    explore_info_str = f'\n\n[{class_name}] [Explore] Perturbed the hyperparameter config of trial{trial.trial_id}:\n'
    explore_info_str += self._summarize_hyperparam_changes(old_params, new_params, operations) or 'No hyperparameters mutated.'
    logger.info(explore_info_str)
    if self._log_config:
        self._log_config_on_step(trial_state, new_state, trial, trial_to_clone, new_config)
    new_tag = _make_experiment_tag(trial_state.orig_tag, new_config, self._hyperparam_mutations)
    if trial.status == Trial.PAUSED:
        if not self._synch:
            raise TuneError('Trials should be paused here only if in synchronous mode. If you encounter this error please raise an issue on Ray Github.')
    else:
        tune_controller.pause_trial(trial, should_checkpoint=False)
    trial.set_experiment_tag(new_tag)
    trial.set_config(new_config)
    checkpoint_to_exploit: Checkpoint = copy.copy(new_state.last_checkpoint)
    trial.run_metadata.checkpoint_manager._latest_checkpoint_result = _TrainingResult(checkpoint=checkpoint_to_exploit, metrics=new_state.last_result)
    self._num_perturbations += 1
    trial_state.last_perturbation_time = new_state.last_perturbation_time
    trial_state.last_train_time = new_state.last_train_time