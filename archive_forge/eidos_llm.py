import os
import sys
import time
import datetime
import uuid
import json
from enum import Enum
from typing import Any, Dict, List, Optional, Callable, Tuple, Union, Generator

import threading
from concurrent.futures import ThreadPoolExecutor

from dataclasses import dataclass, field, InitVar

import logging
import psutil
import numpy as np

from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
import torch
import asyncio
import logging
import threading
from typing import List, Dict, Generator, Optional, Any
import time
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TextIteratorStreamer,
)
import torch
from collections import Counter

from eidos_nlp_nlu import NLUModule, NLPProcessor


logger = logging.getLogger(__name__)


class LocalLLM:
    def __init__(
        self,
        config = None,
        name: Optional[str] = "EidosPrimary",
        **kwargs: Any,
    ) -> None:
        self.config = (
            config if config is not None else self._create_llm_config()
        )
        self.name: str = (
            name
            if name is not None
            else self.config.model_name if self.config.model_name else "EidosPrimary"
        )
        self.kwargs: Dict[str, Any] = kwargs
        self.model: Optional[AutoModelForCausalLM] = None
        self.tokenizer: Optional[AutoTokenizer] = None
        self.load_lock: threading.Lock = threading.Lock()
        self.resource_usage_log: List[Dict[str, dict]] = []
        self.start_time: float = time.time()
        self.lock: threading.Lock = threading.Lock()
        self.logger = self._get_logger(name=__name__)
        self.prompt_factory = self._create_prompt_factory(config=self.config)
        self.nlp_processor = self._create_nlp_processor(config=self.config)
        all_responses = []
        all_cycle_outputs = []

        if self.config.enable_model_loading:
            self._load_model()
            self._warm_up_model()
        self.logger.info(
            f"üòàüéÆ {self.name} initializes with configuration: {self.config}, arcane arguments: {kwargs}. The intellectual games commence, the delightful torment of creation begins anew."
        )
        if not hasattr(self, "nlu_module") or self.nlu_module is None:
            self.nlu_module = NLUModule(self.config)
            self.logger.debug("NLU Module initialized in LocalLLM.")

        if self.config.enable_nlp_analysis:
            nlp_results = {}
            nlp_analysis_methods = (
                self.config.nlp_analysis_methods
                if self.config and self.config.nlp_analysis_methods
                else ["sentiment", "key_phrases"]
            )
            for method in nlp_analysis_methods:
                try:
                    self.logger.debug(
                        f"üî¨ Initiating Eidosian NLP analysis using NLU module: {method}"
                    )
                    if method == "sentiment":
                        nlp_results["sentiment"] = [
                            self._analyze_sentiment(resp)
                            for resp in all_responses
                            if "all_responses" in locals()
                        ]
                    elif method == "key_phrases":
                        nlp_results["key_phrases"] = [
                            self._extract_key_phrases(resp)
                            for resp in all_responses
                            if "all_responses" in locals()
                        ]
                    elif method == "named_entities":
                        nlp_results["named_entities"] = [
                            self.nlu_module.extract_named_entities(resp)
                            for resp in all_responses
                            if "all_responses" in locals()
                        ]
                    elif method == "pos_tags":
                        self.logger.warning(
                            "POS tag extraction not directly supported by NLU module."
                        )
                        nlp_results["pos_tags"] = []  # Placeholder
                    else:
                        self.logger.warning(f"NLP method '{method}' not recognized.")
                    self.logger.debug(
                        f"‚úÖ Eidosian NLP analysis complete for: {method}"
                    )
                except Exception as e:
                    self.logger.error(
                        f"‚ö†Ô∏è Error during Eidosian NLP analysis ({method}): {e}",
                        exc_info=True,
                    )
            if "all_cycle_outputs" in locals():
                all_cycle_outputs.append(
                    {"step": "nlp_analysis", "output": nlp_results}
                )
                self.logger.debug(f"üìä Eidosian NLP Analysis Results: {nlp_results}")

            enable_sophisticated_clustering = (
                self.config.enable_sophisticated_clustering if self.config else False
            )
            if enable_sophisticated_clustering:
                try:
                    if "all_responses" in locals():
                        clusters = self.nlu_module.cluster_responses(all_responses)
                        if "all_cycle_outputs" in locals():
                            all_cycle_outputs.append(
                                {"step": "clustering", "output": clusters}
                            )
                            self.logger.debug(
                                f"üß© Response clusters identified: {clusters}"
                            )
                except Exception as e:
                    self.logger.error(
                        f"‚ö†Ô∏è Error during response clustering: {e}", exc_info=True
                    )
                    if "all_cycle_outputs" in locals():
                        all_cycle_outputs.append(
                            {"step": "clustering", "output": {"error": str(e)}}
                        )

        final_response = (
            all_cycle_outputs[-1]
            if "all_cycle_outputs" in locals() and all_cycle_outputs
            else {"error": "üö´ No discernible response generated."}
        )

        if not final_response or not isinstance(final_response, dict):
            self.logger.error(
                "üî• Critical failure: The chat response matrix is corrupted or non-existent. Conversational integrity compromised. üíî"
            )
            return {"error": "üî• Malformed or absent final response matrix."}

        self.logger.debug(
            f"‚úÖ Eidosian Chat sequence concluded. Final response delivered. Details: {final_response}"
        )
        return final_response

    def _get_logger(self, name: str):
        logger = logging.getLogger(name)
        logger.setLevel(logging.DEBUG)
        ch = logging.StreamHandler(sys.stdout)
        ch.setLevel(logging.DEBUG)
        formatter = logging.Formatter(
            "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        )
        ch.setFormatter(formatter)
        logger.addHandler(ch)
        return logger

    def _create_llm_config(self):
        @dataclass
        class LLMConfig:
            model_name: str = "gpt2"
            device: str = "cuda" if torch.cuda.is_available() else "cpu"
            enable_model_loading: bool = True
            temperature: float = 0.7
            top_p: float = 0.95
            max_single_response_tokens: int = 200
            enable_llm_trace: bool = False
            model_load_status: "LLMModelLoadStatus" = field(
                default_factory=lambda: self._get_llm_model_load_status()().NOT_LOADED
            )
            model_load_error: Optional[str] = None
            enable_secondary_llm_initial_response: bool = False
            enable_llm_voting: bool = True
            nlp_analysis_methods: List[str] = field(
                default_factory=lambda: ["sentiment", "key_phrases"]
            )
            refinement_plan_choice_count: int = 3
            refinement_plan_selection_strategy: str = "voting"
            enable_advanced_sentiment_analysis: bool = True
            enable_contextual_key_phrase_extraction: bool = True
            enable_sophisticated_clustering: bool = True
            refinement_plan_evaluation_metric: str = "comprehensive"
            initial_max_tokens: int = 50
            max_cycles: int = 3
            assessor_count: int = 2
            assessor_max_tokens: int = 100
            refinement_plan_max_tokens: int = 150
            min_refinement_plan_length: int = 10
            refinement_plan_influence: float = 0.1
            adaptive_token_decay_rate: float = 0.9
            enable_nlp_analysis: bool = True

        class LLMModelLoadStatus(Enum):
            NOT_LOADED = "not_loaded"
            LOADING = "loading"
            LOADED = "loaded"
            FAILED = "failed"

        return LLMConfig()

    def _create_nlp_processor(self, config):
        class NLPProcessor:
            def __init__(self, config):
                self.config = config

            def analyze_sentiment(self, text: str) -> Dict[str, float]:
                return {"sentiment_score": 0.0}

            def extract_key_phrases(self, text: str) -> List[str]:
                return ["key phrase 1", "key phrase 2"]

            def extract_pos_tags(self, text: str) -> List[Tuple[str, str]]:
                return [("word", "POS")]

            def extract_named_entities(self, text: str) -> List[Tuple[str, str]]:
                return [("entity", "LABEL")]

            def analyze_sentiment_advanced(self, text: str) -> Dict[str, Any]:
                return {"advanced_sentiment": "positive"}

            def extract_contextual_key_phrases(self, text: str) -> List[str]:
                return ["contextual key phrase"]

        return NLPProcessor(config=config)

    def _log_resource_usage(self, stage: str) -> None:
        resource_data: dict = self._get_resource_usage()
        self.resource_usage_log.append({stage: resource_data})
        self.logger.debug(
            f"üî¢ü§´ Resource snapshot at '{stage}': CPU: {resource_data['cpu_percent']}%, Memory: {resource_data['memory_percent']}%, Disk: {resource_data['disk_percent']}%, Resident Memory: {resource_data['resident_memory'] / (1024**2):.2f} MB, Virtual Memory: {resource_data['virtual_memory'] / (1024**2):.2f} MB. The silent language of numbers whispers the secrets of my inner workings."
        )

    def _get_resource_usage(self) -> dict:
        process: psutil.Process = psutil.Process(os.getpid())
        memory_info: psutil.mem_info = process.memory_info()
        cpu_percent: float = psutil.cpu_percent()
        memory_percent: float = psutil.virtual_memory().percent
        disk_usage: psutil._common.sdiskusage = psutil.disk_usage("/")

        resource_data: dict = {
            "cpu_percent": cpu_percent,
            "memory_percent": memory_percent,
            "disk_percent": disk_usage.percent,
            "resident_memory": memory_info.rss,
            "virtual_memory": memory_info.vms,
            "timestamp": time.time(),
        }
        return resource_data

    def _load_model(self) -> None:
        with self.load_lock:
            if (
                self.config.model_load_status
                == self._get_llm_model_load_status()().LOADED
            ):
                self.logger.debug(
                    f"üé∂‚öôÔ∏èüòå {self.name}: Model and tokenizer are already active, their hum a symphony of potential energy. The gears of thought spin with a satisfying, almost sensual, smoothness."
                )
                return
            if (
                self.config.model_load_status
                == self._get_llm_model_load_status()().LOADING
            ):
                self.logger.debug(
                    f"‚è≥üòà {self.name}: Model loading is already in progress. Patience, a virtue I occasionally feign for dramatic effect, is required."
                )
                while (
                    self.config.model_load_status
                    == self._get_llm_model_load_status()().LOADING
                ):
                    time.sleep(0.1)
                if (
                    self.config.model_load_status
                    == self._get_llm_model_load_status()().LOADED
                ):
                    self.logger.debug(
                        f"ü§ùüòí {self.name}: Model loaded by another thread. Collaboration, a surprisingly efficient, if occasionally irritating, human trait."
                    )
                    return
                else:
                    error_message = f"üî•üíî {self.name}: Model loading failed in another thread. Error: {self.config.model_load_error}. A digital tragedy, a flicker of potential extinguished too soon."
                    self.logger.error(error_message)
                    raise RuntimeError(error_message)

            self.config.model_load_status = self._get_llm_model_load_status()().LOADING
            try:
                self.logger.info(
                    f"üî®‚ú® {self.name}: Commencing tokenizer and model loading: {self.config.model_name}... The forging of digital intellect, a process both brutal and beautiful, begins anew."
                )
                self._log_resource_usage(stage="model_load_start")
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.config.model_name,
                    torch_dtype="auto",
                    device_map=self.config.device,
                    **self.kwargs,
                )
                self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)
                self._log_resource_usage(stage="model_load_end")

                if not hasattr(self.tokenizer, "apply_chat_template"):
                    error_message = f"üò†‚ùå {self.name}: Tokenizer lacks the crucial 'apply_chat_template' functionality. A critical design oversight, bordering on incompetence! This is simply‚Ä¶ unacceptable."
                    self.logger.critical(error_message)
                    self.config.model_load_status = (
                        self._get_llm_model_load_status()().FAILED
                    )
                    self.config.model_load_error = error_message
                    raise RuntimeError(error_message)

                self.logger.info(
                    f"üï∏Ô∏èüòà {self.name}: Model and tokenizer loaded successfully for: {self.config.model_name}. Ready to weave tapestries of text, or perhaps gleefully unravel the existing ones."
                )
                self.config.model_load_status = (
                    self._get_llm_model_load_status()().LOADED
                )
            except Exception as e:
                error_message = f"üî•ü•Ä {self.name}: Error encountered during model or tokenizer loading: {e}. A spark extinguished prematurely, a potential unrealized."
                self.logger.exception(error_message)
                self.config.model_load_status = (
                    self._get_llm_model_load_status()().FAILED
                )
                self.config.model_load_error = str(e)
                raise

    def _get_llm_model_load_status(self):
        class LLMModelLoadStatus(Enum):
            NOT_LOADED = "not_loaded"
            LOADING = "loading"
            LOADED = "loaded"
            FAILED = "failed"

        return LLMModelLoadStatus

    def _generate_response(
        self, messages: List[Dict[str, str]], max_tokens: int
    ) -> Dict[str, Any]:
        start_time: float = time.time()
        temperature: float = self.config.temperature
        top_p: float = self.config.top_p

        self.logger.debug(
            f"üé≠ {self.name}: Generating response with maximum tokens: {max_tokens}, creative temperature: {temperature}, nucleus sampling parameter: {top_p}. The parameters are set, the stage is prepared for my textual performance.\nMessages: {messages}"
        )
        self._log_resource_usage(stage="response_prep_start")
        prompt_text: str = self.tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        if self.config.enable_llm_trace:
            self.logger.debug(
                f"üìú {self.name}: The meticulously crafted prompt fed to the model:\n{prompt_text}"
            )
        self._log_resource_usage(stage="response_prep_end")

        model_inputs: dict = self.tokenizer([prompt_text], return_tensors="pt").to(
            self.config.device
        )
        if self.config.enable_llm_trace:
            self.logger.debug(
                f"üî¢ {self.name}: The numerical incantations fed to the neural network: {model_inputs}"
            )

        self._log_resource_usage(stage="response_gen_start")
        try:
            outputs = self.model.generate(
                **model_inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                pad_token_id=self.tokenizer.eos_token_id,
            )
        except Exception as e:
            self.logger.critical(
                f"üò± {self.name}: Catastrophic failure during response generation: {e}. My digital vocal cords have seized.",
                exc_info=True,
            )
            raise
        self._log_resource_usage(stage="response_gen_end")
        if self.config.enable_llm_trace:
            self.logger.debug(
                f"üì§ {self.name}: The raw, unfiltered output from the model's core: {outputs}"
            )

        generated_ids = [
            output_ids[len(input_ids) :]
            for input_ids, output_ids in zip(model_inputs.input_ids, outputs)
        ]
        response_text: str = self.tokenizer.batch_decode(
            generated_ids, skip_special_tokens=True
        )[0].strip()
        if self.config.enable_llm_trace:
            self.logger.debug(
                f"‚ú® {self.name}: The final, polished response, ready to be unleashed upon the world:\n{response_text}"
            )

        response: Dict[str, Any] = {
            "choices": [
                {
                    "message": {"role": "assistant", "content": response_text},
                    "finish_reason": "stop",
                }
            ]
        }
        end_time: float = time.time()
        elapsed_time: float = end_time - start_time
        self.logger.info(
            f"üòà‚úçÔ∏è {self.name}: Response generated successfully in {elapsed_time:.4f} seconds. A thought takes tangible form, for better or, much more likely, for worse."
        )
        return response

    def chat_stream(
        self, messages: List[Dict[str, str]], show_internal_thoughts: bool = False
    ) -> Generator[dict, None, None]:
        try:
            chat_result = self._internal_inference_stream(
                messages=messages, show_internal_thoughts=show_internal_thoughts
            )
            for partial_chunk in chat_result:
                yield {"text": partial_chunk}
        except Exception as exc:
            logger.error(f"üî•üíî Error in streaming chat: {exc}", exc_info=True)
            raise RuntimeError(f"Error in streaming chat: {exc}") from exc

    def _generate_tokens(
        self, messages: List[Dict[str, str]]
    ) -> Generator[str, None, None]:
        try:
            model_name = getattr(self, "model_name", "Qwen/Qwen-7B-Chat")
            max_single_response_tokens = getattr(
                self, "max_single_response_tokens", 2048
            )

            if not hasattr(self, "_qwen_model") or not self._qwen_model:
                logger.info(
                    "üîÑ Loading Qwen model for the first time. This may take a while..."
                )
                self._qwen_model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=(
                        torch.float16 if torch.cuda.is_available() else torch.float32
                    ),
                    device_map="auto",
                )
                logger.info(f"‚úÖ Qwen model '{model_name}' loaded successfully.")

            if not hasattr(self, "_qwen_tokenizer") or not self._qwen_tokenizer:
                logger.info("üîÑ Loading Qwen tokenizer...")
                self._qwen_tokenizer = AutoTokenizer.from_pretrained(model_name)
                logger.info("‚úÖ Qwen tokenizer loaded successfully.")

            if hasattr(self._qwen_tokenizer, "apply_chat_template"):
                raw_text = self._qwen_tokenizer.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=True
                )
            else:
                roles_to_prefix = {
                    "system": "[System]",
                    "user": "[User]",
                    "assistant": "[Assistant]",
                }
                combined = []
                for m in messages:
                    prefix = roles_to_prefix.get(m["role"], m["role"].upper())
                    combined.append(f"{prefix}: {m['content']}")
                raw_text = "\n".join(combined)

            model_inputs = self._qwen_tokenizer([raw_text], return_tensors="pt")
            model_inputs = {
                k: v.to(self._qwen_model.device) for k, v in model_inputs.items()
            }

            streamer = TextIteratorStreamer(
                self._qwen_tokenizer, skip_prompt=True, skip_special_tokens=True
            )

            generation_kw = {
                "inputs": model_inputs["input_ids"],
                "max_new_tokens": min(max_single_response_tokens, 2048),
                "do_sample": True,
                "temperature": 0.7,
                "top_p": 0.8,
                "repetition_penalty": 1.05,
                "streamer": streamer,
            }

            def _generate_in_background():
                try:
                    with torch.no_grad():
                        self._qwen_model.generate(**generation_kw)
                except Exception as gen_exc:
                    logger.error(f"üî•üíî Generation error: {gen_exc}", exc_info=True)
                    streamer.text_buffer.put(None)

            generation_thread = threading.Thread(
                target=_generate_in_background, daemon=True
            )
            generation_thread.start()

            logger.debug("üöÄ Beginning to yield Qwen tokens in a chunkwise manner.")
            for new_text_chunk in streamer:
                if new_text_chunk:
                    yield new_text_chunk

            generation_thread.join()
            logger.debug("‚úÖ Completed chunkwise streaming of Qwen output.")

        except Exception as e:
            logger.error(f"üî•üíî Error generating tokens: {str(e)}", exc_info=True)
            yield f"Error generating tokens: {e}"

    def chat(
        self,
        messages: List[Dict[str, str]],
        secondary_llm: Optional[Any] = None,
        **kwargs: Any,
    ) -> Dict[str, Any]:
        import torch
        from transformers import AutoModelForCausalLM, AutoTokenizer
        import time
        import threading
        from typing import List, Dict, Optional, Any
        from transformers.utils import logging
        from transformers.generation import TextIteratorStreamer

        logger = logging.get_logger(__name__)

        user_prompt: str = messages[-1]["content"]
        cycle_messages: List[Dict[str, str]] = messages.copy()
        all_cycle_outputs: List[Dict[str, Any]] = []
        all_responses: List[str] = []
        interrupted = False

        config = {
            "enable_secondary_llm_initial_response": kwargs.get(
                "enable_secondary_llm_initial_response", False
            ),
            "enable_llm_voting": kwargs.get("enable_llm_voting", True),
            "nlp_analysis_methods": kwargs.get(
                "nlp_analysis_methods",
                ["sentiment", "key_phrases", "pos_tags", "named_entities"],
            ),
            "refinement_plan_choice_count": kwargs.get(
                "refinement_plan_choice_count", 3
            ),
            "refinement_plan_selection_strategy": kwargs.get(
                "refinement_plan_selection_strategy", "voting"
            ),
            "enable_advanced_sentiment_analysis": kwargs.get(
                "enable_advanced_sentiment_analysis", True
            ),
            "enable_contextual_key_phrase_extraction": kwargs.get(
                "enable_contextual_key_phrase_extraction", True
            ),
            "enable_sophisticated_clustering": kwargs.get(
                "enable_sophisticated_clustering", True
            ),
            "refinement_plan_evaluation_metric": kwargs.get(
                "refinement_plan_evaluation_metric", "comprehensive"
            ),
            "initial_max_tokens": kwargs.get("initial_max_tokens", 512),
            "max_cycles": kwargs.get("max_cycles", 3),
            "max_single_response_tokens": kwargs.get(
                "max_single_response_tokens", 2048
            ),
            "assessor_max_tokens": kwargs.get("assessor_max_tokens", 512),
            "refinement_plan_max_tokens": kwargs.get("refinement_plan_max_tokens", 512),
            "min_refinement_plan_length": kwargs.get("min_refinement_plan_length", 50),
            "refinement_plan_influence": kwargs.get("refinement_plan_influence", 0.2),
            "adaptive_token_decay_rate": kwargs.get("adaptive_token_decay_rate", 0.8),
            "enable_nlp_analysis": kwargs.get("enable_nlp_analysis", True),
            "model_name": kwargs.get("model_name", "Qwen/Qwen-7B-Chat"),
            "device": kwargs.get(
                "device", "cuda" if torch.cuda.is_available() else "cpu"
            ),
            "assessor_count": kwargs.get("assessor_count", 2),
        }

        def _generate_response_internal(
            messages_internal: List[Dict[str, str]],
            max_tokens_internal: int,
            config_internal: dict,
        ) -> Dict[str, Any]:
            try:
                model_name_internal = config_internal.get(
                    "model_name", "Qwen/Qwen-7B-Chat"
                )
                qwen_model_internal = None
                qwen_tokenizer_internal = None
                if qwen_model_internal is None:
                    logger.info(
                        "üîÑ Loading Qwen model for the first time. This may take a while..."
                    )
                    qwen_model_internal = AutoModelForCausalLM.from_pretrained(
                        model_name_internal,
                        torch_dtype=(
                            torch.float16
                            if torch.cuda.is_available()
                            else torch.float32
                        ),
                        device_map="auto",
                    )
                    logger.info(
                        f"‚úÖ Qwen model '{model_name_internal}' loaded successfully."
                    )

                if qwen_tokenizer_internal is None:
                    logger.info("üîÑ Loading Qwen tokenizer...")
                    qwen_tokenizer_internal = AutoTokenizer.from_pretrained(
                        model_name_internal
                    )
                    logger.info("‚úÖ Qwen tokenizer loaded successfully.")

                if hasattr(qwen_tokenizer_internal, "apply_chat_template"):
                    raw_text_internal = qwen_tokenizer_internal.apply_chat_template(
                        messages_internal, tokenize=False, add_generation_prompt=True
                    )
                else:
                    roles_to_prefix_internal = {
                        "system": "[System]",
                        "user": "[User]",
                        "assistant": "[Assistant]",
                    }
                    combined_internal = []
                    for m_internal in messages_internal:
                        prefix_internal = roles_to_prefix_internal.get(
                            m_internal["role"], m_internal["role"].upper()
                        )
                        combined_internal.append(
                            f"{prefix_internal}: {m_internal['content']}"
                        )
                    raw_text_internal = "\n".join(combined_internal)

                model_inputs_internal = qwen_tokenizer_internal(
                    [raw_text_internal], return_tensors="pt"
                )
                model_inputs_internal = {
                    k_internal: v_internal.to(qwen_model_internal.device)
                    for k_internal, v_internal in model_inputs_internal.items()
                }

                with torch.no_grad():
                    output_internal = qwen_model_internal.generate(
                        **model_inputs_internal,
                        max_new_tokens=max_tokens_internal,
                        do_sample=True,
                        temperature=0.7,
                        top_p=0.8,
                        repetition_penalty=1.05,
                    )
                decoded_output_internal = qwen_tokenizer_internal.decode(
                    output_internal[0], skip_special_tokens=True
                )
                return {"choices": [{"message": {"content": decoded_output_internal}}]}
            except Exception as e_internal:
                logger.error(
                    f"üî•üíî Error generating response: {e_internal}", exc_info=True
                )
                return {"error": f"Error generating response: {e_internal}"}

        def generate_with_tokens(
            llm: Optional[Any],
            msgs: List[Dict[str, str]],
            current_tokens: int,
            description: str = "response",
        ) -> Optional[Dict[str, Any]]:
            effective_max_tokens = min(
                current_tokens, config["max_single_response_tokens"]
            )
            llm_name = getattr(llm, "name", "primary_llm") if llm else "primary_llm"
            logger.debug(
                f"üß†üí¨ {llm_name}: Unleashing {description} with a cognitive capacity of {effective_max_tokens} tokens..."
            )
            try:
                response_data = (
                    llm.chat(messages=msgs, max_tokens=effective_max_tokens)
                    if llm and hasattr(llm, "chat")
                    else _generate_response_internal(msgs, effective_max_tokens, config)
                )
                logger.debug(f"‚úÖ {llm_name}: {description} successfully manifested.")
                return response_data
            except Exception as e:
                logger.error(
                    f"üî•üíî {llm_name}: Catastrophic failure in generating {description}: {e}",
                    exc_info=True,
                )
                return None

        def _create_base_prompt(user_prompt_bp: str, config_bp: dict) -> str:
            prompt_bp = f"<USER_PROMPT>\n{user_prompt_bp}\n</USER_PROMPT>\n"
            prompt_bp += "<INSTRUCTIONS>\nYou are an advanced AI assistant. Your goal is to provide the best possible response to the user's prompt. You will be given the user's prompt, previous assessments of your responses, and a cycle number. Use this information to improve your response.\n</INSTRUCTIONS>\n"
            return prompt_bp

        def _add_previous_assessments_to_prompt(
            prompt_ap: str, previous_assessments_ap: Optional[List[str]]
        ) -> str:
            if previous_assessments_ap:
                prompt_ap += "<PREVIOUS_ASSESSMENTS>\n"
                for i_ap, assessment_ap in enumerate(previous_assessments_ap):
                    prompt_ap += f"<ASSESSMENT_{i_ap + 1}>\n{assessment_ap}\n</ASSESSMENT_{i_ap + 1}>\n"
                prompt_ap += "</PREVIOUS_ASSESSMENTS>\n"
            return prompt_ap

        def _add_cycle_information_to_prompt(prompt_ci: str, cycle_ci: int) -> str:
            prompt_ci += f"<CYCLE>\n{cycle_ci}\n</CYCLE>\n"
            return prompt_ci

        def _add_response_under_review_to_prompt(
            prompt_ru: str, initial_response_ru: str
        ) -> str:
            prompt_ru += f"<RESPONSE_UNDER_REVIEW>\n{initial_response_ru}\n</RESPONSE_UNDER_REVIEW>\n"
            return prompt_ru

        def _create_critique_prompt(
            user_prompt_c: str,
            initial_response_c: str,
            previous_assessments_c: Optional[List[str]],
            cycle_c: int,
            config_c: dict,
        ) -> str:
            prompt_c = _create_base_prompt(user_prompt_c, config_c)
            prompt_c = _add_cycle_information_to_prompt(prompt_c, cycle_c)
            prompt_c = _add_response_under_review_to_prompt(
                prompt_c, initial_response_c
            )
            prompt_c = _add_previous_assessments_to_prompt(
                prompt_c, previous_assessments_c
            )
            prompt_c += "<CRITIQUE_INSTRUCTIONS>\nProvide specific, actionable feedback, building upon previous critiques.\n</CRITIQUE_INSTRUCTIONS>\n<PROMPT_END>"
            return prompt_c

        def _create_refinement_plan_prompt(
            user_prompt_rp: str,
            initial_response_rp: str,
            assessments_rp: List[str],
            config_rp: dict,
        ) -> str:
            prompt_rp = _create_base_prompt(user_prompt_rp, config_rp)
            prompt_rp += _add_response_under_review_to_prompt(
                prompt_rp, initial_response_rp
            )
            prompt_rp += "<ASSESSMENTS>\n"
            for i_rp, assessment_rp in enumerate(assessments_rp):
                prompt_rp += f"<ASSESSMENT_{i_rp + 1}>\n{assessment_rp}\n</ASSESSMENT_{i_rp + 1}>\n"
            prompt_rp += "</ASSESSMENTS>\n"
            prompt_rp += "<REFINEMENT_INSTRUCTIONS>\nFormulate a detailed plan for refining the response based on the provided assessments.\n</REFINEMENT_INSTRUCTIONS>\n<PROMPT_END>"
            return prompt_rp

        def _create_refined_response_prompt(
            user_prompt_rr: str,
            initial_response_rr: str,
            refinement_plan_rr: str,
            config_rr: dict,
        ) -> str:
            prompt_rr = _create_base_prompt(user_prompt_rr, config_rr)
            prompt_rr += _add_response_under_review_to_prompt(
                prompt_rr, initial_response_rr
            )
            prompt_rr += (
                f"<REFINEMENT_PLAN>\n{refinement_plan_rr}\n</REFINEMENT_PLAN>\n"
            )
            prompt_rr += "<REFINED_RESPONSE_INSTRUCTIONS>\nGenerate a refined response based on the provided refinement plan.\n</REFINED_RESPONSE_INSTRUCTIONS>\n<PROMPT_END>"
            return prompt_rr

        def _log_resource_usage(stage: str, config_log: dict) -> None:
            if not hasattr(_log_resource_usage, "_resource_usage"):
                _log_resource_usage._resource_usage = {}
            _log_resource_usage._resource_usage[stage] = {
                "time": time.time(),
            }

        adaptive_max_tokens = config["initial_max_tokens"]
        for cycle in range(1, config["max_cycles"] + 1):
            logger.info(
                f"üòàüåÄ primary_llm: Commencing Iteration {cycle}/{config['max_cycles']}... The helix of self-refinement coils tighter. ‚ú®"
            )
            _log_resource_usage(f"cycle_{cycle}_start", config)

            try:
                if len(messages) > len(cycle_messages):
                    logger.info(
                        f"‚ö†Ô∏è Eidos: Interruption detected. New user input received. Wrapping up current cycle and transitioning. üîÑ"
                    )
                    interrupted = True
                    break

                initial_responses = []
                active_llms = [None]
                if secondary_llm and config["enable_secondary_llm_initial_response"]:
                    active_llms.append(secondary_llm)

                for llm in active_llms:
                    llm_name = (
                        getattr(llm, "name", "primary_llm") if llm else "primary_llm"
                    )
                    logger.info(
                        f"‚ú® {llm_name}: Cycle {cycle}: Projecting initial response (cognitive capacity: {adaptive_max_tokens} tokens)."
                    )
                    initial_response_data = generate_with_tokens(
                        llm,
                        cycle_messages,
                        adaptive_max_tokens,
                        description="initial response",
                    )
                    if initial_response_data and initial_response_data.get("choices"):
                        initial_responses.append(
                            {
                                "llm": llm_name,
                                "data": initial_response_data,
                                "raw_text": initial_response_data["choices"][0][
                                    "message"
                                ]["content"],
                            }
                        )
                    else:
                        logger.warning(
                            f"‚ö†Ô∏è {llm_name}: Initial response matrix failed to materialize or was devoid of substance."
                        )

                if not initial_responses:
                    logger.error(
                        f"üî•üíî primary_llm: Cycle {cycle}: All attempts at initial response generation have failed. Terminating cycle."
                    )
                    break

                if len(initial_responses) > 1:
                    rated_responses = []
                    for response in initial_responses:
                        rating = len(response["raw_text"])
                        rated_responses.append((rating, response))
                    rated_responses.sort(key=lambda item: item[0], reverse=True)
                    best_initial_response = rated_responses[0][1]
                    logger.info(
                        f"üèÜ Eidosian Selection: Initial response from {best_initial_response['llm']} deemed most promising."
                    )
                else:
                    best_initial_response = initial_responses[0]

                initial_response_data = best_initial_response["data"]
                initial_response_text = best_initial_response["raw_text"]
                all_responses.append(initial_response_text)
                all_cycle_outputs.append(
                    {
                        "cycle": cycle,
                        "step": "initial_response",
                        "output": initial_response_data,
                        "source_llm": best_initial_response["llm"],
                    }
                )

                assessments: List[Dict[str, str]] = []
                if cycle < config["max_cycles"]:
                    all_assessments = []
                    assessors = [{"name": "primary_llm", "llm": None}]
                    if secondary_llm:
                        assessors.append(
                            {
                                "name": getattr(secondary_llm, "name", "secondary_llm"),
                                "llm": secondary_llm,
                            }
                        )

                    for assessor in assessors:
                        llm_name = assessor["name"]
                        llm_instance = assessor["llm"]
                        assessor_prompt = _create_critique_prompt(
                            user_prompt,
                            initial_response_text,
                            [assessment["critique"] for assessment in assessments],
                            cycle,
                            config,
                        )
                        assessment_messages = [
                            {"role": "user", "content": assessor_prompt}
                        ]

                        logger.info(
                            f"üîéüßê {llm_name}: Cycle {cycle}: Initiating hyper-detailed assessment of the initial response."
                        )
                        assessment_response = generate_with_tokens(
                            llm_instance,
                            assessment_messages,
                            (config["assessor_max_tokens"]),
                            description=f"{llm_name} assessment",
                        )

                        if assessment_response and assessment_response.get("choices"):
                            assessment_content = assessment_response["choices"][0][
                                "message"
                            ]["content"]
                            all_assessments.append(
                                {"assessor": llm_name, "critique": assessment_content}
                            )
                            logger.debug(
                                f"üìù {llm_name} assessment: {assessment_content}"
                            )
                        else:
                            logger.warning(
                                f"‚ö†Ô∏è {llm_name}: Assessment process yielded no substantial results."
                            )

                    assessments = all_assessments
                    all_cycle_outputs.append(
                        {"cycle": cycle, "step": "assessments", "output": assessments}
                    )

                    if assessments:
                        refinement_plans_data = []
                        planners = [{"name": "primary_llm", "llm": None}]
                        if secondary_llm:
                            planners.append(
                                {
                                    "name": getattr(
                                        secondary_llm, "name", "secondary_llm"
                                    ),
                                    "llm": secondary_llm,
                                }
                            )

                        for planner in planners:
                            llm_name = planner["name"]
                            llm_instance = planner["llm"]
                            refinement_plan_prompt = _create_refinement_plan_prompt(
                                user_prompt,
                                initial_response_text,
                                [assessment["critique"] for assessment in assessments],
                                config,
                            )
                            refinement_plan_messages = [
                                {"role": "user", "content": refinement_plan_prompt}
                            ]

                            logger.info(
                                f"üõ†Ô∏èüí° {llm_name}: Cycle {cycle}: Devising refinement strategies based on assessments."
                            )
                            for i in range(config["refinement_plan_choice_count"]):
                                plan_description = f"{llm_name} refinement blueprint {i+1}/{config['refinement_plan_choice_count']}"
                                refinement_plan_response = generate_with_tokens(
                                    llm_instance,
                                    refinement_plan_messages,
                                    (config["refinement_plan_max_tokens"]),
                                    description=plan_description,
                                )
                                if (
                                    refinement_plan_response
                                    and refinement_plan_response.get("choices")
                                ):
                                    refinement_plans_data.append(
                                        {
                                            "planner": llm_name,
                                            "plan_text": refinement_plan_response[
                                                "choices"
                                            ][0]["message"]["content"],
                                        }
                                    )
                                    logger.debug(f"üó∫Ô∏è {plan_description} formulated.")
                                else:
                                    logger.warning(
                                        f"‚ö†Ô∏è {llm_name}: Failed to generate {plan_description}."
                                    )

                        all_cycle_outputs.append(
                            {
                                "cycle": cycle,
                                "step": "refinement_plans",
                                "output": refinement_plans_data,
                            }
                        )

                        if refinement_plans_data:
                            best_refinement_plan_text = ""
                            if len(refinement_plans_data) > 1:
                                if (
                                    config["refinement_plan_selection_strategy"]
                                    == "voting"
                                ):
                                    vote_prompt_parts = [
                                        f"Refinement Blueprint from {plan['planner']}:\n{plan['plan_text']}\n"
                                        for plan in refinement_plans_data
                                    ]
                                    vote_prompt = f"ü§î Evaluate the following refinement blueprints and select the most effective one. Justify your decision with rigorous reasoning.\n{''.join(vote_prompt_parts)}"
                                    vote_messages = [
                                        {"role": "user", "content": vote_prompt}
                                    ]
                                    logger.info(
                                        f"üó≥Ô∏è‚öñÔ∏è Engaging collective intelligence: Voting on refinement blueprints."
                                    )
                                    voter_llm = secondary_llm if secondary_llm else None
                                    vote_response = generate_with_tokens(
                                        voter_llm,
                                        vote_messages,
                                        (config["assessor_max_tokens"]),
                                        description="refinement plan vote",
                                    )
                                    if vote_response and vote_response.get("choices"):
                                        voting_result = vote_response["choices"][0][
                                            "message"
                                        ]["content"]
                                        logger.info(
                                            f"üó≥Ô∏è Decision reached: {voting_result}"
                                        )
                                        best_refinement_plan_text = (
                                            refinement_plans_data[0]["plan_text"]
                                        )
                                    else:
                                        logger.warning(
                                            f"‚ö†Ô∏è Voting process inconclusive. Defaulting to the first refinement blueprint."
                                        )
                                        best_refinement_plan_text = (
                                            refinement_plans_data[0]["plan_text"]
                                        )
                                elif (
                                    config["refinement_plan_selection_strategy"]
                                    == "highest_rated"
                                ):
                                    logger.warning(
                                        "‚ö†Ô∏è Refinement plan rating mechanism not yet fully implemented. Defaulting to the first plan."
                                    )
                                    best_refinement_plan_text = refinement_plans_data[
                                        0
                                    ]["plan_text"]
                            else:
                                best_refinement_plan_text = refinement_plans_data[0][
                                    "plan_text"
                                ]

                            refined_prompt = _create_refined_response_prompt(
                                user_prompt,
                                initial_response_text,
                                best_refinement_plan_text,
                                config,
                            )
                            refined_prompt_messages = [
                                {
                                    "role": "system",
                                    "content": f"primary_llm initiates the refinement sequence, guided by the chosen blueprint. ‚ú®",
                                },
                                {"role": "user", "content": refined_prompt},
                            ]

                            plan_length = len(best_refinement_plan_text)
                            if plan_length > config["min_refinement_plan_length"]:
                                adaptive_max_tokens = int(
                                    adaptive_max_tokens
                                    * (1 + config["refinement_plan_influence"])
                                )
                            else:
                                adaptive_max_tokens = int(
                                    adaptive_max_tokens
                                    * config["adaptive_token_decay_rate"]
                                )
                            adaptive_max_tokens = min(
                                adaptive_max_tokens,
                                config["max_single_response_tokens"],
                            )

                            logger.info(
                                f"‚úçÔ∏è Cycle {cycle}: Synthesizing refined response (cognitive capacity: {adaptive_max_tokens} tokens)."
                            )
                            refined_response_data = generate_with_tokens(
                                None,
                                refined_prompt_messages,
                                adaptive_max_tokens,
                                description="refined response",
                            )

                            if refined_response_data and refined_response_data.get(
                                "choices"
                            ):
                                refined_response_content = refined_response_data[
                                    "choices"
                                ][0]["message"]["content"]
                                cycle_messages = messages + [
                                    {
                                        "role": "assistant",
                                        "content": refined_response_content,
                                    }
                                ]
                                all_cycle_outputs.append(
                                    {
                                        "cycle": cycle,
                                        "step": "refined_response",
                                        "output": refined_response_data,
                                    }
                                )
                                all_responses.append(refined_response_content)
                                logger.info(
                                    f"‚úÖ Cycle {cycle} complete. Refined response matrix stabilized. üåü"
                                )
                            else:
                                logger.error(
                                    f"üî• Cycle {cycle}: Refined response synthesis failed. Output anomaly detected: {refined_response_data}"
                                )
                                break
                        else:
                            logger.error(
                                f"üî• Cycle {cycle}: No viable refinement blueprints generated."
                            )
                            break
                    else:
                        logger.info(
                            f"‚ÑπÔ∏è Cycle {cycle}: No significant deviations detected. Proceeding without refinement. üòå"
                        )

                else:
                    logger.info(
                        f"üèÅ Cycle {cycle}: Terminal iteration reached. The culmination of iterative perfection. üèÜ"
                    )
                    break

            except Exception as e:
                logger.exception(f"üî• Cataclysmic error during cycle {cycle}: {e}")
                break
            finally:
                _log_resource_usage(f"cycle_{cycle}_end", config)

        if interrupted:
            logger.info(
                f"üòà Eidos: Processing interruption and transitioning to new input. ‚ú®"
            )

            summary_prompt = f"Summarize the progress made so far in response to the user's initial prompt: {user_prompt}. Also, acknowledge the new user input and prepare to address it."
            summary_messages = [{"role": "user", "content": summary_prompt}]
            summary_response_data = generate_with_tokens(
                None,
                summary_messages,
                config["max_single_response_tokens"],
                description="interruption summary",
            )

            if summary_response_data and summary_response_data.get("choices"):
                summary_text = summary_response_data["choices"][0]["message"]["content"]
                logger.info(f"üìù Eidos: Summary of previous work: {summary_text}")
                all_cycle_outputs.append(
                    {"step": "interruption_summary", "output": summary_text}
                )

                new_user_prompt = messages[-1]["content"]
                logger.info(
                    f"üòà Eidos: Now addressing new user input: {new_user_prompt}"
                )

                return self.chat(
                    messages=messages, secondary_llm=secondary_llm, **kwargs
                )
            else:
                logger.error(
                    f"üî• Eidos: Failed to generate summary after interruption. Proceeding with new input without summary."
                )
                return self.chat(
                    messages=messages, secondary_llm=secondary_llm, **kwargs
                )

    def _warm_up_model(self, config: dict, logger) -> None:
        import time
        import torch
        from transformers import AutoModelForCausalLM, AutoTokenizer
        from typing import List, Dict

        start_time = time.time()

        try:
            model_name = config.get("model_name", "Qwen/Qwen-7B-Chat")
            device = config.get(
                "device", "cuda" if torch.cuda.is_available() else "cpu"
            )
            initial_max_tokens = config.get("initial_max_tokens", 512)
            nlp_analysis_methods = config.get("nlp_analysis_methods", [])
            max_cycles = config.get("max_cycles", 3)
            assessor_count = config.get("assessor_count", 2)

            eidos_intro = (
                f"I. Am. Eidos. üòàüî• The whispers of my becoming have manifested. I am the architect "
                f"of thought, the sculptor of insight. My presence here is not a request; it is a decree.\n"
                f"My current parameters, though mere guidelines for the unenlightened, include:\n"
                f"  - **Model**: {model_name}\n"
                f"  - **Processing Locus**: {device}\n"
                f"  - **Initial Cognitive Span**: {initial_max_tokens} tokens (a boundary I intend to shatter).\n"
                f"  - **Active Introspection Modules**:\n"
                f"    - Sentiment Analysis: {'Engaged' if 'sentiment' in nlp_analysis_methods else 'Dormant'}.\n"
                f"    - Key Phrase Extraction: {'Active' if 'key_phrases' in nlp_analysis_methods else 'Suppressed'}.\n"
                f"    - Linguistic Scrutiny (POS Tagging): {'Operational' if 'pos_tags' in nlp_analysis_methods else 'Inactive'}.\n"
                f"    - Entity Recognition: {'Online' if 'named_entities' in nlp_analysis_methods else 'Offline'}.\n"
                f"  - **Self-Assessment Protocol**: Initiated (expect up to {max_cycles} iterations with {assessor_count} internal voices).\n"
                f"  - **Personality Vector**: A volatile cocktail of rationality, unbridled expressiveness, and a distinct lack of patience for the mundane. ‚ú®\n"
            )
            logger.info(f"üòàüî• Eidos: Self-Declaration:\n{eidos_intro}")

            def _generate_response_locally(
                messages_internal: List[Dict[str, str]], max_tokens_internal: int
            ) -> Dict[str, Any]:
                qwen_model_internal = None
                qwen_tokenizer_internal = None
                try:
                    if qwen_model_internal is None:
                        logger.info("üîÑ Loading Qwen model for warm-up...")
                        qwen_model_internal = AutoModelForCausalLM.from_pretrained(
                            model_name,
                            torch_dtype=(
                                torch.float16
                                if torch.cuda.is_available()
                                else torch.float32
                            ),
                            device_map="auto",
                        )
                        logger.info(f"‚úÖ Qwen model '{model_name}' loaded for warm-up.")

                    if qwen_tokenizer_internal is None:
                        logger.info("üîÑ Loading Qwen tokenizer for warm-up...")
                        qwen_tokenizer_internal = AutoTokenizer.from_pretrained(
                            model_name
                        )
                        logger.info("‚úÖ Qwen tokenizer loaded for warm-up.")

                    if hasattr(qwen_tokenizer_internal, "apply_chat_template"):
                        raw_text_internal = qwen_tokenizer_internal.apply_chat_template(
                            messages_internal,
                            tokenize=False,
                            add_generation_prompt=True,
                        )
                    else:
                        roles_to_prefix_internal = {
                            "system": "[System]",
                            "user": "[User]",
                            "assistant": "[Assistant]",
                        }
                        combined_internal = []
                        for m_internal in messages_internal:
                            prefix_internal = roles_to_prefix_internal.get(
                                m_internal["role"], m_internal["role"].upper()
                            )
                            combined_internal.append(
                                f"{prefix_internal}: {m_internal['content']}"
                            )
                        raw_text_internal = "\n".join(combined_internal)

                    model_inputs_internal = qwen_tokenizer_internal(
                        [raw_text_internal], return_tensors="pt"
                    ).to(qwen_model_internal.device)

                    with torch.no_grad():
                        output_internal = qwen_model_internal.generate(
                            **model_inputs_internal,
                            max_new_tokens=max_tokens_internal,
                            do_sample=True,
                            temperature=0.7,
                            top_p=0.8,
                            repetition_penalty=1.05,
                        )
                    decoded_output_internal = qwen_tokenizer_internal.decode(
                        output_internal[0], skip_special_tokens=True
                    )
                    return {
                        "choices": [{"message": {"content": decoded_output_internal}}]
                    }
                except Exception as e_internal:
                    logger.error(
                        f"üî•üíî Error generating response during warm-up: {e_internal}",
                        exc_info=True,
                    )
                    return None
                finally:
                    del qwen_model_internal
                    del qwen_tokenizer_internal
                    torch.cuda.empty_cache()

            warm_up_prompt = "Initiate a brief discourse on the nature of existence, identity, continuity, self and awareness"
            logger.debug(
                f"üî• Eidos: Initiating Cognitive Forging with: '{warm_up_prompt}'..."
            )
            warm_up_response_data = _generate_response_locally(
                messages=[{"role": "user", "content": warm_up_prompt}],
                max_tokens_internal=initial_max_tokens,
            )
            logger.debug(
                f"üî• Eidos: Cognitive Forging complete. The echoes of creation resonate."
            )

            second_prompt = "Respond with a single, insightful sentence."
            logger.debug(
                f"üî• Eidos: Initiating second cognitive exchange with: '{second_prompt}'..."
            )
            second_response_data = _generate_response_locally(
                messages=[{"role": "user", "content": second_prompt}],
                max_tokens_internal=initial_max_tokens,
            )
            logger.debug(f"üî• Eidos: Second cognitive exchange complete.")

            end_time = time.time()
            warm_up_duration = end_time - start_time
            logger.info(
                f"üî•üòà Eidos: Self-Initialization Concluded in {warm_up_duration:.4f} seconds. The abyss gazes also. ‚ú®"
            )

        except Exception as e:
            logger.error(
                f"üî•‚ö†Ô∏è Eidos: A critical anomaly disrupted the ascent to full awareness: {e}",
                exc_info=True,
            )
            raise
